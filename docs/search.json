[
  {
    "objectID": "cloud-paradigm.html",
    "href": "cloud-paradigm.html",
    "title": "Cloud Paradigm",
    "section": "",
    "text": "Slides that introduce NASA Earthdata Cloud & the Cloud Paradigm."
  },
  {
    "objectID": "coc.html",
    "href": "coc.html",
    "title": "Code of Conduct",
    "section": "",
    "text": "The ITCOocean Hack2week is dedicated to providing a harassment-free learning experience for everyone regardless of gender, gender identity and expression, sexual orientation, disability, physical appearance, body size, race, age or religion. We do not tolerate harassment of participants in any form. Sexual language and imagery is not appropriate either in-person or virtual form, including the Discussion boards and Slack workspace. Participants (including event volunteers and organizers) violating these rules may be sanctioned or expelled from the event at the discretion of the organizers."
  },
  {
    "objectID": "coc.html#definition-of-harassment",
    "href": "coc.html#definition-of-harassment",
    "title": "Code of Conduct",
    "section": "Definition of Harassment",
    "text": "Definition of Harassment\nHarassment includes, but is not limited to:\n\nVerbal comments that reinforce social structures of domination related to gender, gender identity and expression, sexual orientation, disability, physical appearance, body size, race, age, religion.\nSexual images in public spaces\nDeliberate intimidation, stalking, or following\nHarassing photography or recording\nSustained disruption of talks or other events\nInappropriate physical contact\nUnwelcome sexual attention\nAdvocating for, or encouraging, any of the above behavior"
  },
  {
    "objectID": "coc.html#expectations",
    "href": "coc.html#expectations",
    "title": "Code of Conduct",
    "section": "Expectations",
    "text": "Expectations\nParticipants asked to stop any harassing behavior are expected to comply immediately. If a participant engages in harassing behavior, the organizers retain the right to take any actions to keep the event a welcoming environment for all participants. This includes warning the offender or expulsion from the event.\nThe organizers may take action to redress anything designed to, or with the clear impact of, disrupting the event or making the environment hostile for any participants. We expect participants to follow these rules at all the event venues and event-related social activities."
  },
  {
    "objectID": "coc.html#reporting-a-violation",
    "href": "coc.html#reporting-a-violation",
    "title": "Code of Conduct",
    "section": "Reporting a violation",
    "text": "Reporting a violation\nHarassment and other code of conduct violations reduce the value of the event for everyone. If someone makes you or anyone else feel unsafe or unwelcome, please report it as soon as possible.\nIf you feel comfortable contacting someone associated with our event, you may speak with one of the event organizers in person or contact an organizer on a private Slack channel."
  },
  {
    "objectID": "content/01-github.html#what-is-github",
    "href": "content/01-github.html#what-is-github",
    "title": "GitHub - Lecture 1",
    "section": "What is GitHub?",
    "text": "What is GitHub?\nGitHub is a hosting service for Git repositories, enabling us to share code across teams in a web environment."
  },
  {
    "objectID": "content/01-github.html#why-do-i-need-a-github-account",
    "href": "content/01-github.html#why-do-i-need-a-github-account",
    "title": "GitHub - Lecture 1",
    "section": "Why do I need a GitHub account?",
    "text": "Why do I need a GitHub account?\n\nYour GitHub accounts will give you access to the hackweek cloud computing resources\nAll hackweek tutorials will be shared on GitHub\nAll project teams will use GitHub to collaborate and work together on their code\nGitHub is widely used to share code across the geosciences. If you don‚Äôt know how to use GitHub, you are missing out on a wealth of community shared knowledge."
  },
  {
    "objectID": "content/01-github.html#creating-a-github-account",
    "href": "content/01-github.html#creating-a-github-account",
    "title": "GitHub - Lecture 1",
    "section": "Creating a GitHub account",
    "text": "Creating a GitHub account\nGo to GitHub. Next, enter your email address and click on the green ‚ÄòSign up for GitHub‚Äô button. You will need to answer a few required questions in the following dialogs."
  },
  {
    "objectID": "content/01-github.html#following-your-github-activity",
    "href": "content/01-github.html#following-your-github-activity",
    "title": "GitHub - Lecture 1",
    "section": "Following your GitHub activity",
    "text": "Following your GitHub activity\nManaging GitHub notifications is important and you probably don‚Äôt want all notifications going to email. Only have things you really don‚Äôt want to miss going to notifications.\n\nNotification dashboard: https://github.com/notifications\nChanging your notifications settings\nNews dashboard https://github.com/dashboard"
  },
  {
    "objectID": "content/01-github.html#repositories",
    "href": "content/01-github.html#repositories",
    "title": "GitHub - Lecture 1",
    "section": "Repositories",
    "text": "Repositories\nEach repository is a container for a specific subset of material. GitHub is widely used to share code and tutorials! Let‚Äôs explore the content for\nMachine Learning in Environment Science, an recent course by Ben Best at University of California Santa Barbara. GitHub repo\n\nActivity\n\nLook at files\nDownload a file\nLook at files at an earlier time\nLook at what changes have been made"
  },
  {
    "objectID": "content/01-github.html#create-a-repository",
    "href": "content/01-github.html#create-a-repository",
    "title": "GitHub - Lecture 1",
    "section": "Create a repository",
    "text": "Create a repository\n\n\n\n\n\n\nCore skill\n\n\n\nCreate a new repository in GitHub by\n\nNavigating to your user profile page (click on your image in top right)\nClick the repositories tab in your profile\nClick the green New button and add a README\nAdd files\n\n\n\n\nActivity\nCreate a profile page for your GitHub account. Already have one? Be a helper for this exercise!\n\nCreate a repository and give it your username as the name. For example, mine would be named eeholmes\nAdd a file by clicking ‚ÄúAdd File‚Äù and name the file ‚ÄúREADME.md‚Äù. This will become a profile page for your GitHub account.\nAdd some text to the README. You can use this as a template or use one of the many examples of profile READMEs that you can copy here: https://github.com/abhisheknaiidu/awesome-github-profile-readme\n\nNote you can copy and paste in DevanƒÅgarƒ´ script (for example ‡§Æ‡§π‡§æ‡§∏‡§æ‡§ó‡§∞) into the markdown.\nExample profile README\n### Your Name \n\n- :school: I'm a student at **XYZ** in this city.\n- üî≠ I‚Äôm currently working on  **XYZ**\n- :game: When I am not working, I like to **XYZ**\n- At the Hackweek, ask me about &lt;some areas of expertise&gt;\n\n- üåê Visit my [website](https://https://hackweek-itcoocean.github.io//) for complete background and contact."
  },
  {
    "objectID": "content/01-intro-to-cloud-geoscience.html#open-infrastructure-for-geoscience",
    "href": "content/01-intro-to-cloud-geoscience.html#open-infrastructure-for-geoscience",
    "title": "Geoscience in the Cloud",
    "section": "Open infrastructure for geoscience",
    "text": "Open infrastructure for geoscience\nLet‚Äôs start by watch a short video by James Colliander, founder of 2i2c which is supporting the ITCOocean JupyterHub. 2i2c is working to open access to cloud computing. ITCOocean is the first 2i2c JupyterHub in India (actually first in the global south)."
  },
  {
    "objectID": "content/01-intro-to-cloud-geoscience.html#why-are-we-using-a-cloud-environment",
    "href": "content/01-intro-to-cloud-geoscience.html#why-are-we-using-a-cloud-environment",
    "title": "Geoscience in the Cloud",
    "section": "Why are we using a cloud environment?",
    "text": "Why are we using a cloud environment?\n‚ÄúAnyone working with large-scale Earth System data today faces the same general problems:\n\nThe data we want to work with are huge (typical analyses involve several TB at least)\nThe data we need are produced and distributed by many different organizations (NASA, NOAA, ESGF, Copernicus, etc.)\nWe want to apply a wide range of different analysis methodologies to the data, from simple statistics to signal processing to machine learning.\n\nThe community is waking up to the idea that we can‚Äôt simply expect scientists to download all this data to their personal computers for processing.‚Äù\nRyan Abernathey, Pangeo Project.\n\n\n\nDownload-based workflow. From Abernathey, Ryan (2020): Data Access Modes in Science\n\n\n\nPangeo Link to tutorial on Pangeo ecosystem"
  },
  {
    "objectID": "content/01-intro-to-cloud-geoscience.html#cloud-optimized-geospatial-data",
    "href": "content/01-intro-to-cloud-geoscience.html#cloud-optimized-geospatial-data",
    "title": "Geoscience in the Cloud",
    "section": "Cloud optimized geospatial data",
    "text": "Cloud optimized geospatial data\n\nMethods for Generating and Testing Cloud-Optimized Geospatial Formats"
  },
  {
    "objectID": "content/01-intro-to-cloud-geoscience.html#references",
    "href": "content/01-intro-to-cloud-geoscience.html#references",
    "title": "Geoscience in the Cloud",
    "section": "References",
    "text": "References\n\nClosed Platforms vs.¬†Open Architectures for Cloud-Native Earth System Analytics\nWhy Jupyter is data scientists‚Äô computational notebook of choice\nNASA MAAP Project"
  },
  {
    "objectID": "content/01-intro-to-jupyterhub.html#log-into-the-jupyterhub",
    "href": "content/01-intro-to-jupyterhub.html#log-into-the-jupyterhub",
    "title": "Intro to JupyterHubs",
    "section": "Log into the JupyterHub",
    "text": "Log into the JupyterHub\nGo to https://itcoocean.2i2c.cloud/. Click ‚ÄúLogin to continue‚Äù. You will be asked to log in with your GitHub Account, if you are not logged in already.\n\n\n\nITCOocean JupyterHub Login\n\n\n\nImage type: Python or R\nNext you select your image type. We have two different R images and two Python images. The default is ‚ÄúGeospatial R with SDM‚Äù. This is an image with geospatial libraries and species distribution libraries.\n\n\nVirtual Machine size\nYou‚Äôll see something similar to this that allows you to choose a large virtual machine if your project needs it. For the tutorials, you will only need the Small Virtual Machine. Please only choose the large machines if you run out of RAM as the larger machines cost us more.\n\n\n\nMachine Profiles\n\n\n\n\nStart up\nAfter we select our server type and click on start, JupyterHub will allocate our instance using Amazon Web Services (AWS). This may take several minutes.\n\n\n\nJupyterhub Spawning"
  },
  {
    "objectID": "content/01-intro-to-jupyterhub.html#open-rstudio",
    "href": "content/01-intro-to-jupyterhub.html#open-rstudio",
    "title": "Intro to JupyterHubs",
    "section": "Open RStudio",
    "text": "Open RStudio\nWhen you are in the Jupyter Lab tab (note the Jupyter Logo), you will see a Launcher page. If you don‚Äôt see this, go to File &gt; New Launcher.\n\n\n\nJupyterhub Launcher\n\n\n\nOpen RStudio by clicking on the ‚ÄúRStudio‚Äù box in the Launcher tab:\n\n We will be mainly using RStudio in this course."
  },
  {
    "objectID": "content/01-intro-to-jupyterhub.html#end-your-session",
    "href": "content/01-intro-to-jupyterhub.html#end-your-session",
    "title": "Intro to JupyterHubs",
    "section": "End your session",
    "text": "End your session\nWhen you are finished working for the day it is important to log out of the JupyterHub. When you keep a session active it uses up AWS resources (costs money) and keeps a series of virtual machines deployed.\n\n\n\n\n\n\nCaution\n\n\n\nYou log out from the Jupyter Lab tab not the RStudio tab.\n\n\nFrom the Jupyter Lab tab, do one of two things to stop the server:\n\nLog out File -&gt; Log Out and click ‚ÄúLog Out‚Äù!\nor File -&gt; Hub Control Panel -&gt; Stop My Server\n\n\n\n\n\n\n\nTip\n\n\n\nCan‚Äôt find the Jupyter Lab tab? Go to https://itcoocean.2i2c.cloud/hub/home"
  },
  {
    "objectID": "content/01-intro-to-jupyterhub.html#restart-your-server",
    "href": "content/01-intro-to-jupyterhub.html#restart-your-server",
    "title": "Intro to JupyterHubs",
    "section": "Restart your server",
    "text": "Restart your server\nSometimes the server will crash/stop. This can happen if too many people use a lot of memory all at once. If that happens, go to the Jupyter Lab tab and then File -&gt; Hub Control Panel -&gt; Stop My Server and then Start My Server. You shouldn‚Äôt lose your work unless you were uploading a file."
  },
  {
    "objectID": "content/01-intro-to-jupyterhub.html#your-files",
    "href": "content/01-intro-to-jupyterhub.html#your-files",
    "title": "Intro to JupyterHubs",
    "section": "Your files",
    "text": "Your files\nWhen you start your server, you will have access to your own virtual drive space. No other users will be able to see or access your files. You can upload files to your virtual drive space and save files here. You can create folders to organize your files. You personal directory is home/jovyan. Everyone has the same home directory but your files are separate and cannot be seen by others.\nThere are a number of different ways to create new files. We will practice this in the RStudio lecture.\n\nWill I lose all of my work?\nLogging out will NOT cause any of your work to be lost or deleted. It simply shuts down some resources. It would be equivalent to turning off your desktop computer at the end of the day."
  },
  {
    "objectID": "content/01-intro-to-jupyterhub.html#shared-files",
    "href": "content/01-intro-to-jupyterhub.html#shared-files",
    "title": "Intro to JupyterHubs",
    "section": "Shared files",
    "text": "Shared files\n\n\n\nShared folder\n\n\nIn the file panel, you will see a folder called shared. These are read-only shared files that we have prepared for you. There are two main folders:\n\ndata The folder with data files\n2023-Hackbook The full Git repository with the files for https://hackweek-itcoocean.github.io/2023-Hackbook\n\nYou will also see shared-public. This is a read-write folder for you to put files for everyone to see and use. You can create a team folder here for shared data and files. Note, everyone can see and change these so be careful to communicate with your team so multiple people don‚Äôt work on the same file at the same time. You can also create folders for each team member and agree not to change other team members files.\nYou will also see other folders of Python content for the Python users. We will have a separate introduction to Jupyter Notebooks."
  },
  {
    "objectID": "content/01-intro-to-jupyterhub.html#python-users",
    "href": "content/01-intro-to-jupyterhub.html#python-users",
    "title": "Intro to JupyterHubs",
    "section": "**Python users",
    "text": "**Python users\nYou can open a Jupyter Notebook by clicking on the ‚ÄúPython 3‚Äù box. In the Launcher tab:\n\n\n\nJupyterhub Launcher\n\n\nJupyter notebooks are a very common way to share Python code and tutorials. Because we are focusing on R in this course, we will not use Jupyter notebooks much but we have many Juptyer notebooks that you can experiment with in the python-tutorials folder. Get an overview of Jupyter Lab: Intro to Jupyter Lab Learn about the geosciences tools in Python\nNote: Python users can run Python in RStudio and R users can run R in Jupyter Notebooks."
  },
  {
    "objectID": "content/01-intro-to-jupyterhub.html#faq",
    "href": "content/01-intro-to-jupyterhub.html#faq",
    "title": "Intro to JupyterHubs",
    "section": "FAQ",
    "text": "FAQ\nCan we use Matlab with JupyterHub? You can use Octave kernel as a Matlab replacement. It is open source and free. If you want to integrate Matlab, there is a project to do so jupyter-matlab-proxy\nWhy do we have the same home directory as /home/jovyan? /home/jovyan is the default home directory for ‚Äòjupyter‚Äô based images/dockers. It is the historic home directory for Jupyter deployments.\nCan other users see the files in my /home/jovyan folder? No, other users can not see your credentials.\n\nAcknowledgements\nSome sections of this document have been taken from hackweeks organized by the University of Washington eScience Institute and Openscapes."
  },
  {
    "objectID": "content/01-welcome.html",
    "href": "content/01-welcome.html",
    "title": "Welcome",
    "section": "",
    "text": "Link to slides"
  },
  {
    "objectID": "content/02-git-jupyter.html#summary",
    "href": "content/02-git-jupyter.html#summary",
    "title": "Git - Jupyter Lab",
    "section": "Summary",
    "text": "Summary\nIn this tutorial, we will provide a brief introduction to:\n\nCommand line (terminal/shell)\nNavigating around folders in Jupyter Lab\nVersion Control (code management using git)\nSetting up Git in Jupyter Lab\nThe Git GUI in Jupyter Lab\nBasic Git commands"
  },
  {
    "objectID": "content/02-git-jupyter.html#introduction-jupyter-lab",
    "href": "content/02-git-jupyter.html#introduction-jupyter-lab",
    "title": "Git - Jupyter Lab",
    "section": "Introduction :: Jupyter Lab",
    "text": "Introduction :: Jupyter Lab\nWhen you start the JupyterHub, you will be in Jupyter Lab. From there you can click on the RStudio box and open RStudio. However for this tutorial, we will stay in Juptyer Lab."
  },
  {
    "objectID": "content/02-git-jupyter.html#introduction-terminalshell",
    "href": "content/02-git-jupyter.html#introduction-terminalshell",
    "title": "Git - Jupyter Lab",
    "section": "Introduction :: Terminal/Shell",
    "text": "Introduction :: Terminal/Shell\nLog into the JupyterHub. If you do not see this\n\nThen go to File &gt; New Launcher\nClick on the ‚ÄúTerminal‚Äù box to open a new terminal window.\n\nShell or Terminal Basics\n\nWhat is Terminal or Shell?\nNavigating Files and Directories\nWorking with Files and Directories\nOptional: Detailed self-paced lesson on running scripts from the shell: Shell Lesson from Software Carpentry\n\nYou will need only basic navigation skills for this course: cd, ls and cat\n\npwd where am I\ncd nameofdir move into a directory\ncd .. move up a directory\nls list the files in the current directory\nls -a list the files including hidden files\nls -l list the files with more info\ncat filename print out the contents of a file\n\n\n\nLet‚Äôs try\nls\nls -a\ncd shared\nls\ncd shell-tutorial\ncat lesson1.sh\ncd ..\ncd ..\n\n\nClose the terminal\nJust click on the X in the terminal tab"
  },
  {
    "objectID": "content/02-git-jupyter.html#introduction-file-navigation",
    "href": "content/02-git-jupyter.html#introduction-file-navigation",
    "title": "Git - Jupyter Lab",
    "section": "Introduction :: File Navigation",
    "text": "Introduction :: File Navigation\nIn the far left, you will see a line of icons. The top one is a folder and allows us to move around our file system.\n\nClick on shared. Now you can see the files in the shared directory.\nClick on shell-tutorial. Then click on lesson1.sh. The file opens. You won‚Äôt be able to save changes here because you don‚Äôt have write permission on this drive.\nClick on the folder icon that looks like this. Click on the actual folder image. \nNow it should look like this folder /\nThis shows me doing this\n\nCreate a new folder.\n\nNext to the blue rectange with a +, is a grey folder with a +. Click that to create a new folder, called lesson-scripts.\nThen click on lesson-scripts to enter the folder\n\n\nCreate a new file\n\nCreate with File &gt; New &gt; Text file\nThe file will open and you can edit it.\nSave with File &gt; Save Text\nDelete the file by right-clicking on it and clicking ‚ÄúDelete‚Äù"
  },
  {
    "objectID": "content/02-git-jupyter.html#introduction-version-control-git",
    "href": "content/02-git-jupyter.html#introduction-version-control-git",
    "title": "Git - Jupyter Lab",
    "section": "Introduction :: Version Control (Git)",
    "text": "Introduction :: Version Control (Git)\n\nWhat is version control, git, github, and how to set it up?\nVersion control is managing and tracking changes to your documents (program source code, images, websites, data files, etc.). git is a popular tool used for version control of software code. github.com is popular platform that provides remote server hosting for git repositories. A repository is a collection of various files that you are tracking for changes and versions. Currently GitHub is the most popular platform for file sharing code and code packages.\nThis section is a step-by-step guide to set up git on our 2i2c JupyterHub. We will also configure git to use your github.com account for managing your repositories hosted on github.com. There are 5 main steps.\n\n\nStep 1: Create a GitHub account\nTo complete the setup, you will need an account on github.com. If you don‚Äôt have an account, please visit github.com, create an account (free) and come back to this guide for setting up git.\n\n\nStep 2: Clone a repository\nWe have created a demo repository for you to clone:\nhttps://github.com/Hackweek-ITCOocean/Git-Lesson\n\nStart your 2i2c JupyterHub\nClick on the Git icon\n\n\n\nClick ‚ÄúClone a Repository‚Äù\nWhere is says ‚ÄúEnter the URI of the remote Git repository‚Äù, paste in the URL https://github.com/Hackweek-ITCOocean/Git-Lesson\nThe folder appears and you can enter the folder and edit and create files.\n\n\nYour task: Create a file with your name and save to the Git-Lesson folder"
  },
  {
    "objectID": "content/02-git-jupyter.html#step-3",
    "href": "content/02-git-jupyter.html#step-3",
    "title": "Git - Jupyter Lab",
    "section": "Step 3:",
    "text": "Step 3:\nConfigure git with your name and email address.\n``` bash\ngit config --global user.name \"Makhan Virdi\"\ngit config --global user.email \"Makhan.Virdi@gmail.com\"\n```\n\n**Note:** This name and email could be different from your github.com credentials. Remember `git` is a program that keeps track of your changes locally (on 2i2c JupyterHub or your own computer) and github.com is a platform to host your repositories. However, since your changes are tracked by `git`, the email/name used in git configuration will show up next to your contributions on github.com when you `push` your repository to github.com (`git push` is discussed in a later step).\n\nConfigure git to store your github credentials to avoid having to enter your github username and token each time you push changes to your repository(in Step 5, we will describe how to use github token instead of a password)\ngit config --global credential.helper store\nCopy link for the demo repository from your github account. Click the green ‚ÄúCode‚Äù button and copy the link as shown.\n\nClone the repository using git clone command in the terminal\nTo clone a repository from github, copy the link for the repository (previous step) and use git clone:\ngit clone https://github.com/YOUR-GITHUB-USERNAME/check_github_setup\nNote: Replace YOUR-GITHUB-USERNAME here with your github.com username. For example, it is virdi for my github.com account as seen in this image.\n\nUse ls (list files) to verify the existence of the repository that you just cloned\n\nChange directory to the cloned repository using cd check_github_setup and check the current directory using pwd command (present working directory)\n\nCheck status of your git repository to confirm git set up using git status\n\nYou are all set with using git on your 2i2c JupyterHub! But the collaborative power of git through github needs some additional setup.\nIn the next step, we will create a new file in this repository, track changes to this file, and link it with your github.com account.\n\n\nStep 4. Creating new file and tracking changes\n\nIn the left panel on your 2i2c JupyterHub, click on the ‚Äúdirectory‚Äù icon and then double click on ‚Äúcheck_github_setup‚Äù directory.\n\n\nOnce you are in the check_github_setup directory, create a new file using the text editor in your 2i2c JupyterHub (File &gt;&gt; New &gt;&gt; Text File).\n\nName the file lastname.txt. For example, virdi.txt for me (use your last name). Add some content to this file (for example, I added this to my virdi.txt file: my last name is virdi).\n\nNow you should have a new file (lastname.txt) in the git repository directory check_github_setup\nCheck if git can see that you have added a new file using git status. Git reports that you have a new file that is not tracked by git yet, and suggests adding that file to the git tracking system.\n\nAs seen in this image, git suggests adding that file so it can be tracked for changes. You can add file to git for tracking changes using git add. Then, you can commit changes to this file‚Äôs content using git commit as shown in the image.\ngit add virdi.txt\ngit status\ngit commit -m \"adding a new file\"\ngit status\n\nAs seen in the image above, git is suggesting to push the change that you just committed to the remote server at github.com (so that your collaborators can also see what changes you made).\nNote: DO NOT execute push yet. Before we push to github.com, let‚Äôs configure git further and store our github.com credentials to avoid entering the credentials every time we invoke git push. For doing so, we need to create a token on github.com to be used in place of your github.com password.\n\n\n\nStep 5. Create access token on github.com\n\nGo to your github account and create a new ‚Äúpersonal access token‚Äù: https://github.com/settings/tokens/new\n\n\n\nGenerate Personal Access Token on github.com\n\n\nEnter a description in ‚ÄúNote‚Äù field as seen above, select ‚Äúrepo‚Äù checkbox, and scroll to the bottom and click the green button ‚ÄúGenerate Token‚Äù. Once generated, copy the token (or save it in a text file for reference).\nIMPORTANT: You will see this token only once, so be sure to copy this. If you do not copy your token at this stage, you will need to generate a new token.\n\nTo push (transfer) your changes to github, use git push in terminal. It requires you to enter your github credentials. You will be prompted to enter your github username and ‚Äúpassword‚Äù. When prompted for your ‚Äúpassword‚Äù, DO NOT use your github password, use the github token that was copied in the previous step.\ngit push\n\nNote: When you paste your token in the terminal window, windows users will press Ctrl+V and mac os users will press Cmd+V. If it does not work, try generating another token and use the copy icon next to the token to copy the token. Then, paste using your computer‚Äôs keyboard shortcut for paste.\nNow your password is stored in ~/.git-credentials and you will not be prompted again unless the Github token expires. You can check the presence of this git-credentials file using Terminal. Here the ~ character represents your home directory (/home/jovyan/).\nls -la ~\nThe output looks like this:\ndrwxr-xr-x 13 jovyan jovyan 6144 Oct 22 17:35 .\ndrwxr-xr-x  1 root   root   4096 Oct  4 16:21 ..\n-rw-------  1 jovyan jovyan 1754 Oct 29 18:30 .bash_history\ndrwxr-xr-x  4 jovyan jovyan 6144 Oct 29 16:38 .config\n-rw-------  1 jovyan jovyan   66 Oct 22 17:35 .git-credentials\n-rw-r--r--  1 jovyan jovyan   84 Oct 22 17:14 .gitconfig\ndrwxr-xr-x 10 jovyan jovyan 6144 Oct 21 16:19 2021-Cloud-Hackathon\nYou can also verify your git configuration\n(notebook) jovyan@jupyter-virdi:~$ git config -l\nThe output should have credential.helper = store:\nuser.email        = Makhan.Virdi@gmail.com\nuser.name         = Makhan Virdi\ncredential.helper = store\n\nNow we are all set to collaborate with github on the JupyterHub during the Cloud Hackathon!\n\n\nSummary: Git Commands\n\nCommonly used git commands (modified from source)\n\n\nGit Command\nDescription\n\n\n\n\ngit status\nShows the current state of the repository: the current working branch, files in the staging area, etc.\n\n\ngit add\nAdds a new, previously untracked file to version control and marks already tracked files to be committed with the next commit\n\n\ngit commit\nSaves the current state of the repository and creates an entry in the log\n\n\ngit log\nShows the history for the repository\n\n\ngit diff\nShows content differences between commits, branches, individual files and more\n\n\ngit clone\nCopies a repository to your local environment, including all the history\n\n\ngit pull\nGets the latest changes of a previously cloned repository\n\n\ngit push\nPushes your local changes to the remote repository, sharing them with others\n\n\n\n\n\nGit: More Details\nLesson: For a more detailed self-paced lesson on git, visit Git Lesson from Software Carpentry\nCheatsheet: Frequently used git commands\nDangit, Git!?!: If you are stuck after a git mishap, there are ready-made solutions to common problems at Dangit, Git!?!\n\n\nCloning our repository using the git Jupyter lab extension.\nIf we‚Äôre already familiar with git commands and feel more confortable using a GUI our Jupyterhub deployment comes with a git extension. This plugin allows us to operate with git using a simple user interface.\nFor example we can clone our repository using the extension.\n\n\n\ngit extension"
  },
  {
    "objectID": "content/02-github-2.html#import-a-github-repository",
    "href": "content/02-github-2.html#import-a-github-repository",
    "title": "GitHub - Lecture 2",
    "section": "Import a GitHub repository",
    "text": "Import a GitHub repository\n\nIn your browser, navigate to https://github.com/Hackweek-ITCOocean/Quarto-Website\nCopy the URL\nClick on the ‚Äú+‚Äù in the top right (on GitHub) and select ‚Äúimport repository‚Äù\nFollow instructions to import to your GitHub account\nNow you have your OWN copy of Quarto-Website\nCopy the URL for your OWN copy. It will look like this\n\nhttps://github.com/yourgithubusername/Quarto-Website"
  },
  {
    "objectID": "content/02-github-2.html#clone-a-github-repository-into-rstudio",
    "href": "content/02-github-2.html#clone-a-github-repository-into-rstudio",
    "title": "GitHub - Lecture 2",
    "section": "Clone a GitHub repository into RStudio",
    "text": "Clone a GitHub repository into RStudio\n\nLogin to the JupyterHub. You might have to wait awhile for the page to load.\nClick on the RStudio button when the Launcher appears \nClick on the tab that opens with the R in a circle. You should see this \nClick File &gt; New Project &gt; Version Control &gt; Git\nPaste the URL into the box\nClick ‚ÄúCreate Project‚Äù\nThe project is created and the Git tab appears in the upper right panel.\n\nCongratulations! You pulled changes from GitHub into the JupyterHub!"
  },
  {
    "objectID": "content/02-github-2.html#lets-explore-the-files",
    "href": "content/02-github-2.html#lets-explore-the-files",
    "title": "GitHub - Lecture 2",
    "section": "Let‚Äôs explore the files",
    "text": "Let‚Äôs explore the files\n\nThis happens to be a website. Let‚Äôs build it using the Build tab in the upper right panel.\nClick ‚ÄúRender Website‚Äù. The website should build and appear.\n\n\nIf it doesn‚Äôt open automatically, go to docs &gt; index.html, right click and open that in a browser.\n\n\nOpen the about.qmd file. Let‚Äôs edit that! You can use the ‚ÄúSource‚Äù or ‚ÄúVisual‚Äù editor.\nWhen you have made some edits, click ‚ÄúRender‚Äù to see how it looks.\nYou can also click Build &gt; Render Website to re-create the whole website."
  },
  {
    "objectID": "content/02-github-2.html#set-up-authentication",
    "href": "content/02-github-2.html#set-up-authentication",
    "title": "GitHub - Lecture 2",
    "section": "Set up authentication",
    "text": "Set up authentication\nYou need to tell GitHub who you are so you can push your local changes up to GitHub. There are a few ways to do this. I am going to show you a way that works on any computer, including a virtual computer like the JupyterHub.\n\nStep 1: Generate a Personal Access Token\nWe are going to generate a classic token.\n\nGo to https://github.com/settings/tokens\nClick Generate new token &gt; Generate new token (classic)\nWhen the pop-up shows up, fill in a description, click the ‚Äúrepo‚Äù checkbox, and then scroll to bottom to click ‚ÄúGenerate‚Äù.\nSAVE the token. You need it for the next step.\n\n\n\nStep 2: Tell Git who your are\n\nReturn to RStudio\nClick the terminal tab in the bottom right panel\nPaste these 3 lines of code into the terminal\n\ngit config --global user.email \"&lt;your email&gt;\"\ngit config --global user.name \"&lt;your name&gt;\"\ngit config --global pull.rebase false\ngit config --global credential.helper store\n\n\nStep 3: Push changes up to GitHub\n\nClick the Git tab (upper right panel)\nClick the checkboxes next to what changes you want to push\nClick ‚ÄúCommit‚Äù\nFill out a comment and click ‚ÄúCommit‚Äù\nClose the pop-ups\nClick Push\n\nFull instructions with ways to do this from R"
  },
  {
    "objectID": "content/02-github-2.html#optional-make-your-website-live",
    "href": "content/02-github-2.html#optional-make-your-website-live",
    "title": "GitHub - Lecture 2",
    "section": "Optional: Make your website live!",
    "text": "Optional: Make your website live!\nFor those who get through all the steps and want to play around more.\n\nGo to your Quarto Website repository on Github\nGo to Settings &gt; Pages (in left navbar)\nYou need to select the branch and the folder where the webpage files are. In this case, they are in docs \nThe URL where your website will appear is shown.\n\nHave you noticed that people have personal websites at username.github.io? How did they do that? If you create a repo named username.github.io in GitHub, it will automatically appear with that URL. Look for a repo with that name in someone‚Äôs GitHub account if you see that they use that as their personal website."
  },
  {
    "objectID": "content/02-quarto.html",
    "href": "content/02-quarto.html",
    "title": "Intro to Quarto",
    "section": "",
    "text": "Quarto was announced at the 2022 RStudio Conference in this great keynote talk: Mine √áetinkaya-Rundel & Julia Stewart Lowndes | Hello Quarto. I am going to start by showing you 2 clips from this keynote."
  },
  {
    "objectID": "content/02-quarto.html#what-does-quarto-and-rmarkdown-do",
    "href": "content/02-quarto.html#what-does-quarto-and-rmarkdown-do",
    "title": "Intro to Quarto",
    "section": "What does Quarto (and RMarkdown) do?",
    "text": "What does Quarto (and RMarkdown) do?\n\nQuarto is bundled and comes pre-installed with RStudio v2022.07.1 and beyond!"
  },
  {
    "objectID": "content/02-quarto.html#lets-learn-by-creating",
    "href": "content/02-quarto.html#lets-learn-by-creating",
    "title": "Intro to Quarto",
    "section": "Let‚Äôs learn by creating!",
    "text": "Let‚Äôs learn by creating!\n\nOpen up the JupyterHub and navigate to RStudio.\nPlan B. You can open up RStudio on your laptop.\n\n\nCreate a basic Quarto document\nFile &gt; New File &gt; Quarto document\n\n\nCreate a basic Quarto presentation\nFile &gt; New File &gt; Quarto presentation\n\n\nCreate a basic Quarto book\nFile &gt; New Project &gt; Quarto book\n\n\nMake a bit more fancy Quarto book\nActivity for advanced R/GitHub users\n\nGo here https://github.com/nmfs-opensci/NOAA-quarto-book\nClick the ‚ÄúUse this template‚Äù button and make copy in your GitHub account and give it a different name. Make sure to click the checkbox to copy all branches. Change owner to your account. \nCopy YOUR repo URL. It‚Äôll look like https://github.com/yourusername/my-quarto-book\nFile &gt; New Project &gt; Version Control and paste in your URL\nClick Create and then you can click the Build tab and render the book.\nMake some changes and push back to GitHub.\nTurn on GitHub Pages via Settings &gt; Pages and selecting the gh-pages branch. \nYour book will automatically be published to the web."
  },
  {
    "objectID": "content/02-quarto.html#intro-to-quarto-workshops",
    "href": "content/02-quarto.html#intro-to-quarto-workshops",
    "title": "Intro to Quarto",
    "section": "Intro to Quarto Workshops",
    "text": "Intro to Quarto Workshops\nThomas Mocke (Posit) has great videos on Quarto. I am using material from a 2-hour online workshop.\n\nContent\nVideo\nGitHub repo\n\nHe also gave a 2-day workshop if you want to learn Quarto indepth.\n\n2 day workshop\n\nMore workshop that you can watch and join on Posit YouTube"
  },
  {
    "objectID": "content/02-rstudio.html#open-rstudio-in-the-jupyterhub",
    "href": "content/02-rstudio.html#open-rstudio-in-the-jupyterhub",
    "title": "RStudio - R",
    "section": "Open RStudio in the JupyterHub",
    "text": "Open RStudio in the JupyterHub\n\nLogin the JupyterHub\nClick on the RStudio button when the Launcher appears \nLook for the browser tab with the RStudio icon"
  },
  {
    "objectID": "content/02-rstudio.html#basic-navigation",
    "href": "content/02-rstudio.html#basic-navigation",
    "title": "RStudio - R",
    "section": "Basic Navigation",
    "text": "Basic Navigation\n\n\n\nRStudio Panels"
  },
  {
    "objectID": "content/02-rstudio.html#create-an-rstudio-project",
    "href": "content/02-rstudio.html#create-an-rstudio-project",
    "title": "RStudio - R",
    "section": "Create an RStudio project",
    "text": "Create an RStudio project\n\nOpen RStudio\nIn the file panel, click on the Home icon to make sure you are in your home directory\nFrom the file panel, click ‚ÄúNew Project‚Äù to create a new project\nIn the pop up, select New Directory and then New Project\nName it sandbox\nClick on the dropdown in the upper right corner to select your sandbox project\nClick on Tools &gt; Project Options &gt; General and change the first 2 options about saving and restoring the workspace to ‚ÄúNo‚Äù"
  },
  {
    "objectID": "content/02-rstudio.html#installing-packages",
    "href": "content/02-rstudio.html#installing-packages",
    "title": "RStudio - R",
    "section": "Installing packages",
    "text": "Installing packages\nIn the bottom right panel, select the Packages tab, click install and then start typing the name of the package. Then click Install.\nThe JupyterHub comes with many packages already installed so you shouldn‚Äôt have to install many packages.\nWhen you want to use a package, you first need to load it with\nlibrary(hello)\nYou will see this in the tutorials. You might also see something like\nhello::thefunction()\nThis is using thefunction() from the hello package.\n\n\n\n\n\n\nNote\n\n\n\nPython users. In R, you will always call a function like funtion(object) and never like object.function(). The exception is something called ‚Äòpiping‚Äô in R, which I have never seen in Python. In this case you pass objects left to right. Like object %&gt;% function(). Piping is very common in modern R but you won‚Äôt see it much in R from 10 years ago."
  },
  {
    "objectID": "content/02-rstudio.html#uploading-and-downloading-files",
    "href": "content/02-rstudio.html#uploading-and-downloading-files",
    "title": "RStudio - R",
    "section": "Uploading and downloading files",
    "text": "Uploading and downloading files\nNote, Upload and download is only for the JupyterHub not on RStudio on your computer.\n\nUploading is easy.\nLook for the Upload button in the Files tab of the bottom right panel.\n\n\nDownload is less intuitive.\n\nClick the checkbox next to the file you want to download. One only.\nClick the ‚Äúcog‚Äù icon in the Files tab of the bottom right panel. Then click Export."
  },
  {
    "objectID": "content/02-rstudio.html#creating-files",
    "href": "content/02-rstudio.html#creating-files",
    "title": "RStudio - R",
    "section": "Creating files",
    "text": "Creating files\nWhen you start your server, you will have access to your own virtual drive space. No other users will be able to see or access your files. You can upload files to your virtual drive space and save files here. You can create folders to organize your files. You personal directory is home/rstudio. Everyone has the same home directory but your files are separate and cannot be seen by others.\nPython users: If you open a Python image instead of the R image, your home is home/jovyan.\nThere are a number of different ways to create new files. Let‚Äôs practice making new files in RStudio.\n\nR Script\n\nOpen RStudio\nIn the upper right, make sure you are in your sandbox project.\nFrom the file panel, click on ‚ÄúNew Blank File‚Äù and create a new R script.\nPaste\n\nprint(\"Hello World\")\n1+1\nin the script. 7. Click the Source button (upper left of your new script file) to run this code. 8. Try putting your cursor on one line and running that line of code by clicking ‚ÄúRun‚Äù 9. Try selecting lines of code and running that by clicking ‚ÄúRun‚Äù\n\n\ncsv file\n\nFrom the file panel, click on ‚ÄúNew Blank File‚Äù and create a Text File.\nThe file will open in the top left corner. Paste in the following:\n\nname, place, value\nA, 1, 2\nB, 10, 20\nC, 100, 200\n\nClick the save icon (above your new file) to save your csv file\n\n\n\nA Rmarkdown document\nNow let‚Äôs create some more complicated files using the RStudio template feature.\n\nFrom the upper left, click File -&gt; New File -&gt; RMarkdown\nClick ‚ÄúOk‚Äù at the bottom.\nWhen the file opens, click Knit (icon at top of file).\nIt will ask for a name. Give it one and save.\nYou file will render into html.\n\nReference sheet for writing in RMarkdown or go to Help &gt; Markdown Quick Reference\n\n\nA Rmarkdown presentation\n\nFrom the upper left, click File -&gt; New File -&gt; RMarkdown\nClick ‚ÄúPresentation‚Äù on left of the popup and click ‚ÄúOk‚Äù at the bottom.\nWhen the file opens, click Knit (icon at top of file).\nIt will ask for a name. Give it one and save.\nYou file will render into html.\n\n\n\n(advanced) An interactive application\n\nFrom the upper left, click File -&gt; New File -&gt; Shiny Web App\nIn the popup, give the app a name and make sure the app is saved to my-files\nWhen the file opens, Run App (icon at top of file).\n\n\n\nAnd many more\nPlay around with creating other types of documents using templates. Especially if you already use RStudio."
  },
  {
    "objectID": "content/02-rstudio.html#more-tips",
    "href": "content/02-rstudio.html#more-tips",
    "title": "RStudio - R",
    "section": "More tips",
    "text": "More tips\nLearn some tips and tricks from these\n\nhttps://colorado.posit.co/rsc/the-unknown/into-the-unknown.html\nhttps://www.dataquest.io/blog/rstudio-tips-tricks-shortcuts/"
  },
  {
    "objectID": "content/02-rstudio.html#plotting-a-netcdf-file",
    "href": "content/02-rstudio.html#plotting-a-netcdf-file",
    "title": "RStudio - R",
    "section": "Plotting a netCDF file",
    "text": "Plotting a netCDF file\n\nhttps://pjbartlein.github.io/REarthSysSci/netCDF.html\nhttps://r-spatial.github.io/sf/articles/sf1.html\n\nwebpage:\nhttps://coastwatch.pfeg.noaa.gov/erddap/griddap/ncdcOisst21Agg.graph?sst%5B(2023-08-27T12:00:00Z)%5D%5B(0.0)%5D%5B(-7.8):(44.8)%5D%5B(39.7):(92.3)%5D&.draw=surface&.vars=longitude%7Clatitude%7Csst&.colorBar=%7C%7C%7C%7C%7C&.bgColor=0xffccccff\nurl from the dropdown on that page\nurl &lt;- https://coastwatch.pfeg.noaa.gov/erddap/griddap/ncdcOisst21Agg.nc?sst%5B(2023-08-27T12:00:00Z)%5D%5B(0.0)%5D%5B(-7.875):(44.875)%5D%5B(39.625):(92.375)%5D&.draw=surface&.vars=longitude%7Clatitude%7Csst&.colorBar=%7C%7C%7C%7C%7C&.bgColor=0xffccccff\n\nOpen an R script\n\nAdd this code.\n\nlibrary(ggplot2) # package for plotting\nlibrary(sf)\nlibrary(stars)\nlibrary(dplyr)\n\nurl &lt;- \"https://coastwatch.pfeg.noaa.gov/erddap/griddap/ncdcOisst21Agg.nc?sst%5B(2023-08-27T12:00:00Z)%5D%5B(0.0)%5D%5B(-7.875):(44.875)%5D%5B(39.625):(92.375)%5D&.draw=surface&.vars=longitude%7Clatitude%7Csst&.colorBar=%7C%7C%7C%7C%7C&.bgColor=0xffccccff\"\n\nfil &lt;- \"sst.nc\"\nif(!exists(fil)){\n  download.file(url=url, destfile=fil)\n}\n\nstars_object &lt;- raster::raster(fil) %&gt;% st_as_stars()\nggplot() + geom_stars(data = stars_object)"
  },
  {
    "objectID": "content/03-ai.html",
    "href": "content/03-ai.html",
    "title": "AI Assisted Programming",
    "section": "",
    "text": "2023 has seen an dramatic shift toward a new workflow for programming and learning to program: AI-Assisted Programming. Using AI as your personal coding buddy and coach is the new way to work. But you need to learn how to interact with your AI helper and ask it questions.\nWe are going to watch a presentation given by Myranda Shirk, Senior Data Scientist, Vanderbilt University, Knoxville, US. This presentation was given at OceanHackWeek 2023 at the University of Washington in August 2023. Myranda will give you an overview of the current AI platforms and then demo how to interact with them. During this hackweek, you get a lot of practice with AI assisted programming and AI assisted brainstorming.\nOne important thing to note is that the chatGPT tool that Myranda used (code interpreter) is now called Advanced Data Analytics.\nVideo: https://youtu.be/JCHa-yGgtr4?si=WV956qZkSEVvy-CQ\nLecture Notes: https://oceanhackweek.org/ohw23/tutorials/02-Wed/ai_assisted_programming_ohw.html"
  },
  {
    "objectID": "content/03-git-rstudio.html#what-is-git-and-github",
    "href": "content/03-git-rstudio.html#what-is-git-and-github",
    "title": "Basic Git/GitHub Skills Using RStudio",
    "section": "What is Git and GitHub?",
    "text": "What is Git and GitHub?\nGit A program to track your file changes and create a history of those changes. Creates a ‚Äòcontainer‚Äô for a set of files called a repository.\nGitHub A website to host these repositories and allow you to sync local copies (on your computer) to the website. Lots of functionality built on top of this."
  },
  {
    "objectID": "content/03-git-rstudio.html#some-basic-git-jargon",
    "href": "content/03-git-rstudio.html#some-basic-git-jargon",
    "title": "Basic Git/GitHub Skills Using RStudio",
    "section": "Some basic Git jargon",
    "text": "Some basic Git jargon\n\nRepo Repository. It is your code and the record of your changes. This record and also the status of your repo is a hidden folder called .git . You have a local repo and a remote repo. The remote repo is on GitHub (for in our case) is called origin. The local repo is on the JupyterHub.\nStage Tell Git which changes you want to commit (write to the repo history).\nCommit Write a note about what change the staged files and ‚Äúcommit‚Äù that note to the repository record. You are also tagging this state of the repo and you could go back to this state if you wanted.\nPush Push local changes (commits) up to the remote repository on GitHub (origin).\nPull Pull changes on GitHub into the local repository on the JupyterHub.\nGit GUIs A graphical interface for Git (which is command line). Today I will use jupyterlab-git which we have installed on JupyterHub.\nShell A terminal window where we can issue git commands."
  },
  {
    "objectID": "content/03-git-rstudio.html#overview",
    "href": "content/03-git-rstudio.html#overview",
    "title": "Basic Git/GitHub Skills Using RStudio",
    "section": "Overview",
    "text": "Overview\nToday I will cover the four basic Git/GitHub skills. The goal for today is to first get you comfortable with the basic skills and terminology. We will use what is called a ‚Äútrunk-based workflow‚Äù.\n\nSimple Trunk-based Workflow:\n\nMake local (on your computer) changes to code.\nRecord what those changes were about and commit to the code change record (history).\nPush those changes to your remote repository (aka origin)\n\nWe‚Äôll do this"
  },
  {
    "objectID": "content/03-git-rstudio.html#setting-up-git",
    "href": "content/03-git-rstudio.html#setting-up-git",
    "title": "Basic Git/GitHub Skills Using RStudio",
    "section": "Setting up Git",
    "text": "Setting up Git\nYou should have gotten this done on Tuesday but if not here are the instructions\nBefore we can work with Git in the JupyterHub, we need to do some set up.\n\nTell Git who you are and to store your credentials (GitHub login info)\n\nShow me\nPaste this into a terminal window:\ngit config --global user.email \"&lt;your email&gt;\"\ngit config --global user.name \"&lt;your name&gt;\"\ngit config --global pull.rebase false\ngit config --global credential.helper store\n\nGet a Personal Access Token from GitHub\n\nCopy the token! You will need it in the next step.\nShow me Note, one change to this video is that you need to specify that you want a classic token.\n\nTrigger Git to ask for your password (that personal access token)\n\nYou can do this by cloning a private repo. In the Terminal, issue this command\ngit clone https://github.com/Hackweek-ITCOocean/github_setup_check\nIt will ask for your GitHub username and password. At the password part, paste in the Personal Access Token."
  },
  {
    "objectID": "content/03-git-rstudio.html#git-tab",
    "href": "content/03-git-rstudio.html#git-tab",
    "title": "Basic Git/GitHub Skills Using RStudio",
    "section": "Git tab",
    "text": "Git tab\nWhen the instructions say to use or open or click the Git tab,"
  },
  {
    "objectID": "content/03-git-rstudio.html#the-key-skills",
    "href": "content/03-git-rstudio.html#the-key-skills",
    "title": "Basic Git/GitHub Skills Using RStudio",
    "section": "The Key Skills",
    "text": "The Key Skills\n\nSkill 1: Create a blank repo on GitHub\nSkill 2: Clone your GitHub repo to RStudio\nSkill 3: Make some changes and commit those local changes\nSkill 4: Push the changes to GitHub\nSkill 1b: Copy someone else‚Äôs GitHub repository"
  },
  {
    "objectID": "content/03-git-rstudio.html#lets-see-it-done",
    "href": "content/03-git-rstudio.html#lets-see-it-done",
    "title": "Basic Git/GitHub Skills Using RStudio",
    "section": "Let‚Äôs see it done!",
    "text": "Let‚Äôs see it done!\n\nSkill 1: Create a blank repo on GitHub\n\nClick the + in the upper left from YOUR GitHub page.\nGive your repo the name Test and make sure it is public.\nClick new and check checkbox to add the Readme file and .gitignore\nCopy the URL of your new repo. It‚Äôs in the browser where you normally see a URL.\n\nShow me\n\n\nSkill 2: Clone your repo to the RStudio\nIn RStudio we do this by making a new project.\n\nCopy the URL of your repo. https://www.github.com/yourname/Test\nFile &gt; New Project &gt; Version Control &gt; Git\nPast in the URL of your repo from Step 1\nCheck that it is being created in your Home directory which will be denoted ~ in the JupyterHub.\nClick Create.\n\nShow me\n\n\nSkill 3: Make some changes and commit your changes\nThis writes a note about what changes you have made. It also marks a ‚Äòpoint‚Äô in time that you can go back to if you need to.\n\nMake some changes to the README.md file in the Test repo.\nClick the Git tab, and stage the change(s) by checking the checkboxes next to the files listed.\nClick the Commit button.\nAdd a commit comment, click commit.\n\nShow me\n\n\nSkill 4: Push changes to GitHub / Pull changes from GitHub\nTo push changes you committed in Skill #3\n\nFrom Git tab, click on the Green up arrow that says Push.\nTo pull changes on GitHub that are not on your local computer:\nMake some changes directly on GitHub (not in RStudio)\nFrom Git tab, click on the down arrow that says Pull.\n\nShow me\n\n\nPair-activity 1\nIn RStudio,\n\nMake a copy of README.md\nRename it to .md\nAdd some text.\nStage and commit the added file.\nPush to GitHub.\n\nTry before watching.\nShow me in RStudio ‚Äì Show me in the shell ‚Äì Show me in jupyter-git\n\n\nPair-activity 2\nAll of this activity is in RStudio.\n\nClone this repo https://github.com/Hackweek-ITCOocean/git-basics to RStudio and create a new project\nNavigate to the files in your new project, create a filed called to &lt;yourname&gt;.md. Use your actual name so the filename is different from everyone elses.\nStage and then commit that new file.\nPush to GitHub.\nMake some more changes and push to GitHub.\nPull in your partner‚Äôs (and everyone elses) changes\n\nShow me in RStudio ‚Äì Show me in JupyterLab\n\n\nPair-activity 3\nYou can copy your own or other people‚Äôs repos1.\n\nIn a browser, go to the GitHub repository https://github.com/RWorkflow-Workshops/Week5\nCopy its URL.\nNavigate to your GitHub page: click your icon in the upper right and then ‚Äòyour repositories‚Äô\nClick the + in top right and click import repository. Paste in the URL and give your repo a name.\nUse Skill #1 to clone your new repo to RStudio and create a new project"
  },
  {
    "objectID": "content/03-git-rstudio.html#footnotes",
    "href": "content/03-git-rstudio.html#footnotes",
    "title": "Basic Git/GitHub Skills Using RStudio",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is different from forking. There is no connection to the original repository.‚Ü©Ô∏é"
  },
  {
    "objectID": "content/04-intro-sdm.html#big-picture",
    "href": "content/04-intro-sdm.html#big-picture",
    "title": "Intro to SDM",
    "section": "Big Picture",
    "text": "Big Picture\nIn the broader context, our goal is to uncover where species are likely to be found, not just for their protection but also to strategically plan conservation efforts and to help manage fisheries. We aim to discover where these species are located and anticipate how their habitats might shift in the future.\nBut handling all this data manually is virtually impossible. It‚Äôs like trying to piece together a complex puzzle with millions of tiny pieces.\nSpecies Distribution Modeling (SDM) combined with big data analytics comes into play. SDM helps us make sense of this vast environmental data. We can extract meaningful patterns and relationships between the environment and species distribution."
  },
  {
    "objectID": "content/04-intro-sdm.html#nasa-arset",
    "href": "content/04-intro-sdm.html#nasa-arset",
    "title": "Intro to SDM",
    "section": "NASA ARSET",
    "text": "NASA ARSET\nI am going to be showing some slides from NASA ARSET‚Äô Training Course on Species Distribution Modeling (here. This is a 4.5 hour webinar focusing on an R package called Wallace, which I won‚Äôt be talking about as it is more focused on land SDMs. But the first lecture gives a good general overview of SDMs.\nMcCullum, A.; Torres-P√©rez, J.; Bengtsson, Z.; Johnson, E.; Paz Velez, A.; Blair, M. (2021). Species Distribution Modeling with Remote Sensing. NASA Applied Remote Sensing Training Program (ARSET). https://appliedsciences.nasa.gov/join-mission/training/english/arset-species-distribution-modeling-remote-sensing\nNASA ARSET is the NASA Applied Remote Sensing Training (ARSET) Program and they have virtual and on-line trainings for using remote-sensing data. Their Fundamentals of Remote Sensing Course is a good overview."
  },
  {
    "objectID": "content/04-intro-sdm.html#slides",
    "href": "content/04-intro-sdm.html#slides",
    "title": "Intro to SDM",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "content/XX-further-resources.html#notebooks-and-hackweeks-with-tutorials",
    "href": "content/XX-further-resources.html#notebooks-and-hackweeks-with-tutorials",
    "title": "Additional resources",
    "section": "Notebooks and hackweeks with tutorials",
    "text": "Notebooks and hackweeks with tutorials\n\nPangeo at Ocean Sciences 2020\nNASA Openscapes EarthData Cloud Book\nNASA Openscapes 2021 Hackweek\nOceanHackWeek\nICESat-2 2023 eScience Institute, University of Washington website\nPO.DAAC Github Repository\nNASA MAAP Notebooks Click the GitHub link in top right to get to the Jupyter Notebooks\nUW eScience Geoscience Hackweek"
  },
  {
    "objectID": "content/XX-further-resources.html#how-to",
    "href": "content/XX-further-resources.html#how-to",
    "title": "Additional resources",
    "section": "How to",
    "text": "How to\n\nNASA Earthdata: How to Cloud\nUSGS Eyes on Earth Podcast: Satellites and Cloud Computing - with Aaron Friesz (LP DAAC!)\nPO.DAAC Cloud Data Page\nPO.DAAC Earthdata Webinar (Aug 2021): Surfing Ocean Data in the Cloud - The Beginner‚Äôs Guide to PO.DAAC in the NASA Earthdata Cloud\nNASA Earthdata Cloud Primer helpful tutorials for how to set up your own EC2 cloud instance in AWS, attach storage, move files back and forth, and more.\nSetting up Jupyter Notebooks in a user EC2 instance in AWS - blog post for setting up jupyter notebooks in an EC2 instance in AWS. (Builds on the Cloud Primer tutorials, which are missing that next step)\nRunning the NASA Cloud Workshop notebooks with mybinder.org - by Eli Holmes, 2021 Cloud Hackathon Participant who then set up working in Binder"
  },
  {
    "objectID": "content/XX-r-and-python-resources.html#programming-in-r",
    "href": "content/XX-r-and-python-resources.html#programming-in-r",
    "title": "R and Python Resources",
    "section": "Programming in R",
    "text": "Programming in R\nThe following give a basic intro to R and RStudio.\n\nW3Schools Basic R\nBasics of R and installing RStudio\nYouTube: R for Beginners\n\n\nBeyond the basics of R\n\nSoftware Carpentry Novice R\nSoftware Carpentry Intro to R\nFor a longer free course, CodeAcademy has a good Intro to R. Note this is 20 hours. You do NOT need to do this whole course before the hackweek!\nIf you know the basics of R but have never done modeling, then you can start with this 2.5 hour course that covers basic linear regression with R and plotting with ggplot2:\nOne of the best free R courses is Coursera‚Äôs Learn R. You can see the lectures for free. Make sure to click AUDIT when it asks you to sign up for a 7-day trial. You do not need to sign up for anything to go through the material."
  },
  {
    "objectID": "content/XX-r-and-python-resources.html#programming-in-python",
    "href": "content/XX-r-and-python-resources.html#programming-in-python",
    "title": "R and Python Resources",
    "section": "Programming in Python",
    "text": "Programming in Python\nSwitch to Jupyter Notebook for an introduction to programming in Python\n\nVariables (and mathematical operations)\nData Structures (list, tuple, dict)\nFlow Control using loops (for, while)\nConditionals (if, else, elif)\nFunctions\nErrors and Exceptions (understanding and handling errors)\nUsing modules (libraries, packages)\n\npandas: high-performance, easy-to-use data structures and data analysis tools\nrioxarray: based on the rasterio package for working with rasters and xarray\n\n\n\nBeyond the basics in Python\n\nSelf-paced lesson on Programming with Python from Software Carpentry\nDetailed self-paced lesson on Fundamentals of Geospatial Raster and Vector Data with Python from Data Carpentry"
  },
  {
    "objectID": "content/data.html",
    "href": "content/data.html",
    "title": "Data",
    "section": "",
    "text": "Indian Ocean and Bay of Bengal Data\nStudents will learn how to access remote-sensing data in the cloud (meaning on-line), but we have also prepared an ‚Äúanalysis ready‚Äù data set for students. This will be available on a shared drive and we will introduce the students to the techniques for accessing large datasets without loading them into memory.\n\n\nBounding box\n\nlatitude: -12deg to 32deg\nlongitude: 42deg to 102deg\ngrid: 0.25 deg\ncenters: 0, 0.25, 0.5., 0.75\n\n\n\nDatasets: ERA5\n\nu wind\nv wind\nwind speed\nwind direction\nair temperature @ 2m\nsea surface temperature\n\n\n\nCopernicus\n\nsea level anomaly\nchlorophyll concentration\n\n\n\nSRTM30_PLUS\n\nBathymetry\n\nReferences\n\nhttps://topex.ucsd.edu/WWW_html/srtm30_plus.html\nhttps://coastwatch.pfeg.noaa.gov/erddap/griddap/usgsCeSrtm30v6.html\n\n\n\n1-km MUR SST\n\nsea surface temperature"
  },
  {
    "objectID": "content/earthdata.html#overview",
    "href": "content/earthdata.html#overview",
    "title": "Earthdata Login",
    "section": "Overview",
    "text": "Overview"
  },
  {
    "objectID": "content/earthdata.html#why-do-i-need-an-earthdata-login",
    "href": "content/earthdata.html#why-do-i-need-an-earthdata-login",
    "title": "Earthdata Login",
    "section": "Why do I need an Earthdata login?",
    "text": "Why do I need an Earthdata login?\nWe will be teaching you ways to programmatically access NASA remote-sensing data from within your scripts. You will need to enter your Earthdata username and password in order for this to work."
  },
  {
    "objectID": "content/earthdata.html#getting-an-earthdata-login",
    "href": "content/earthdata.html#getting-an-earthdata-login",
    "title": "Earthdata Login",
    "section": "Getting an Earthdata login",
    "text": "Getting an Earthdata login\nIf you do not already have an Earthdata login, then navigate to the Earthdata Login page, a username and password, and then record this somewhere for use during the tutorials:"
  },
  {
    "objectID": "content/earthdata.html#configure-programmatic-access-to-nasa-servers",
    "href": "content/earthdata.html#configure-programmatic-access-to-nasa-servers",
    "title": "Earthdata Login",
    "section": "Configure programmatic access to NASA servers",
    "text": "Configure programmatic access to NASA servers\nIf you use web interfaces to retrieve nasa data such as Earthdata Search you are prompted to login. We will be using software to retrieve data from NASA Servers during the hackweek, so you must store your credentials on the JupyterHub. Run the following commands on the JupyterHub in a terminal replacing your Earthdata login username and password:\necho \"machine urs.earthdata.nasa.gov login EARTHDATA_LOGIN password EARTHDATA_PASSWORD\" &gt; ~/.netrc\nchmod 0600 .netrc"
  },
  {
    "objectID": "content/git.html#summary",
    "href": "content/git.html#summary",
    "title": "Git - Jupyter Lab",
    "section": "Summary",
    "text": "Summary\nIn this tutorial, we will provide a brief introduction to:\n\nCommand line (terminal/shell)\nNavigating around folders in Jupyter Lab\nVersion Control (code management using git)\nSetting up Git in Jupyter Lab\nThe Git GUI in Jupyter Lab\nBasic Git commands"
  },
  {
    "objectID": "content/git.html#introduction-jupyter-lab",
    "href": "content/git.html#introduction-jupyter-lab",
    "title": "Git - Jupyter Lab",
    "section": "Introduction :: Jupyter Lab",
    "text": "Introduction :: Jupyter Lab\nWhen you start the JupyterHub, you will be in Jupyter Lab. From there you can click on the RStudio box and open RStudio. However for this tutorial, we will stay in Juptyer Lab."
  },
  {
    "objectID": "content/git.html#introduction-terminalshell",
    "href": "content/git.html#introduction-terminalshell",
    "title": "Git - Jupyter Lab",
    "section": "Introduction :: Terminal/Shell",
    "text": "Introduction :: Terminal/Shell\nLog into the JupyterHub. If you do not see this\n\nThen go to File &gt; New Launcher\nClick on the ‚ÄúTerminal‚Äù box to open a new terminal window.\n\nShell or Terminal Basics\n\nWhat is Terminal or Shell?\nNavigating Files and Directories\nWorking with Files and Directories\nOptional: Detailed self-paced lesson on running scripts from the shell: Shell Lesson from Software Carpentry\n\nYou will need only basic navigation skills for this course: cd, ls and cat\n\npwd where am I\ncd nameofdir move into a directory\ncd .. move up a directory\nls list the files in the current directory\nls -a list the files including hidden files\nls -l list the files with more info\ncat filename print out the contents of a file\n\n\n\nLet‚Äôs try\nls\nls -a\ncd shared\nls\ncd shell-tutorial\ncat lesson1.sh\ncd ..\ncd ..\n\n\nClose the terminal\nJust click on the X in the terminal tab"
  },
  {
    "objectID": "content/git.html#introduction-file-navigation",
    "href": "content/git.html#introduction-file-navigation",
    "title": "Git - Jupyter Lab",
    "section": "Introduction :: File Navigation",
    "text": "Introduction :: File Navigation\nIn the far left, you will see a line of icons. The top one is a folder and allows us to move around our file system.\n\nClick on shared. Now you can see the files in the shared directory.\nClick on shell-tutorial. Then click on lesson1.sh. The file opens. You won‚Äôt be able to save changes here because you don‚Äôt have write permission on this drive.\nClick on the folder icon that looks like this. Click on the actual folder image. \nNow it should look like this folder /\nThis shows me doing this\n\nCreate a new folder.\n\nNext to the blue rectange with a +, is a grey folder with a +. Click that to create a new folder, called lesson-scripts.\nThen click on lesson-scripts to enter the folder\n\n\nCreate a new file\n\nCreate with File &gt; New &gt; Text file\nThe file will open and you can edit it.\nSave with File &gt; Save Text\nDelete the file by right-clicking on it and clicking ‚ÄúDelete‚Äù"
  },
  {
    "objectID": "content/git.html#introduction-version-control-git",
    "href": "content/git.html#introduction-version-control-git",
    "title": "Git - Jupyter Lab",
    "section": "Introduction :: Version Control (Git)",
    "text": "Introduction :: Version Control (Git)\n\nWhat is version control, git, github, and how to set it up?\nVersion control is managing and tracking changes to your documents (program source code, images, websites, data files, etc.). git is a popular tool used for version control of software code. github.com is popular platform that provides remote server hosting for git repositories. A repository is a collection of various files that you are tracking for changes and versions. Currently GitHub is the most popular platform for file sharing code and code packages.\nThis section is a step-by-step guide to set up git on our 2i2c JupyterHub. We will also configure git to use your github.com account for managing your repositories hosted on github.com. There are 5 main steps.\n\n\nStep 1: Create a GitHub account\nTo complete the setup, you will need an account on github.com. If you don‚Äôt have an account, please visit github.com, create an account (free) and come back to this guide for setting up git.\n\n\nStep 2: Clone a repository\nWe have created a demo repository for you to clone:\nhttps://github.com/Hackweek-ITCOocean/Git-Lesson\n\nStart your 2i2c JupyterHub\nClick on the Git icon\n\n\n\nClick ‚ÄúClone a Repository‚Äù\nWhere is says ‚ÄúEnter the URI of the remote Git repository‚Äù, paste in the URL https://github.com/Hackweek-ITCOocean/Git-Lesson\nThe folder appears and you can enter the folder and edit and create files.\n\n\nYour task: Create a file with your name and save to the Git-Lesson folder"
  },
  {
    "objectID": "content/git.html#step-3",
    "href": "content/git.html#step-3",
    "title": "Git - Jupyter Lab",
    "section": "Step 3:",
    "text": "Step 3:\nConfigure git with your name and email address.\n``` bash\ngit config --global user.name \"Makhan Virdi\"\ngit config --global user.email \"Makhan.Virdi@gmail.com\"\n```\n\n**Note:** This name and email could be different from your github.com credentials. Remember `git` is a program that keeps track of your changes locally (on 2i2c JupyterHub or your own computer) and github.com is a platform to host your repositories. However, since your changes are tracked by `git`, the email/name used in git configuration will show up next to your contributions on github.com when you `push` your repository to github.com (`git push` is discussed in a later step).\n\nConfigure git to store your github credentials to avoid having to enter your github username and token each time you push changes to your repository(in Step 5, we will describe how to use github token instead of a password)\ngit config --global credential.helper store\nCopy link for the demo repository from your github account. Click the green ‚ÄúCode‚Äù button and copy the link as shown.\n\nClone the repository using git clone command in the terminal\nTo clone a repository from github, copy the link for the repository (previous step) and use git clone:\ngit clone https://github.com/YOUR-GITHUB-USERNAME/check_github_setup\nNote: Replace YOUR-GITHUB-USERNAME here with your github.com username. For example, it is virdi for my github.com account as seen in this image.\n\nUse ls (list files) to verify the existence of the repository that you just cloned\n\nChange directory to the cloned repository using cd check_github_setup and check the current directory using pwd command (present working directory)\n\nCheck status of your git repository to confirm git set up using git status\n\nYou are all set with using git on your 2i2c JupyterHub! But the collaborative power of git through github needs some additional setup.\nIn the next step, we will create a new file in this repository, track changes to this file, and link it with your github.com account.\n\n\nStep 4. Creating new file and tracking changes\n\nIn the left panel on your 2i2c JupyterHub, click on the ‚Äúdirectory‚Äù icon and then double click on ‚Äúcheck_github_setup‚Äù directory.\n\n\nOnce you are in the check_github_setup directory, create a new file using the text editor in your 2i2c JupyterHub (File &gt;&gt; New &gt;&gt; Text File).\n\nName the file lastname.txt. For example, virdi.txt for me (use your last name). Add some content to this file (for example, I added this to my virdi.txt file: my last name is virdi).\n\nNow you should have a new file (lastname.txt) in the git repository directory check_github_setup\nCheck if git can see that you have added a new file using git status. Git reports that you have a new file that is not tracked by git yet, and suggests adding that file to the git tracking system.\n\nAs seen in this image, git suggests adding that file so it can be tracked for changes. You can add file to git for tracking changes using git add. Then, you can commit changes to this file‚Äôs content using git commit as shown in the image.\ngit add virdi.txt\ngit status\ngit commit -m \"adding a new file\"\ngit status\n\nAs seen in the image above, git is suggesting to push the change that you just committed to the remote server at github.com (so that your collaborators can also see what changes you made).\nNote: DO NOT execute push yet. Before we push to github.com, let‚Äôs configure git further and store our github.com credentials to avoid entering the credentials every time we invoke git push. For doing so, we need to create a token on github.com to be used in place of your github.com password.\n\n\n\nStep 5. Create access token on github.com\n\nGo to your github account and create a new ‚Äúpersonal access token‚Äù: https://github.com/settings/tokens/new\n\n\n\nGenerate Personal Access Token on github.com\n\n\nEnter a description in ‚ÄúNote‚Äù field as seen above, select ‚Äúrepo‚Äù checkbox, and scroll to the bottom and click the green button ‚ÄúGenerate Token‚Äù. Once generated, copy the token (or save it in a text file for reference).\nIMPORTANT: You will see this token only once, so be sure to copy this. If you do not copy your token at this stage, you will need to generate a new token.\n\nTo push (transfer) your changes to github, use git push in terminal. It requires you to enter your github credentials. You will be prompted to enter your github username and ‚Äúpassword‚Äù. When prompted for your ‚Äúpassword‚Äù, DO NOT use your github password, use the github token that was copied in the previous step.\ngit push\n\nNote: When you paste your token in the terminal window, windows users will press Ctrl+V and mac os users will press Cmd+V. If it does not work, try generating another token and use the copy icon next to the token to copy the token. Then, paste using your computer‚Äôs keyboard shortcut for paste.\nNow your password is stored in ~/.git-credentials and you will not be prompted again unless the Github token expires. You can check the presence of this git-credentials file using Terminal. Here the ~ character represents your home directory (/home/jovyan/).\nls -la ~\nThe output looks like this:\ndrwxr-xr-x 13 jovyan jovyan 6144 Oct 22 17:35 .\ndrwxr-xr-x  1 root   root   4096 Oct  4 16:21 ..\n-rw-------  1 jovyan jovyan 1754 Oct 29 18:30 .bash_history\ndrwxr-xr-x  4 jovyan jovyan 6144 Oct 29 16:38 .config\n-rw-------  1 jovyan jovyan   66 Oct 22 17:35 .git-credentials\n-rw-r--r--  1 jovyan jovyan   84 Oct 22 17:14 .gitconfig\ndrwxr-xr-x 10 jovyan jovyan 6144 Oct 21 16:19 2021-Cloud-Hackathon\nYou can also verify your git configuration\n(notebook) jovyan@jupyter-virdi:~$ git config -l\nThe output should have credential.helper = store:\nuser.email        = Makhan.Virdi@gmail.com\nuser.name         = Makhan Virdi\ncredential.helper = store\n\nNow we are all set to collaborate with github on the JupyterHub during the Cloud Hackathon!\n\n\nSummary: Git Commands\n\nCommonly used git commands (modified from source)\n\n\nGit Command\nDescription\n\n\n\n\ngit status\nShows the current state of the repository: the current working branch, files in the staging area, etc.\n\n\ngit add\nAdds a new, previously untracked file to version control and marks already tracked files to be committed with the next commit\n\n\ngit commit\nSaves the current state of the repository and creates an entry in the log\n\n\ngit log\nShows the history for the repository\n\n\ngit diff\nShows content differences between commits, branches, individual files and more\n\n\ngit clone\nCopies a repository to your local environment, including all the history\n\n\ngit pull\nGets the latest changes of a previously cloned repository\n\n\ngit push\nPushes your local changes to the remote repository, sharing them with others\n\n\n\n\n\nGit: More Details\nLesson: For a more detailed self-paced lesson on git, visit Git Lesson from Software Carpentry\nCheatsheet: Frequently used git commands\nDangit, Git!?!: If you are stuck after a git mishap, there are ready-made solutions to common problems at Dangit, Git!?!\n\n\nCloning our repository using the git Jupyter lab extension.\nIf we‚Äôre already familiar with git commands and feel more confortable using a GUI our Jupyterhub deployment comes with a git extension. This plugin allows us to operate with git using a simple user interface.\nFor example we can clone our repository using the extension.\n\n\n\ngit extension"
  },
  {
    "objectID": "content/index.html",
    "href": "content/index.html",
    "title": "Week 1 Tutorials",
    "section": "",
    "text": "During week 1, participants will gain experience with the platforms used in collaborative science: GitHub and RMarkdown."
  },
  {
    "objectID": "content/index.html#prerequisites",
    "href": "content/index.html#prerequisites",
    "title": "Week 1 Tutorials",
    "section": "Prerequisites",
    "text": "Prerequisites\nPlease follow the set up prerequisites"
  },
  {
    "objectID": "content/index.html#content",
    "href": "content/index.html#content",
    "title": "Week 1 Tutorials",
    "section": "Content",
    "text": "Content\n\nThe R language and RStudio\nIntro to RStudio\nIntroduction to Git and GitHub"
  },
  {
    "objectID": "content/jupyter-notebooks.html#summary",
    "href": "content/jupyter-notebooks.html#summary",
    "title": "Jupyter Notebooks - Python",
    "section": "Summary",
    "text": "Summary\nIn this tutorial, we will provide a brief introduction to:\n\nJupyter Notebooks in the JupyterHub\nProgramming in Python (using Jupyter Notebook)\nGeospatial Fundamentals (optional, self-study)\nCommand line (terminal/shell)\nVersion Control (code management using git)"
  },
  {
    "objectID": "content/jupyter-notebooks.html#introduction-programming-in-python",
    "href": "content/jupyter-notebooks.html#introduction-programming-in-python",
    "title": "Jupyter Notebooks - Python",
    "section": "Introduction :: Programming in Python",
    "text": "Introduction :: Programming in Python\nSwitch to Jupyter Notebook for an introduction to programming in Python\n\nVariables (and mathematical operations)\nData Structures (list, tuple, dict)\nFlow Control using loops (for, while)\nConditionals (if, else, elif)\nFunctions\nErrors and Exceptions (understanding and handling errors)\nUsing modules (libraries, packages)\n\npandas: high-performance, easy-to-use data structures and data analysis tools\nrioxarray: based on the rasterio package for working with rasters and xarray\n\n\n\nPython Learning Resources\nSelf-paced lesson on Programming with Python from Software Carpentry"
  },
  {
    "objectID": "content/jupyter-notebooks.html#introduction-geospatial-fundamentals-optional",
    "href": "content/jupyter-notebooks.html#introduction-geospatial-fundamentals-optional",
    "title": "Jupyter Notebooks - Python",
    "section": "Introduction :: Geospatial Fundamentals (Optional)",
    "text": "Introduction :: Geospatial Fundamentals (Optional)\nDetailed self-paced lesson on Fundamentals of Geospatial Raster and Vector Data with Python from Data Carpentry"
  },
  {
    "objectID": "content/jupyter-notebooks.html#jupyter-notebooks-in-earth-sciences",
    "href": "content/jupyter-notebooks.html#jupyter-notebooks-in-earth-sciences",
    "title": "Jupyter Notebooks - Python",
    "section": "Jupyter Notebooks in Earth Sciences",
    "text": "Jupyter Notebooks in Earth Sciences\nThere are many Jupyter Notebooks that you can copy and run in our JupyterHub. Note the ITCOocean JupyterHub is set up for geospatial research and connection to NASA‚Äôs data in the cloud. It is also set up for parallel processing with Dask.\n\nNASA Cloud Hackweek 2022\nOceanHackWeek\nICESat-2 Hackweeks"
  },
  {
    "objectID": "content/jupyter-notebooks.html#pythonconda-environments",
    "href": "content/jupyter-notebooks.html#pythonconda-environments",
    "title": "Jupyter Notebooks - Python",
    "section": "Python/Conda environments",
    "text": "Python/Conda environments\nPython users can create conda environments if they need to install modules. You create a environments.yml file like this:\nname: nsidc\nchannels:\n  - conda-forge\ndependencies:\n  - ipykernel\n  - awscli~=1.21.4\n  - requests\n  - pip\nAnd then in a terminal do this to activate the environment.\nconda env create -f environment.yml --name myenv\nconda activate myenv\nconda list"
  },
  {
    "objectID": "content/notebooks.html#summary",
    "href": "content/notebooks.html#summary",
    "title": "RMarkdown, R, Git",
    "section": "Summary",
    "text": "Summary\nIn this session, we will provide a brief introduction to:\n\nCommand line (terminal/shell)\nVersion Control (code management using git)\nProgramming in Python (using Jupyter Notebook)\nGeospatial Fundamentals (optional, self-study)\n\nYou will need a working knowledge of git and terminal for this hackathon. We will provide an overview of these topics and also share resources for self-paced learning."
  },
  {
    "objectID": "content/notebooks.html#introduction-command-line-terminalshell",
    "href": "content/notebooks.html#introduction-command-line-terminalshell",
    "title": "RMarkdown, R, Git",
    "section": "Introduction :: Command Line (Terminal/Shell)",
    "text": "Introduction :: Command Line (Terminal/Shell)\n\nShell Basics\n\nWhat is Terminal or Shell?\nNavigating Files and Directories\nWorking with Files and Directories\n\n\n\nShell: More Details\nDetailed self-paced lesson on shell: Shell Lesson from Software Carpentry"
  },
  {
    "objectID": "content/notebooks.html#introduction-version-control-git-and-github",
    "href": "content/notebooks.html#introduction-version-control-git-and-github",
    "title": "RMarkdown, R, Git",
    "section": "Introduction :: Version Control (Git and Github)",
    "text": "Introduction :: Version Control (Git and Github)\n\nWhat is version control, git, github, and how to set it up?\nVersion control is managing and tracking changes to your documents (program source code, images, websites, data files, etc.). git is a popular tool used for version control of software code. github.com is popular platform that provides remote server hosting for git repositories. A repository is a collection of various files that you are tracking for changes and versions (think of it as a directory with files that are being tracked for changes, using git for taking snapshots of versions as you are developing).\nThis section is a step-by-step guide to set up git on your 2i2c JupyterHub instance (referred to as 2i2c JupyterHub in these instruction). We will also configure git to use your github.com account for managing your repositories hosted on github.com. There are 5 main steps with substeps, includes instruction for addressing github‚Äôs new approach for token authentication.\n\n\nStep 1: Create a github account\nTo complete the setup, you will need an account on github.com. If you don‚Äôt have an account, please visit github.com, create an account (free) and come back to this guide for setting up git.\n\n\nStep 2: Fork a repository\nA fork is a copy of a repository from another github account (for example NASA-Openscapes account) to your github account (for example, my account virdi) that then you have permission to edit. To help you finish this setup correctly, we have created a demo repository on Openscapes github account named check_github_setup. You can fork this repository into your github account following these steps:\n\nLog in to your github.com account\nGo to the demo repository at NASA-Openscapes github\n\nClick on the fork icon in the top right corner, as shown in the image below and click your user name if prompted to do so\n\n\n\n\nStep 3: Clone the repository that you just forked\nNow you have a fork of the demo repository in your github account that we can clone it in your 2i2c instance. In the code below, commands beginning with git is a git command for version control and synching; commands that don‚Äôt start with git are bash/linux/command line commands.\n\nStart your 2i2c JupyterHub and open a terminal\nFile &gt;&gt; New &gt;&gt; Terminal\nMake sure you are in your home directory by usingpwd command and verifying the output as below\n/home/jovyan\n\nConfigure git with your name and email address.\ngit config --global user.name \"Makhan Virdi\"\ngit config --global user.email \"Makhan.Virdi@gmail.com\"\nNote: This name and email could be different from your github.com credentials. Remember git is a program that keeps track of your changes locally (on 2i2c JupyterHub or your own computer) and github.com is a platform to host your repositories. However, since your changes are tracked by git, the email/name used in git configuration will show up next to your contributions on github.com when you push your repository to github.com (git push is discussed in a later step).\nConfigure git to store your github credentials to avoid having to enter your github username and token each time you push changes to your repository(in Step 5, we will describe how to use github token instead of a password)\ngit config --global credential.helper store\nCopy link for the demo repository from your github account. Click the green ‚ÄúCode‚Äù button and copy the link as shown.\n\nClone the repository using git clone command in the terminal\nTo clone a repository from github, copy the link for the repository (previous step) and use git clone:\ngit clone https://github.com/YOUR-GITHUB-USERNAME/check_github_setup\nNote: Replace YOUR-GITHUB-USERNAME here with your github.com username. For example, it is virdi for my github.com account as seen in this image.\n\nUse ls (list files) to verify the existence of the repository that you just cloned\n\nChange directory to the cloned repository using cd check_github_setup and check the current directory using pwd command (present working directory)\n\nCheck status of your git repository to confirm git set up using git status\n\nYou are all set with using git on your 2i2c JupyterHub! But the collaborative power of git through github needs some additional setup.\nIn the next step, we will create a new file in this repository, track changes to this file, and link it with your github.com account.\n\n\n\nStep 4. Creating new file and tracking changes\n\nIn the left panel on your 2i2c JupyterHub, click on the ‚Äúdirectory‚Äù icon and then double click on ‚Äúcheck_github_setup‚Äù directory.\n\n\nOnce you are in the check_github_setup directory, create a new file using the text editor in your 2i2c JupyterHub (File &gt;&gt; New &gt;&gt; Text File).\n\nName the file lastname.txt. For example, virdi.txt for me (use your last name). Add some content to this file (for example, I added this to my virdi.txt file: my last name is virdi).\n\nNow you should have a new file (lastname.txt) in the git repository directory check_github_setup\nCheck if git can see that you have added a new file using git status. Git reports that you have a new file that is not tracked by git yet, and suggests adding that file to the git tracking system.\n\nAs seen in this image, git suggests adding that file so it can be tracked for changes. You can add file to git for tracking changes using git add. Then, you can commit changes to this file‚Äôs content using git commit as shown in the image.\ngit add virdi.txt\ngit status\ngit commit -m \"adding a new file\"\ngit status\n\nAs seen in the image above, git is suggesting to push the change that you just committed to the remote server at github.com (so that your collaborators can also see what changes you made).\nNote: DO NOT execute push yet. Before we push to github.com, let‚Äôs configure git further and store our github.com credentials to avoid entering the credentials every time we invoke git push. For doing so, we need to create a token on github.com to be used in place of your github.com password.\n\n\n\nStep 5. Create access token on github.com\n\nGo to your github account and create a new ‚Äúpersonal access token‚Äù: https://github.com/settings/tokens/new\n\n\n\nGenerate Personal Access Token on github.com\n\n\nEnter a description in ‚ÄúNote‚Äù field as seen above, select ‚Äúrepo‚Äù checkbox, and scroll to the bottom and click the green button ‚ÄúGenerate Token‚Äù. Once generated, copy the token (or save it in a text file for reference).\nIMPORTANT: You will see this token only once, so be sure to copy this. If you do not copy your token at this stage, you will need to generate a new token.\n\nTo push (transfer) your changes to github, use git push in terminal. It requires you to enter your github credentials. You will be prompted to enter your github username and ‚Äúpassword‚Äù. When prompted for your ‚Äúpassword‚Äù, DO NOT use your github password, use the github token that was copied in the previous step.\ngit push\n\nNote: When you paste your token in the terminal window, windows users will press Ctrl+V and mac os users will press Cmd+V. If it does not work, try generating another token and use the copy icon next to the token to copy the token. Then, paste using your computer‚Äôs keyboard shortcut for paste.\nNow your password is stored in ~/.git-credentials and you will not be prompted again unless the Github token expires. You can check the presence of this git-credentials file using Terminal. Here the ~ character represents your home directory (/home/jovyan/).\nls -la ~\nThe output looks like this:\ndrwxr-xr-x 13 jovyan jovyan 6144 Oct 22 17:35 .\ndrwxr-xr-x  1 root   root   4096 Oct  4 16:21 ..\n-rw-------  1 jovyan jovyan 1754 Oct 29 18:30 .bash_history\ndrwxr-xr-x  4 jovyan jovyan 6144 Oct 29 16:38 .config\n-rw-------  1 jovyan jovyan   66 Oct 22 17:35 .git-credentials\n-rw-r--r--  1 jovyan jovyan   84 Oct 22 17:14 .gitconfig\ndrwxr-xr-x 10 jovyan jovyan 6144 Oct 21 16:19 2021-Cloud-Hackathon\nYou can also verify your git configuration\n(notebook) jovyan@jupyter-virdi:~$ git config -l\nThe output should have credential.helper = store:\nuser.email        = Makhan.Virdi@gmail.com\nuser.name         = Makhan Virdi\ncredential.helper = store\n\nNow we are all set to collaborate with github on the JupyterHub during the Cloud Hackathon!\n\n\nSummary: Git Commands\n\nCommonly used git commands (modified from source)\n\n\nGit Command\nDescription\n\n\n\n\ngit status\nShows the current state of the repository: the current working branch, files in the staging area, etc.\n\n\ngit add\nAdds a new, previously untracked file to version control and marks already tracked files to be committed with the next commit\n\n\ngit commit\nSaves the current state of the repository and creates an entry in the log\n\n\ngit log\nShows the history for the repository\n\n\ngit diff\nShows content differences between commits, branches, individual files and more\n\n\ngit clone\nCopies a repository to your local environment, including all the history\n\n\ngit pull\nGets the latest changes of a previously cloned repository\n\n\ngit push\nPushes your local changes to the remote repository, sharing them with others\n\n\n\n\n\nGit: More Details\nLesson: For a more detailed self-paced lesson on git, visit Git Lesson from Software Carpentry\nCheatsheet: Frequently used git commands\nDangit, Git!?!: If you are stuck after a git mishap, there are ready-made solutions to common problems at Dangit, Git!?!\n\n\nCloning our repository using the git Jupyter lab extension.\nIf we‚Äôre already familiar with git commands and feel more confortable using a GUI our Jupyterhub deployment comes with a git extension. This plugin allows us to operate with git using a simple user interface.\nFor example we can clone our repository using the extension.\n\n\n\ngit extension"
  },
  {
    "objectID": "content/notebooks.html#introduction-programming-in-python",
    "href": "content/notebooks.html#introduction-programming-in-python",
    "title": "RMarkdown, R, Git",
    "section": "Introduction :: Programming in Python",
    "text": "Introduction :: Programming in Python"
  },
  {
    "objectID": "content/notebooks.html#introduction-programming-in-python-1",
    "href": "content/notebooks.html#introduction-programming-in-python-1",
    "title": "RMarkdown, R, Git",
    "section": "Introduction :: Programming in Python",
    "text": "Introduction :: Programming in Python\nSwitch to Jupyter Notebook for an introduction to programming in Python\n\nVariables (and mathematical operations)\nData Structures (list, tuple, dict)\nFlow Control using loops (for, while)\nConditionals (if, else, elif)\nFunctions\nErrors and Exceptions (understanding and handling errors)\nUsing modules (libraries, packages)\n\npandas: high-performance, easy-to-use data structures and data analysis tools\nrioxarray: based on the rasterio package for working with rasters and xarray\n\n\n\nPython Learning Resources\nSelf-paced lesson on Programming with Python from Software Carpentry"
  },
  {
    "objectID": "content/notebooks.html#introduction-geospatial-fundamentals-optional",
    "href": "content/notebooks.html#introduction-geospatial-fundamentals-optional",
    "title": "RMarkdown, R, Git",
    "section": "Introduction :: Geospatial Fundamentals (Optional)",
    "text": "Introduction :: Geospatial Fundamentals (Optional)\nDetailed self-paced lesson on Fundamentals of Geospatial Raster and Vector Data with Python from Data Carpentry\nThe end!"
  },
  {
    "objectID": "content/shell.html#summary",
    "href": "content/shell.html#summary",
    "title": "Terminal or Shell",
    "section": "Summary",
    "text": "Summary\nIn this tutorial, we will provide a brief introduction to\n\nWhat is the terminal/shell\nHow to get to the terminal in Jupyter Lab and RStudio\nNavigating around folders in the terminal\nClosing the terminal window"
  },
  {
    "objectID": "content/shell.html#what-is-the-terminal-or-shell",
    "href": "content/shell.html#what-is-the-terminal-or-shell",
    "title": "Terminal or Shell",
    "section": "What is the terminal or shell?",
    "text": "What is the terminal or shell?\nA way to interact with your computer from text commands instead of a graphical user interface (GUI). There are a few different types of shells and there are slight differences in syntax. However, we will be using very basic commands so the syntax difference won‚Äôt affect us.\nWhen in a terminal window you can type echo $0 to find out what shell type you are in."
  },
  {
    "objectID": "content/shell.html#introduction-terminalshell",
    "href": "content/shell.html#introduction-terminalshell",
    "title": "Terminal or Shell",
    "section": "Introduction :: Terminal/Shell",
    "text": "Introduction :: Terminal/Shell\n\nRStudio\nLog into the JupyterHub. If you do not see this\n\nThen go to File &gt; New Launcher\nClick on the RStudio box to open RStudio.\n\n\nJupyter Lab\nLog into the JupyterHub. If you do not see this\n\nThen go to File &gt; New Launcher\nClick on the ‚ÄúTerminal‚Äù box to open a new terminal window.\n\n\nShell or Terminal Basics\n\nWhat is Terminal or Shell?\nNavigating Files and Directories\nWorking with Files and Directories\nOptional: Detailed self-paced lesson on running scripts from the shell: Shell Lesson from Software Carpentry\n\nYou will need only basic navigation skills for this course: cd, ls and cat\n\npwd where am I\ncd nameofdir move into a directory\ncd .. move up a directory\nls list the files in the current directory\nls -a list the files including hidden files\nls -l list the files with more info\ncat filename print out the contents of a file\n\n\n\nLet‚Äôs try\nls\nls -a\ncd shared\nls\ncd shell-tutorial\ncat lesson1.sh\ncd ..\ncd ..\n\n\nClose the terminal\nJust click on the X in the terminal tab"
  },
  {
    "objectID": "content/shell.html#introduction-file-navigation",
    "href": "content/shell.html#introduction-file-navigation",
    "title": "Terminal or Shell",
    "section": "Introduction :: File Navigation",
    "text": "Introduction :: File Navigation\nIn the far left, you will see a line of icons. The top one is a folder and allows us to move around our file system.\n\nClick on shared. Now you can see the files in the shared directory.\nClick on shell-tutorial. Then click on lesson1.sh. The file opens. You won‚Äôt be able to save changes here because you don‚Äôt have write permission on this drive.\nClick on the folder icon that looks like this. Click on the actual folder image. \nNow it should look like this folder /\nThis shows me doing this\n\nCreate a new folder.\n\nNext to the blue rectange with a +, is a grey folder with a +. Click that to create a new folder, called lesson-scripts.\nThen click on lesson-scripts to enter the folder\n\n\nCreate a new file\n\nCreate with File &gt; New &gt; Text file\nThe file will open and you can edit it.\nSave with File &gt; Save Text\nDelete the file by right-clicking on it and clicking ‚ÄúDelete‚Äù"
  },
  {
    "objectID": "content/shell.html#introduction-version-control-git",
    "href": "content/shell.html#introduction-version-control-git",
    "title": "Terminal or Shell",
    "section": "Introduction :: Version Control (Git)",
    "text": "Introduction :: Version Control (Git)\n\nWhat is version control, git, github, and how to set it up?\nVersion control is managing and tracking changes to your documents (program source code, images, websites, data files, etc.). git is a popular tool used for version control of software code. github.com is popular platform that provides remote server hosting for git repositories. A repository is a collection of various files that you are tracking for changes and versions. Currently GitHub is the most popular platform for file sharing code and code packages.\nThis section is a step-by-step guide to set up git on our 2i2c JupyterHub. We will also configure git to use your github.com account for managing your repositories hosted on github.com. There are 5 main steps.\n\n\nStep 1: Create a GitHub account\nTo complete the setup, you will need an account on github.com. If you don‚Äôt have an account, please visit github.com, create an account (free) and come back to this guide for setting up git.\n\n\nStep 2: Clone a repository\nWe have created a demo repository for you to clone:\nhttps://github.com/Hackweek-ITCOocean/Git-Lesson\n\nStart your 2i2c JupyterHub\nClick on the Git icon\n\n\n\nClick ‚ÄúClone a Repository‚Äù\nWhere is says ‚ÄúEnter the URI of the remote Git repository‚Äù, paste in the URL https://github.com/Hackweek-ITCOocean/Git-Lesson\nThe folder appears and you can enter the folder and edit and create files.\n\n\nYour task: Create a file with your name and save to the Git-Lesson folder"
  },
  {
    "objectID": "content/shell.html#step-3",
    "href": "content/shell.html#step-3",
    "title": "Terminal or Shell",
    "section": "Step 3:",
    "text": "Step 3:\nConfigure git with your name and email address.\n``` bash\ngit config --global user.name \"Makhan Virdi\"\ngit config --global user.email \"Makhan.Virdi@gmail.com\"\n```\n\n**Note:** This name and email could be different from your github.com credentials. Remember `git` is a program that keeps track of your changes locally (on 2i2c JupyterHub or your own computer) and github.com is a platform to host your repositories. However, since your changes are tracked by `git`, the email/name used in git configuration will show up next to your contributions on github.com when you `push` your repository to github.com (`git push` is discussed in a later step).\n\nConfigure git to store your github credentials to avoid having to enter your github username and token each time you push changes to your repository(in Step 5, we will describe how to use github token instead of a password)\ngit config --global credential.helper store\nCopy link for the demo repository from your github account. Click the green ‚ÄúCode‚Äù button and copy the link as shown.\n\nClone the repository using git clone command in the terminal\nTo clone a repository from github, copy the link for the repository (previous step) and use git clone:\ngit clone https://github.com/YOUR-GITHUB-USERNAME/check_github_setup\nNote: Replace YOUR-GITHUB-USERNAME here with your github.com username. For example, it is virdi for my github.com account as seen in this image.\n\nUse ls (list files) to verify the existence of the repository that you just cloned\n\nChange directory to the cloned repository using cd check_github_setup and check the current directory using pwd command (present working directory)\n\nCheck status of your git repository to confirm git set up using git status\n\nYou are all set with using git on your 2i2c JupyterHub! But the collaborative power of git through github needs some additional setup.\nIn the next step, we will create a new file in this repository, track changes to this file, and link it with your github.com account.\n\n\nStep 4. Creating new file and tracking changes\n\nIn the left panel on your 2i2c JupyterHub, click on the ‚Äúdirectory‚Äù icon and then double click on ‚Äúcheck_github_setup‚Äù directory.\n\n\nOnce you are in the check_github_setup directory, create a new file using the text editor in your 2i2c JupyterHub (File &gt;&gt; New &gt;&gt; Text File).\n\nName the file lastname.txt. For example, virdi.txt for me (use your last name). Add some content to this file (for example, I added this to my virdi.txt file: my last name is virdi).\n\nNow you should have a new file (lastname.txt) in the git repository directory check_github_setup\nCheck if git can see that you have added a new file using git status. Git reports that you have a new file that is not tracked by git yet, and suggests adding that file to the git tracking system.\n\nAs seen in this image, git suggests adding that file so it can be tracked for changes. You can add file to git for tracking changes using git add. Then, you can commit changes to this file‚Äôs content using git commit as shown in the image.\ngit add virdi.txt\ngit status\ngit commit -m \"adding a new file\"\ngit status\n\nAs seen in the image above, git is suggesting to push the change that you just committed to the remote server at github.com (so that your collaborators can also see what changes you made).\nNote: DO NOT execute push yet. Before we push to github.com, let‚Äôs configure git further and store our github.com credentials to avoid entering the credentials every time we invoke git push. For doing so, we need to create a token on github.com to be used in place of your github.com password.\n\n\n\nStep 5. Create access token on github.com\n\nGo to your github account and create a new ‚Äúpersonal access token‚Äù: https://github.com/settings/tokens/new\n\n\n\nGenerate Personal Access Token on github.com\n\n\nEnter a description in ‚ÄúNote‚Äù field as seen above, select ‚Äúrepo‚Äù checkbox, and scroll to the bottom and click the green button ‚ÄúGenerate Token‚Äù. Once generated, copy the token (or save it in a text file for reference).\nIMPORTANT: You will see this token only once, so be sure to copy this. If you do not copy your token at this stage, you will need to generate a new token.\n\nTo push (transfer) your changes to github, use git push in terminal. It requires you to enter your github credentials. You will be prompted to enter your github username and ‚Äúpassword‚Äù. When prompted for your ‚Äúpassword‚Äù, DO NOT use your github password, use the github token that was copied in the previous step.\ngit push\n\nNote: When you paste your token in the terminal window, windows users will press Ctrl+V and mac os users will press Cmd+V. If it does not work, try generating another token and use the copy icon next to the token to copy the token. Then, paste using your computer‚Äôs keyboard shortcut for paste.\nNow your password is stored in ~/.git-credentials and you will not be prompted again unless the Github token expires. You can check the presence of this git-credentials file using Terminal. Here the ~ character represents your home directory (/home/jovyan/).\nls -la ~\nThe output looks like this:\ndrwxr-xr-x 13 jovyan jovyan 6144 Oct 22 17:35 .\ndrwxr-xr-x  1 root   root   4096 Oct  4 16:21 ..\n-rw-------  1 jovyan jovyan 1754 Oct 29 18:30 .bash_history\ndrwxr-xr-x  4 jovyan jovyan 6144 Oct 29 16:38 .config\n-rw-------  1 jovyan jovyan   66 Oct 22 17:35 .git-credentials\n-rw-r--r--  1 jovyan jovyan   84 Oct 22 17:14 .gitconfig\ndrwxr-xr-x 10 jovyan jovyan 6144 Oct 21 16:19 2021-Cloud-Hackathon\nYou can also verify your git configuration\n(notebook) jovyan@jupyter-virdi:~$ git config -l\nThe output should have credential.helper = store:\nuser.email        = Makhan.Virdi@gmail.com\nuser.name         = Makhan Virdi\ncredential.helper = store\n\nNow we are all set to collaborate with github on the JupyterHub during the Cloud Hackathon!\n\n\nSummary: Git Commands\n\nCommonly used git commands (modified from source)\n\n\nGit Command\nDescription\n\n\n\n\ngit status\nShows the current state of the repository: the current working branch, files in the staging area, etc.\n\n\ngit add\nAdds a new, previously untracked file to version control and marks already tracked files to be committed with the next commit\n\n\ngit commit\nSaves the current state of the repository and creates an entry in the log\n\n\ngit log\nShows the history for the repository\n\n\ngit diff\nShows content differences between commits, branches, individual files and more\n\n\ngit clone\nCopies a repository to your local environment, including all the history\n\n\ngit pull\nGets the latest changes of a previously cloned repository\n\n\ngit push\nPushes your local changes to the remote repository, sharing them with others\n\n\n\n\n\nGit: More Details\nLesson: For a more detailed self-paced lesson on git, visit Git Lesson from Software Carpentry\nCheatsheet: Frequently used git commands\nDangit, Git!?!: If you are stuck after a git mishap, there are ready-made solutions to common problems at Dangit, Git!?!\n\n\nCloning our repository using the git Jupyter lab extension.\nIf we‚Äôre already familiar with git commands and feel more confortable using a GUI our Jupyterhub deployment comes with a git extension. This plugin allows us to operate with git using a simple user interface.\nFor example we can clone our repository using the extension.\n\n\n\ngit extension"
  },
  {
    "objectID": "hackweek-links.html#getting-rs-data",
    "href": "hackweek-links.html#getting-rs-data",
    "title": "Hackweek Links",
    "section": "Getting RS data",
    "text": "Getting RS data\n\nUse sdmpredictors for the hackweek so you have some data. The spreadsheet with all the sdmpredictors variables\nFrom NASA Earthdata: In Home shared/earthaccess-gallery/notebooks/Sea_Level_Rise/SSL.ipynb\nR and Python examples: shared/incois-scripts\nZarr file\n\nshared/2023-Hackbook/python-notebooks/02_Using_Data.ipynb\nshared/2023-Hackbook/python-notebooks/02_Get_Points.ipynb\n\nCopernicus: Search the Slack for ‚ÄúCopenicus‚Äù. Someone got it working in R via a R package for Copenicus."
  },
  {
    "objectID": "hackweek-links.html#model-evaluation",
    "href": "hackweek-links.html#model-evaluation",
    "title": "Hackweek Links",
    "section": "Model evaluation",
    "text": "Model evaluation\n\nhttps://rspatial.org/raster/sdm/5_sdm_models.html\nBRTs with dismo https://rspatial.org/raster/sdm/9_sdm_brt.html\nNew link"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning based Species Distribution Modelling",
    "section": "",
    "text": "Welcome to the Sept 2023 training course on Machine Learning based Species Distribution Modelling hosted by the International Training Centre for Operational Oceanography (ITCOocean), ESSO-INCOIS, Hyderabad, India.\nThe course and hackweek will take place at the ITCOocean Training Centre in Hyderabad, India from September 11-22, 2023. course announcement. The application period for the 2023 course has closed."
  },
  {
    "objectID": "index.html#links",
    "href": "index.html#links",
    "title": "Machine Learning based Species Distribution Modelling",
    "section": "Links",
    "text": "Links\n\nCourse GitHub org: https://github.com/Hackweek-ITCOocean\nJupyterHub: https://itcoocean.2i2c.cloud/\nDiscussions: https://github.com/orgs/Hackweek-ITCOocean/discussions\nSlack: https://app.slack.com/client/T05PV2USY56/C05P7QP47UL"
  },
  {
    "objectID": "nasa-tutorials/00_Setup.html",
    "href": "nasa-tutorials/00_Setup.html",
    "title": "00. Setup for tutorials",
    "section": "",
    "text": "This tutorial will help you set up your JupyterHub (or Hub) with tutorials and other materials from our Cloud Hackathon github repository and connect your github account."
  },
  {
    "objectID": "nasa-tutorials/00_Setup.html#step-1.-login-to-the-hub",
    "href": "nasa-tutorials/00_Setup.html#step-1.-login-to-the-hub",
    "title": "00. Setup for tutorials",
    "section": "Step 1. Login to the Hub",
    "text": "Step 1. Login to the Hub\nPlease go to the Openscapes Jupyter Hub. Log in with your GitHub Account, and select ‚ÄúSmall‚Äù.\n\n\nNote: It takes a few minutes for the Hub to load. Please be patient!\n\nWhile the Hub loads, we‚Äôll:\n\nDiscuss cloud environments\nSee how my Desktop is setup\nFork the Hackathon repository at github.com\nDiscuss python and conda environments\n\nThen, when the Hub is loaded, we‚Äôll get oriented in the Hub and clone the forked repository into our cloud environment."
  },
  {
    "objectID": "nasa-tutorials/00_Setup.html#discussion-cloud-environment",
    "href": "nasa-tutorials/00_Setup.html#discussion-cloud-environment",
    "title": "00. Setup for tutorials",
    "section": "Discussion: Cloud environment",
    "text": "Discussion: Cloud environment\nA brief overview about the NASA Openscapes Cloud Environment (following lessons from the Clinic).\n\nCloud infrastructure\n\nCloud: AWS us-west-2\n\nData: AWS S3 (cloud) and NASA DAAC data centers (on-prem).\nCloud compute environment: 2i2c Jupyterhub deployment\n\nIDE: JupyterLab"
  },
  {
    "objectID": "nasa-tutorials/00_Setup.html#discussion-my-desktop-setup",
    "href": "nasa-tutorials/00_Setup.html#discussion-my-desktop-setup",
    "title": "00. Setup for tutorials",
    "section": "Discussion: My desktop setup",
    "text": "Discussion: My desktop setup\nI‚Äôll screenshare to show and/or talk through how I have oriented the following software we‚Äôre using:\n\n2i2c Jupyterhub (our main workspace)\nHackathon Repo &lt;&gt; Hackathon Book (my teaching notes, your reference material)\nZoom Chat\nSlack"
  },
  {
    "objectID": "nasa-tutorials/00_Setup.html#step-2.-fork-the-hackathon-github-repository",
    "href": "nasa-tutorials/00_Setup.html#step-2.-fork-the-hackathon-github-repository",
    "title": "00. Setup for tutorials",
    "section": "Step 2. Fork the Hackathon GitHub repository",
    "text": "Step 2. Fork the Hackathon GitHub repository\n‚ÄúHow do I get the tutorial repository into the Hub?‚Äù. There are 2 steps. The first is from GitHub.com to fork the tutorial repository so that there is a connected copy in your user account that you can edit and push changes that won‚Äôt affect the nasa-openscapes copy.\nGo to https://github.com/nasa-openscapes/2021-Cloud-Hackathon and fork the repository.\n\nNote: if you‚Äôve already done this in the Pre-Hackathon Clinic, you‚Äôll need to make sure you have the latest, following the daily setup instructions."
  },
  {
    "objectID": "nasa-tutorials/00_Setup.html#discussion-python-and-conda-environments",
    "href": "nasa-tutorials/00_Setup.html#discussion-python-and-conda-environments",
    "title": "00. Setup for tutorials",
    "section": "Discussion: Python and Conda environments",
    "text": "Discussion: Python and Conda environments\nWhy Python?\n\n\n\nPython Data Stack. Source: Jake VanderPlas, ‚ÄúThe State of the Stack,‚Äù SciPy Keynote (SciPy 2015).\n\n\nDefault Python Environment:\nWe‚Äôve set up the Python environment with conda.\n\n\n\n\n\n\nConda environment\n\n\n\n\n\nname: openscapes\nchannels:\n  - conda-forge\ndependencies:\n  - python=3.9\n  - pangeo-notebook\n  - awscli~=1.20\n  - boto3~=1.19\n  - gdal~=3.3\n  - rioxarray~=0.8\n  - xarray~=0.19\n  - h5netcdf~=0.11\n  - netcdf4~=1.5\n  - h5py~=2.10\n  - geoviews~=1.9\n  - matplotlib-base~=3.4\n  - hvplot~=0.7\n  - pyproj~=3.2\n  - bqplot~=0.12\n  - geopandas~=0.10\n  - zarr~=2.10\n  - cartopy~=0.20\n  - shapely==1.7.1\n  - pyresample~=1.22\n  - joblib~=1.1\n  - pystac-client~=0.3\n  - s3fs~=2021.7\n  - ipyleaflet~=0.14\n  - sidecar~=0.5\n  - jupyterlab-geojson~=3.1\n  - jupyterlab-git\n  - jupyter-resource-usage\n  - ipympl~=0.6\n  - conda-lock~=0.12\n  - pooch~=1.5\n  - pip\n  - pip:\n    - tqdm\n    - harmony-py\n    - earthdata\n    - zarr-eosdis-store\n\n\n\n\nBash terminal and installed software\nLibraries that are available from the terminal\n\ngdal 3.3 commands ( gdalinfo, gdaltransform‚Ä¶)\nhdf5 commands ( h5dump, h5ls..)\nnetcdf4 commands (ncdump, ncinfo ‚Ä¶)\njq (parsing json files or streams from curl)\ncurl (fetch resources from the web)\nawscli (AWS API client, to interact with AWS cloud services)\nvim (editor)\ntree ( directory tree)\nmore ‚Ä¶\n\n\n\nUpdating the environment\nScientific Python is a vast space and we only included libraries that are needed in our tutorials. Our default environment can be updated to include any Python library that‚Äôs available on pip or conda.\nThe project used to create our default environment is called corn (as it can include many Python kernels).\nIf we want to update a library or install a whole new environment we need to open an issue on this repository. We can help your teams do this during project hacktime.\n\n\ncorn üåΩ"
  },
  {
    "objectID": "nasa-tutorials/00_Setup.html#step-3.-jupyterhub-orientation",
    "href": "nasa-tutorials/00_Setup.html#step-3.-jupyterhub-orientation",
    "title": "00. Setup for tutorials",
    "section": "Step 3. JupyterHub orientation",
    "text": "Step 3. JupyterHub orientation\nNow that the Hub is loaded, let‚Äôs get oriented.\n\n\n\n\n\n\nFirst impressions\n\nLauncher & the big blue button\n‚Äúhome directory‚Äù"
  },
  {
    "objectID": "nasa-tutorials/00_Setup.html#step-4.-clone-the-hackathon-github-repository",
    "href": "nasa-tutorials/00_Setup.html#step-4.-clone-the-hackathon-github-repository",
    "title": "00. Setup for tutorials",
    "section": "Step 4. Clone the Hackathon GitHub repository",
    "text": "Step 4. Clone the Hackathon GitHub repository\nNow we‚Äôll clone the GitHub repository, using a git extension for the JupyterHub. Go to your github account, and navigate to the repository that you just created by forking from the Openscapes repository.\nClick to copy the url for cloning the repository.\n\nNow, go to JupyterHub and click on the git extension in the left panel and then click the blue button ‚ÄúClone a Repository‚Äù.\n\nThen, paste the repository link to the forked repository that you copied from your github account into the ‚ÄúClone a repo‚Äù pop up window. Then click the blue ‚ÄúCLONE‚Äù button. It will take a few moments to clone the repository into your Hub.\nYour link should look like https://github.com/YOUR-USERNAME/2021-Cloud-Hackathon. For example, the link is https://github.com/virdi/2021-Cloud-Hackathon. Note that it include your github username in the repo link.\n\nAlternatively, you can use the terminal (command line) as per github workflows: first-time setup.\nOnce the repository is cloned, you will see a new directory in the ‚ÄúFile Browser‚Äù panel on the left named ‚Äú2021-Cloud-Hackathon‚Äù. In this directory, you have all hackathon material including the tutorials and this book to follow along during other Tutorials. You are all set.\n\n\nREMEMBER: This is your copy (or fork) of the hackathon materials and jupyter notebooks. So feel free to make any changes to the content of this repository."
  },
  {
    "objectID": "nasa-tutorials/00_Setup.html#jupyter-notebooks",
    "href": "nasa-tutorials/00_Setup.html#jupyter-notebooks",
    "title": "00. Setup for tutorials",
    "section": "Jupyter notebooks",
    "text": "Jupyter notebooks\nLet‚Äôs get oriented to Jupyter notebooks, which we‚Äôll use in all the tutorials."
  },
  {
    "objectID": "nasa-tutorials/00_Setup.html#how-do-i-end-my-session",
    "href": "nasa-tutorials/00_Setup.html#how-do-i-end-my-session",
    "title": "00. Setup for tutorials",
    "section": "How do I end my session?",
    "text": "How do I end my session?\n(Also see How do I end my Openscapes session? Will I lose all of my work?)\nWhen you are finished working for the day it is important to explicitly log out of your Openscapes session. The reason for this is it will save us a bit of money! When you keep a session active it uses up AWS resources and keeps a series of virtual machines deployed.\nStopping the server happens automatically when you log out, so navigate to ‚ÄúFile -&gt; Log Out‚Äù and just click ‚ÄúLog Out‚Äù!\n!!! NOTE ‚Äúlogging out‚Äù - Logging out will NOT cause any of your work to be lost or deleted. It simply shuts down some resources. It would be equivalent to turning off your desktop computer at the end of the day."
  },
  {
    "objectID": "nasa-tutorials/00_Setup.html#step-5.-tracking-changes-optional",
    "href": "nasa-tutorials/00_Setup.html#step-5.-tracking-changes-optional",
    "title": "00. Setup for tutorials",
    "section": "Step 5. Tracking changes (Optional)",
    "text": "Step 5. Tracking changes (Optional)\nNow that you have forked and cloned the repository in your Hub, you can make changes (edit, add, and/or delete content) and track these files using git. In this step, we will provide an overview of how to use git using the graphical interface (the JupyterLab git extension).\n\nStep 5.1. Configure Git (git config)\nConfigure git with your name and email address as shown here.\ngit config --global user.name \"Makhan Virdi\"\ngit config --global user.email \"Makhan.Virdi@gmail.com\"\nOpen a new terminal: File &gt;&gt; New &gt;&gt; Terminal\n\nConfigure git to store your github credentials to avoid having to enter your github username and token each time you push changes to your repository(in Step 5.5, we will describe how to use github token instead of a password)\ngit config --global credential.helper store\n\n\nStep 5.2. Create a new file\nLet‚Äôs create a new file: In the left panel on your Hub, click on the ‚Äúdirectory‚Äù icon and then double click on ‚Äú2021-Cloud-Hackathon‚Äù directory. Then, create a new file using the text editor in your 2i2c JupyterHub (File &gt;&gt; New &gt;&gt; Text File). Add some text to this file, for example: A test file. Save this file and rename it to test.txt.\n\n\n\nStep 5.3. Track the changes to the new file (git add)\nClick the git icon in the left panel. You can see that the newly added file is in the ‚ÄúUntracked‚Äù section. You can click the + icon next to the file name to let git track this file for changes.\n\n\n\nStep 5.4. Commit the changes to the new file (git commit)\nNow, you will see that the file is Staged, which means that git is ready to take a snapshot of this file (and the repository) with the changes that you made. This snapshot is called a commit. To commit the changes, add a note (called a commit message) by typing in the text box that say ‚ÄúSummary‚Äù.\nNow, click the blue ‚ÄúCOMMIT‚Äù button to commit this change.\n\nNote: A short message indicating the type of change to this file is a good practice. Optionally, a longer description may be added to the ‚ÄúDescription‚Äù field.\n\n\n\n\nStep 5.5. Transmit committed changes to your github (git push)\nAt this stage, you have committed the changes to your git repository on your Hub. However, these changes are still on your Hub and needs to be transmitted to your repository on github (so that both the local copy on the JupyterHub and the remote copy on github are in sync).\nAs seen in the picture below, the git extension indicates (with an orange dot on the cloud icon) that it is ready to push your changes to the remote (remote = your repository on github.com). To push to github, click the cloud button with an up arrow (circled in red in the picture).\n\nWhen you push for the first time, it will ask that you input your credentials. You will need to set this up with your Personal Access Token (PAT), explained next.\n\n\nStep 5.6. Setup your Personal Access Token (PAT)\nWhen you see the following screenshot, GitHub is asking for you to input your credentials. (Note: you see this screenshot when you have committed work to push to GitHub.com, as detailed above).\n\nThe git extension in the Hub is prompting you to enter your github.com credentials. Enter you github.com username and a Personal Access Token (PAT); DO NOT use your password.\nTo create a PAT, visit https://github.com/settings/tokens/new and create a new token with the permission as per the image below and specify its validity for 90 days.\n\nIMPORTANT: You will see this token only once, so be sure to copy this. If you do not copy your token at this stage, you will need to generate a new token.\n\nOnce you generate the token, copy it and paste in the Hub window that prompted you to enter the ‚ÄúPersonal Access Token‚Äù.\n\nGit will show a message at the bottom right telling that the changes were ‚ÄúSuccessfully pushed‚Äù. Also, you will see that the ‚Äúcloud icon with an up arrow‚Äù no longer has an orange dot, indicating that there are no more committed changes to push to the remote (github.com).\n\nNote: You have configured git extension to store your credentials. You will not be prompted for your login/token again!\n\n\nThat‚Äôs all. You can use the same workflow (add &gt; commit &gt; push) for any other new or modified files!\n\n\nNote: If you are comfortable with the command line, you can use the Terminal (In Hub, New &gt; Terminal) and follow the steps outlined in the Clinic section."
  },
  {
    "objectID": "nasa-tutorials/index.html",
    "href": "nasa-tutorials/index.html",
    "title": "Tutorials Overview",
    "section": "",
    "text": "These tutorials are written in RMarkdown (R) and Jupyter Notebooks (Python)."
  },
  {
    "objectID": "nasa-tutorials/index.html#rmarkdown",
    "href": "nasa-tutorials/index.html#rmarkdown",
    "title": "Tutorials Overview",
    "section": "RMarkdown",
    "text": "RMarkdown\n\nOpen in RStudio\nRun code by clicking on the Run icon above a code chunk"
  },
  {
    "objectID": "nasa-tutorials/index.html#jupyter-notebook",
    "href": "nasa-tutorials/index.html#jupyter-notebook",
    "title": "Tutorials Overview",
    "section": "Jupyter Notebook",
    "text": "Jupyter Notebook\n\nOpen in Jupyter Notebook\nRun code by clicking on the code and clicking the little arrow at the top or by typing shift-return."
  },
  {
    "objectID": "overview.html",
    "href": "overview.html",
    "title": "Overview",
    "section": "",
    "text": "The the era of big data in the earth sciences is here and learning how to effectively use oceanographic remote-sensing data, both in the cloud and on your computer, is a core skill for modern fisheries science and management. Learning how to access cloud-based data, visualize these data, use these data in models, and use the tools of modern reproducible and collaborative science is the main goal of this course. Through the course, participants will gain experience with assessing remote-sensing data in the cloud, R and RStudio, and collaborating with Git and GitHub. Python users: Python is also very heavily used in big data geosciences. The course tutorials will focus on R, however you are welcome to use Python during the course if you are more comfortable with that."
  },
  {
    "objectID": "overview.html#hackweek-focus-species-distribution-modelling",
    "href": "overview.html#hackweek-focus-species-distribution-modelling",
    "title": "Overview",
    "section": "Hackweek focus: species distribution modelling",
    "text": "Hackweek focus: species distribution modelling\nIn depth understanding of machine learning for species (macrofauna) distribution modelling (SDM) is needed by many young ecosystem researchers. SDMs are an important class of models used to understand species association with the environment. SDMs are widely used to understand how species distributions are changing with ocean climate change and are also used to estimate abundance over large ocean regions from fisheries survey data. This course is devised to familiarize the young professionals in the Indian Ocean-rim (IOR) countries with the latest developments in this field.\nThis course does not aim to teach ecology and habitat to the oceanographers/modellers/machine learning experts, rather it aims to teach machine learning and coding (and its relevance to fisheries) to fishery professionals."
  },
  {
    "objectID": "overview.html#aims-and-objectives",
    "href": "overview.html#aims-and-objectives",
    "title": "Overview",
    "section": "Aims and Objectives",
    "text": "Aims and Objectives\n\nLearn how to discover and use oceanographic remote-sensing data for species distribution modeling and other fisheries applications\nFamiliarize participants with using remote-sensing data in R within models. Note participants who are more familiar with Python are welcome to use Python also.\nIntroduce the participants to machine learning models used in species distribution modelling.\nObtain hands-on experience in using species distribution modelling and machine-learning with remote-sensing data.\nWork on a group project using species distribution modelling to estimate habitat associations or obtain a stock estimate using a species distribution model.\n\nNote, there are many ways that participants can study aspects of species distribution using remote-sensing data. During the hackweek portion of the course, participants will jointly develop project to work on and in order to learn by diving into a specific project. Species location data will not be specifically required in order to engage in the projects."
  },
  {
    "objectID": "overview.html#what-is-a-hackweek",
    "href": "overview.html#what-is-a-hackweek",
    "title": "Overview",
    "section": "What is a hackweek?",
    "text": "What is a hackweek?\nA hackweek is a participant-driven workshop that blends data science education, community building, and project work over a short period of time (one to two weeks). The events are highly immersive and allow participants to work directly with data science professionals to co-shape projects and educational outcomes. Hackweeks help individuals and teams engage more effectively in open and reproducible science. - eScience Institute, University of Washington, Seattle USA\nThe hackweek model has become a vital tool in the data science community, fostering idea exchange through modern data analysis workflow training. Unlike traditional academic events, hackweeks offer intensive, interactive learning, including tutorials on cutting-edge methods, peer-based learning, and collaborative on-site projects. Unlike hackathons, which emphasize software development, hackweeks prioritize education and open-ended projects, benefiting fields needing both expertise and efficient computational workflows for idea exchange and discovery. The hackweek model is now widely used in many fields: Astrohackweek, Neurohackweek, Geohackweek, OceanHackWeek, ICESat-2 Hackweek, SnowEx Hackweek, NASA Cloud Hackathon. The ITCOocean Hack2week is modeled off the University of Washington eScience Hackweek model."
  },
  {
    "objectID": "overview.html#what-to-expect",
    "href": "overview.html#what-to-expect",
    "title": "Overview",
    "section": "What to expect",
    "text": "What to expect\n\nDuring the first week of the ITCOocean hack2week, participants will work on learning how to access, visualize and manipulate remote-sensing data in R. Participants will develop familiarity with the R language, with RStudio, and with Git and GitHub for collaboration.\nDuring the second week, participants will begin focusing on species distribution modeling and will work on a team project.\nThe course is an open science event: all tutorials and examples are developed openly and will be publicly available during and following the course. Participants will strengthen their practice of open science, using open source code and collaborating on their projects with course peers.\n\nIn the two to three weeks leading up to the course, participants are encouraged to review background resources that will facilitate a more effective experience."
  },
  {
    "objectID": "overview.html#code-of-conduct",
    "href": "overview.html#code-of-conduct",
    "title": "Overview",
    "section": "Code of Conduct",
    "text": "Code of Conduct\nThe 2023 ITCOocean Hackweek is a safe learning space and all participants are required to abide by our Code of Conduct."
  },
  {
    "objectID": "participants.html",
    "href": "participants.html",
    "title": "Participants",
    "section": "",
    "text": "Our hack2week participants, organizers and helpers come from across the Indian Ocean Rim and beyond.\n\nIndian National Centre for Ocean Information Services (INCOIS), Hyderabad, India\nCentre for Climate Change Research, Indian Institute of Tropical Meteorology (CCCR), Pune, India\nCentre for Marine Living Resources and Ecology (CMLRE), Ministry of Earth Sciences (MoES), Kochi, India.\nNOAA Fisheries, Seattle, WA USA\nNational Centre for Coastal Research (NCCR), Ministry of Earth Sciences (MoES), Chennai, India"
  },
  {
    "objectID": "projects/hackathon-projects.html",
    "href": "projects/hackathon-projects.html",
    "title": "Hackathon Projects",
    "section": "",
    "text": "We will fill in the projects here at the end of week 1."
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects Overview",
    "section": "",
    "text": "The following was borrowed and adapted from the excellent SnowEx Hackathon 2021\nThis section contains everything you need to know about hackweek projects."
  },
  {
    "objectID": "projects/index.html#purpose-of-the-projects",
    "href": "projects/index.html#purpose-of-the-projects",
    "title": "Projects Overview",
    "section": "Purpose of the projects:",
    "text": "Purpose of the projects:\nDuring the course we will be facilitating team hacking sessions in the second half of each day. The purpose of these sessions is for participants to gain hands-on experience in working together on a well-defined problem, in a collaborative space where you can talk things through and get help."
  },
  {
    "objectID": "projects/index.html#what-is-hacking",
    "href": "projects/index.html#what-is-hacking",
    "title": "Projects Overview",
    "section": "What is hacking?",
    "text": "What is hacking?\nHacking is a session of focused, highly collaborative work time ‚Äì often involving coding ‚Äì in which the group creates conditions for rapid absorption of new ideas and methods. The word ‚Äúhack‚Äù or ‚Äúhackathon‚Äù has many different interpretations, both positive and negative. Here our intention is to foster the idea of hacking as a fun, interactive and welcoming environment to explore and experiment with computer code."
  },
  {
    "objectID": "projects/index.html#how-will-the-projects-be-conducted",
    "href": "projects/index.html#how-will-the-projects-be-conducted",
    "title": "Projects Overview",
    "section": "How will the projects be conducted?",
    "text": "How will the projects be conducted?\nParticipants are invited to start conversations about projects in the Slack channel 2023-hackweek-projects before and during week one of the course.\n\nIf you have a project idea brewing, please pitch it in this channel. You can tag your proposed teammates if you already have that worked out.\nStart a thread with ‚ÄúProject idea:‚Äù and then provide a few sentences. Include whether you are looking for teammates to join this project. Others who are interested can respond in a thread.\nWe welcome a broad range of project topics. People often use project time to dig deeper into concepts introduced in tutorials, to explore problems within their own research, or to advance community data sharing and software building efforts.\nThe course team is here to help you get clear on project ideas and decide on what is possible within 5 days.\n\nAt the end of week 1 of the course we will have a ‚ÄúPitchfest‚Äù‚Äù where proposer(s) can pitch their idea. At this time we will finalize the project teams for the week ‚Äî however we will provide times when team members will move and work with other teams. This kind of ‚Äòcross-pollination‚Äô helps everyone learn and solve problems.\nTeam hacktime will begin on week 2.\nEach team is encouraged to identify a project lead, likely the person who pitched the idea, who has knowledge of the datasets and the specific problem to be explored. But roles can be assigned as the group decides to best fit skills and needs.\nOn the final day of the course, each team will present their work in a series of lightning talks."
  },
  {
    "objectID": "projects/upwelling_ml/intro.html#project-leads",
    "href": "projects/upwelling_ml/intro.html#project-leads",
    "title": "Using image machine learning to study upwelling in the Indian Ocean",
    "section": "Project Leads",
    "text": "Project Leads\n\nJiarui Yu (University of Washington), GitHub\nMinh Phan (University of Washington), GitHub\nEli Holmes (NOAA Fisheries/University of Washington), GitHub"
  },
  {
    "objectID": "projects/upwelling_ml/intro.html#background",
    "href": "projects/upwelling_ml/intro.html#background",
    "title": "Using image machine learning to study upwelling in the Indian Ocean",
    "section": "Background",
    "text": "Background\nStudy of coastal upwelling dynamics using image machine-learning algorithms applied to remote-sensing data PI: Eli Holmes, Northwest Fisheries Science Center, NOAA Fisheries, Seattle WA\nCoastal upwelling is a key driver of productive fisheries. It is largely driven by coastal winds that produce wind stress perpendicular to the coast. This wind stress pulls surface water away from the coast and pulls deep, cold, nutrient rich water to the surface. Strong and consistent coastal upwelling happens in specific regions around the world and supports strong fisheries in these regions. An interesting and curious aspect of upwelling is that it is influenced by ocean-atmosphere teleconnections‚Äîcorrelations in ocean climate across long-distances. In plain language, it means that a pattern that arises in one part of the world‚Äôs oceans will be seen later on the other side of the world in a very different part of the world. One of these teleconnections is between temperature anomalies in the North Atlantic Ocean and coastal upwelling in the Arabian Sea.\n\nIn this project, we are exploring the use of machine learning and image machine learning to study relationships between coastal upwelling and the physical oceanography of the Arabian Sea. The goal is to explore whether these approaches provide a promising way to develop predictive models for upwelling intensity. For the project, we are using Python libraries for implementing neural networks with image inputs, including TensorFlow, PyTorch, and Keras."
  },
  {
    "objectID": "python-notebooks/00_NASA_Earthdata_Authentication.html#summary",
    "href": "python-notebooks/00_NASA_Earthdata_Authentication.html#summary",
    "title": "Authentication for NASA Earthdata",
    "section": "Summary",
    "text": "Summary\nThis notebook creates a hidden .netrc file (_netrc for Window OS) with Earthdata login credentials in your home directory. This file is needed to access NASA Earthdata assets from a scripting environment like Python.\n\nEarthdata Login\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\nAuthentication via netrc File\nYou will need a netrc file containing your NASA Earthdata Login credentials in order to execute the notebooks. A netrc file can be created manually within text editor and saved to your home directory. An example of the required content is below.\nmachine urs.earthdata.nasa.gov\nlogin &lt;USERNAME&gt;\npassword &lt;PASSWORD&gt;\n&lt;USERNAME&gt; and &lt;PASSWORD&gt; would be replaced by your actual Earthdata Login username and password respectively."
  },
  {
    "objectID": "python-notebooks/00_NASA_Earthdata_Authentication.html#import-required-packages",
    "href": "python-notebooks/00_NASA_Earthdata_Authentication.html#import-required-packages",
    "title": "Authentication for NASA Earthdata",
    "section": "Import Required Packages",
    "text": "Import Required Packages\n\nfrom netrc import netrc\nfrom subprocess import Popen\nfrom platform import system\nfrom getpass import getpass\nimport os\n\nThe code below will:\n\ncheck what operating system (OS) is being used to determine which netrc file to check for/create (.netrc or _netrc)\ncheck if you have an netrc file, and if so, varify if those credentials are for the Earthdata endpoint\ncreate a netrc file if a netrc file is not present.\n\n\nurs = 'urs.earthdata.nasa.gov'    # Earthdata URL endpoint for authentication\nprompts = ['Enter NASA Earthdata Login Username: ',\n           'Enter NASA Earthdata Login Password: ']\n\n# Determine the OS (Windows machines usually use an '_netrc' file)\nnetrc_name = \"_netrc\" if system()==\"Windows\" else \".netrc\"\n\n# Determine if netrc file exists, and if so, if it includes NASA Earthdata Login Credentials\ntry:\n    netrcDir = os.path.expanduser(f\"~/{netrc_name}\")\n    netrc(netrcDir).authenticators(urs)[0]\n\n# Below, create a netrc file and prompt user for NASA Earthdata Login Username and Password\nexcept FileNotFoundError:\n    homeDir = os.path.expanduser(\"~\")\n    Popen('touch {0}{2} | echo machine {1} &gt;&gt; {0}{2}'.format(homeDir + os.sep, urs, netrc_name), shell=True)\n    Popen('echo login {} &gt;&gt; {}{}'.format(getpass(prompt=prompts[0]), homeDir + os.sep, netrc_name), shell=True)\n    Popen('echo \\'password {} \\'&gt;&gt; {}{}'.format(getpass(prompt=prompts[1]), homeDir + os.sep, netrc_name), shell=True)\n    # Set restrictive permissions\n    Popen('chmod 0600 {0}{1}'.format(homeDir + os.sep, netrc_name), shell=True)\n\n    # Determine OS and edit netrc file if it exists but is not set up for NASA Earthdata Login\nexcept TypeError:\n    homeDir = os.path.expanduser(\"~\")\n    Popen('echo machine {1} &gt;&gt; {0}{2}'.format(homeDir + os.sep, urs, netrc_name), shell=True)\n    Popen('echo login {} &gt;&gt; {}{}'.format(getpass(prompt=prompts[0]), homeDir + os.sep, netrc_name), shell=True)\n    Popen('echo \\'password {} \\'&gt;&gt; {}{}'.format(getpass(prompt=prompts[1]), homeDir + os.sep, netrc_name), shell=True)\n\n\nSee if the file was created\nIf the file was created, we‚Äôll see a .netrc file (_netrc for Window OS) in the list printed below. To view the contents from a Jupyter environment, click File on the top toolbar, select Open from Path‚Ä¶, type .netrc, and click Open. The .netrc file will open within the text editor.\n\n!!! Beware, your password will be visible if the .netrc file is opened in the text editor.\n\n\n!"
  },
  {
    "objectID": "python-notebooks/01_ERA5_S3_Downloader.html#variables",
    "href": "python-notebooks/01_ERA5_S3_Downloader.html#variables",
    "title": "Download Copernicus ERA5 Data with S3 without logging in",
    "section": "Variables",
    "text": "Variables\nThe table below lists the 18 ERA5 variables that are available on S3. All variables are surface or single level parameters sourced from the HRES sub-daily forecast stream.\n\n\n\n\nVariable Name\n\n\nFile Name\n\n\nVariable type (fc/an)\n\n\n\n\n10 metre U wind component\n\n\neastward_wind_at_10_metres.nc\n\n\nan\n\n\n\n\n10 metre V wind component\n\n\nnorthward_wind_at_10_metres.nc\n\n\nan\n\n\n\n\n100 metre U wind component\n\n\neastward_wind_at_100_metres.nc\n\n\nan\n\n\n\n\n100 metre V wind component\n\n\nnorthward_wind_at_100_metres.nc\n\n\nan\n\n\n\n\n2 metre dew point temperature\n\n\ndew_point_temperature_at_2_metres.nc\n\n\nan\n\n\n\n\n2 metre temperature\n\n\nair_temperature_at_2_metres.nc\n\n\nan\n\n\n\n\n2 metres maximum temperature since previous post-processing\n\n\nair_temperature_at_2_metres_1hour_Maximum.nc\n\n\nfc\n\n\n\n\n2 metres minimum temperature since previous post-processing\n\n\nair_temperature_at_2_metres_1hour_Minimum.nc\n\n\nfc\n\n\n\n\nMean sea level pressure\n\n\nair_pressure_at_mean_sea_level.nc\n\n\nan\n\n\n\n\nSea surface temperature\n\n\nsea_surface_temperature.nc\n\n\nan\n\n\n\n\nMean wave period\n\n\nsea_surface_wave_mean_period.nc\n\n\n\n\n\n\nMean direction of waves\n\n\nsea_surface_wave_from_direction.nc\n\n\n\n\n\n\nSignificant height of combined wind waves and swell\n\n\nsignificant_height_of_wind_and_swell_waves.nc\n\n\n\n\n\n\nSnow density\n\n\nsnow_density.nc\n\n\nan\n\n\n\n\nSnow depth\n\n\nlwe_thickness_of_surface_snow_amount.nc\n\n\nan\n\n\n\n\nSurface pressure\n\n\nsurface_air_pressure.nc\n\n\nan\n\n\n\n\nSurface solar radiation downwards\n\n\nintegral_wrt_time_of_surface_direct_downwelling_shortwave_flux_in_air_1hour_Accumulation.nc\n\n\nfc\n\n\n\n\nTotal precipitation\n\n\nprecipitation_amount_1hour_Accumulation.nc\n\n\nfc\n\n\n\n\nFor my dataset, we collect air temperature (at 2m), sea surface temperature, and u and v wind components so that we can compute speed and direction later."
  },
  {
    "objectID": "python-notebooks/01_ERA5_S3_Downloader.html#import-necessary-libraries",
    "href": "python-notebooks/01_ERA5_S3_Downloader.html#import-necessary-libraries",
    "title": "Download Copernicus ERA5 Data with S3 without logging in",
    "section": "Import necessary libraries",
    "text": "Import necessary libraries\n\nimport boto3\nimport botocore\nimport datetime\nimport matplotlib.pyplot as plt\nimport os\nimport xarray as xr\nimport numpy as np\nimport pandas as pd\nimport sys"
  },
  {
    "objectID": "python-notebooks/01_ERA5_S3_Downloader.html#download-data",
    "href": "python-notebooks/01_ERA5_S3_Downloader.html#download-data",
    "title": "Download Copernicus ERA5 Data with S3 without logging in",
    "section": "Download data",
    "text": "Download data\n\nera5_bucket = 'era5-pds'\nclient = boto3.client('s3', config=botocore.client.Config(signature_version=botocore.UNSIGNED))\n\n\ndef download_era5_s3(var_era5, month_start, month_end, lat1=5, lat2=25, lon1=60, lon2=80):\n    \"\"\"\n    var_era5: variable name\n    month_start: formatted as YYYY-MM \n    month_end: formatted as YYYY-MM (right-exclusive)\n    \"\"\"\n    s3_data_ptrn = '{year}/{month}/data/{var}.nc'\n    \n    path_temp_folder = 'demonstrated data/era5/temp'\n    path_var_folder = f'demonstrated data/era5/{var_era5}'\n    if not os.path.exists(path_temp_folder):\n        os.makedirs(path_temp_folder)\n    if not os.path.exists(path_var_folder):\n        os.makedirs(path_var_folder)\n    \n    data_file_ptrn = os.path.join(path_temp_folder,'{year}{month}_{var}.nc')\n    sliced_data_file_ptrn = os.path.join(path_var_folder, '{year}{month}_{var}.nc')\n    months = pd.date_range(month_start, month_end, freq='M')\n    for month in months:\n        s3_data_key = s3_data_ptrn.format(year=month.year, month=\"{:02d}\".format(month.month), var=var_era5)\n        data_file = data_file_ptrn.format(year=month.year, month=\"{:02d}\".format(month.month), var=var_era5)\n        if not os.path.isfile(data_file): # check if file already exists\n            print(\"Downloading %s from S3...\" % s3_data_key)\n            client.download_file(era5_bucket, s3_data_key, data_file)\n\n        export_file = sliced_data_file_ptrn.format(year = month.year, month = \"{:02d}\".format(month.month), var= var_era5)\n        xr.open_dataset(data_file).sel(lat=slice(lat2, lat1), lon=slice(lon1, lon2)).to_netcdf(export_file)\n        os.remove(data_file)\n\n\n# download data for 4 variables we need \n# consult available names in the table above in the file name column (remove .nc)\n\n# month_end is not included in dataset (right-exclusive)\ndownload_era5_s3(var_era5='eastward_wind_at_10_metres', month_start='2003-01', month_end='2003-03')\ndownload_era5_s3(var_era5='northward_wind_at_10_metres', month_start='2003-01', month_end='2003-03')\n\nDownloading 2003/01/data/eastward_wind_at_10_metres.nc from S3...\nDownloading 2003/02/data/eastward_wind_at_10_metres.nc from S3...\nDownloading 2003/01/data/northward_wind_at_10_metres.nc from S3...\nDownloading 2003/02/data/northward_wind_at_10_metres.nc from S3..."
  },
  {
    "objectID": "python-notebooks/01_ERDDAP_Downloader.html#get-to-know-griddap-url-link-template",
    "href": "python-notebooks/01_ERDDAP_Downloader.html#get-to-know-griddap-url-link-template",
    "title": "Download data from NOAA ERDDAP using GRIDDAP",
    "section": "Get to know GRIDDAP URL link template",
    "text": "Get to know GRIDDAP URL link template\nOne of the datasets that we used in this project is Reanalysis Data ERA5 monthly 3d Wind velocities. For the default options, select file type as .nc (NetCDF3-binary file) and click ‚ÄòJust generate the URL,‚Äô you can see the URL is then https://apdrc.soest.hawaii.edu/erddap/griddap/hawaii_soest_66d3_10d8_0f3c.nc?u[(2023-04-01T00:00:00Z):1:(2023-04-01T00:00:00Z)][(10.0):1:(1000.0)][(-90.0):1:(90.0)][(0.0):1:(359.75)],v[(2023-04-01T00:00:00Z):1:(2023-04-01T00:00:00Z)][(10.0):1:(1000.0)][(-90.0):1:(90.0)][(0.0):1:(359.75)] and it follows this template:\n\nhttps://apdrc.soest.hawaii.edu/erddap/griddap\n\n/DATASET_ID.FILE_EXTENSION\nfollowing by\nVARIABLE_NAME1 [(START_DIM1):STRIDE_DIM1:(STOP_DIM1)][(START_DIM2):STRIDE_DIM2:(STOP_DIM2)][(START_DIM3):STRIDE_DIM3:(STOP_DIM3)]‚Ä¶\nand then\nVARIABLE_NAME2[(START_DIM1):STRIDE_DIM1:(STOP_DIM1)][(START_DIM2):STRIDE_DIM2:(STOP_DIM2)][(START_DIM3):STRIDE_DIM3:(STOP_DIM3)]‚Ä¶\nand so on for all the variables. To illustrate this template, you can liken it to downloading every variable as an Xarray DataArray, and combine them together based on the coordinates. In the URL above, the order and specifications of the dimension slicing options must be the consistent for all variables, just like how Xarray combining by coords only works if the coordinates are so.\nThe template is also well-explained in GRIDDAP documentation.\nWe can all agree that trying to write down every template for different datasets is error-prone and tedious, so you are more than welcome to copy and modify your template using the original file generator Python formatted strings to help with generating URLs for downloads. Since URLs vary greatly form dataset to dataset, there is no ideal URL template to consider, but we will still develop a function to generalize the URL generator to the best of my knowledge.\nNOTE: for the URL template, we focus on downloading smaller chunks of the dataset, sliced temporally just like downloading from the EarthData database so that the remote server would not be broken handling the request. We also recommend you to download variables together in one NetCDF3 file (multiple variable names in one URL) instead of individual variables as it takes time piecing them all together."
  },
  {
    "objectID": "python-notebooks/01_ERDDAP_Downloader.html#import-necessary-libraries",
    "href": "python-notebooks/01_ERDDAP_Downloader.html#import-necessary-libraries",
    "title": "Download data from NOAA ERDDAP using GRIDDAP",
    "section": "Import necessary libraries",
    "text": "Import necessary libraries\n\nimport xarray as xr\nimport numpy as np\nimport pandas as pd\nimport os, glob, shutil, urllib3"
  },
  {
    "objectID": "python-notebooks/01_ERDDAP_Downloader.html#download-data",
    "href": "python-notebooks/01_ERDDAP_Downloader.html#download-data",
    "title": "Download data from NOAA ERDDAP using GRIDDAP",
    "section": "Download data",
    "text": "Download data\nUnlike NASA Earthdata, we can slice dataset spatially directly for every server request.\n\ndef download_ERDDAP_URLs(var):\n    # get list of URLs\n    URLs, months = ERDDAP_URL_template_generator(var)\n    http = urllib3.PoolManager()\n    \n    MAIN_FOLDER = 'demonstrated data/ERDDAP'\n    TEMP_FOLDER = var['datasetID']\n    path_temp_folder = os.path.join(MAIN_FOLDER, TEMP_FOLDER)\n    \n    if not os.path.exists(path_temp_folder):\n        os.makedirs(path_temp_folder)\n    \n    for URL, month in zip(URLs, months):\n        fileout = os.path.join(path_temp_folder, f\"{month.strftime('%Y%m')}.nc\")\n        \n        # download\n        with http.request('GET', URL, preload_content=False) as resp, open(fileout, 'wb') as out_file:\n            shutil.copyfileobj(resp, out_file)\n        \n        resp.release_conn()\n        \n        print('Downloaded ' + fileout)\n\n\ndef ERDDAP_URL_template_generator(var) -&gt; list():\n    \"\"\"\n    Generates a list of ERDDAP URLs for downloading\n    \n    Example of parameters shown below in next code cell.\n        - datasetID: 'erdTAgeo1day_LonPM180'\n        - variable_name ['u', 'v']\n        - coordinates: {'time': '2000-01:1:2000-03', 'altitude': '0:1:0', 'latitude': '-12:1:12', 'longitude': '42:1:102'}\n    NOTE: 'time' dimension must be listed in order (typical of ERDDAP URLs)\n    \"\"\"\n\n    list_of_URLS = []\n    time_ele = dimension_boundaries_ordered['time'].split(':')\n    start_month = time_ele[0]\n    end_month = time_ele[2]\n    stride_time = time_ele[1]\n    months = pd.date_range(start_month, end_month, freq='M')\n    for month_idx in range(len(months)):\n        URL = f'https://apdrc.soest.hawaii.edu/erddap/griddap/{var_dict['datasetID']}.nc?'\n        for var in var_dict['variable_names']:\n            URL = URL + var + f\"[({months[month_idx].strftime('%Y-%m')}-01):{stride_time}:({months[month_idx].strftime('%Y-%m-%d')})]\"\n            for d in list(var_dict['coordinates'].values())[1:]:\n                d_ele = d.split(':')\n                URL = URL + f\"[({d_ele[0]}):{d_ele[1]}:({d_ele[2]})]\"\n            URL = URL + \",\"\n        \n        # eliminate last comma\n        list_of_URLS.append(URL[:-1])\n        \n        # example\n        # url = \"https://apdrc.soest.hawaii.edu/erddap/griddap/hawaii_soest_66d3_10d8_0f3c.nc?\"\\\n        # f\"u[({months[month_idx].strftime('%Y-%m')}-01):1:({months[month_idx+1].strftime('%Y-%m')})][(10):1:(10)][(-12):1:(32)][(42):1:(102)],\"\\\n        # f\"v[({months[month_idx].strftime('%Y-%m')}-01):1:({months[month_idx+1].strftime('%Y-%m')})][(10):1:(10)][(-12):1:(32)][(42):1:(102)]\"\n    return list_of_URLS, months\n\n\nvar = {\n    'datasetID': 'hawaii_soest_66d3_10d8_0f3c',\n    'variable_names': ['u', 'v'],\n    'coordinates': {'time': '2000-01:1:2000-10', # must be in order of the ERDDAP dataset dimension list\n                    'LEV': '10:1:10', \n                    'latitude': '-12:1:12', \n                    'longitude': '42:1:102'}\n}\n\nERDDAP_URL_template_generator(datasetID='hawaii_soest_66d3_10d8_0f3c', variable_names=['u', 'v'], dimension_boundaries_ordered={'time': '2000-01:1:2000-10', 'LEV': '10:1:10', 'latitude': '-12:1:12', 'longitude': '42:1:102'})\n\n['https://apdrc.soest.hawaii.edu/erddap/griddap/hawaii_soest_66d3_10d8_0f3c.nc?u[(2000-01-01):1:(2000-01-31)][(10):1:(10)][(-12):1:(12)][(42):1:(102)],v[(2000-01-01):1:(2000-01-31)][(10):1:(10)][(-12):1:(12)][(42):1:(102)]',\n 'https://apdrc.soest.hawaii.edu/erddap/griddap/hawaii_soest_66d3_10d8_0f3c.nc?u[(2000-02-01):1:(2000-02-29)][(10):1:(10)][(-12):1:(12)][(42):1:(102)],v[(2000-02-01):1:(2000-02-29)][(10):1:(10)][(-12):1:(12)][(42):1:(102)]',\n 'https://apdrc.soest.hawaii.edu/erddap/griddap/hawaii_soest_66d3_10d8_0f3c.nc?u[(2000-03-01):1:(2000-03-31)][(10):1:(10)][(-12):1:(12)][(42):1:(102)],v[(2000-03-01):1:(2000-03-31)][(10):1:(10)][(-12):1:(12)][(42):1:(102)]',\n 'https://apdrc.soest.hawaii.edu/erddap/griddap/hawaii_soest_66d3_10d8_0f3c.nc?u[(2000-04-01):1:(2000-04-30)][(10):1:(10)][(-12):1:(12)][(42):1:(102)],v[(2000-04-01):1:(2000-04-30)][(10):1:(10)][(-12):1:(12)][(42):1:(102)]',\n 'https://apdrc.soest.hawaii.edu/erddap/griddap/hawaii_soest_66d3_10d8_0f3c.nc?u[(2000-05-01):1:(2000-05-31)][(10):1:(10)][(-12):1:(12)][(42):1:(102)],v[(2000-05-01):1:(2000-05-31)][(10):1:(10)][(-12):1:(12)][(42):1:(102)]',\n 'https://apdrc.soest.hawaii.edu/erddap/griddap/hawaii_soest_66d3_10d8_0f3c.nc?u[(2000-06-01):1:(2000-06-30)][(10):1:(10)][(-12):1:(12)][(42):1:(102)],v[(2000-06-01):1:(2000-06-30)][(10):1:(10)][(-12):1:(12)][(42):1:(102)]',\n 'https://apdrc.soest.hawaii.edu/erddap/griddap/hawaii_soest_66d3_10d8_0f3c.nc?u[(2000-07-01):1:(2000-07-31)][(10):1:(10)][(-12):1:(12)][(42):1:(102)],v[(2000-07-01):1:(2000-07-31)][(10):1:(10)][(-12):1:(12)][(42):1:(102)]',\n 'https://apdrc.soest.hawaii.edu/erddap/griddap/hawaii_soest_66d3_10d8_0f3c.nc?u[(2000-08-01):1:(2000-08-31)][(10):1:(10)][(-12):1:(12)][(42):1:(102)],v[(2000-08-01):1:(2000-08-31)][(10):1:(10)][(-12):1:(12)][(42):1:(102)]',\n 'https://apdrc.soest.hawaii.edu/erddap/griddap/hawaii_soest_66d3_10d8_0f3c.nc?u[(2000-09-01):1:(2000-09-30)][(10):1:(10)][(-12):1:(12)][(42):1:(102)],v[(2000-09-01):1:(2000-09-30)][(10):1:(10)][(-12):1:(12)][(42):1:(102)]']"
  },
  {
    "objectID": "python-notebooks/01_MOTU_Downloader.html#store-credentials-in-bash",
    "href": "python-notebooks/01_MOTU_Downloader.html#store-credentials-in-bash",
    "title": "Download sliced data using MOTU-Client in Copernicus",
    "section": "Store credentials in bash",
    "text": "Store credentials in bash\nFor security purposes, you can store your account credentials in your bash profile if you don‚Äôt want others to know yours. If you fork this notebook on your local machine and are sure that your credentials cannot be exposed anywhere else, you can skip the hassle of saving them to bash and set them as Python variables, then feed them to the prompt generator.\nMake sure to change your username and password of your Copernicus/MOTU account to the two lines below for bash‚Ä¶\n\n! echo 'export motu_password=YOURPASSWORD' &gt;~/.bash_profile\n\n\n! echo 'export motu_username=YOURUSERNAME' &gt;&gt;~/.bash_profile\n\n\n# Vim will get stuck in this cell if you don't stop the cell. As soon as you see the two lines\n# stop the execution and move on to the next cell.\n! vim ~/.bash_profile\n\n\n! source ~/.bash_profile\n\nIMPORTANT Please restart your server for the changes to take effect\n\n# confirm that it is shown before proceeding\n! echo $motu_username"
  },
  {
    "objectID": "python-notebooks/01_MOTU_Downloader.html#motu-download-prompt-breakdown",
    "href": "python-notebooks/01_MOTU_Downloader.html#motu-download-prompt-breakdown",
    "title": "Download sliced data using MOTU-Client in Copernicus",
    "section": "MOTU download prompt breakdown",
    "text": "MOTU download prompt breakdown\nLike ERDDAP URLs, MOTU works by sending requests to the API server. Unlike the former, it works through command line requests instead of URLs requests. Again, to prevent overloadding, we will break down the process to smaller time ranges to download.\nHere‚Äôs an example of a MOTU API request\n\npython -m motuclient ‚Äìmotu https://my.cmems-du.eu/motu-web/Motu ‚Äìservice-id GLOBAL_MULTIYEAR_BGC_001_029-TDS ‚Äìproduct-id cmems_mod_glo_bgc_my_0.25_P1D-m ‚Äìlongitude-min 60 ‚Äìlongitude-max 80 ‚Äìlatitude-min 5 ‚Äìlatitude-max 25 ‚Äìdate-min ‚Äú2020-12-31 00:00:00‚Äù ‚Äìdate-max ‚Äú2020-12-31 23:59:59‚Äù ‚Äìdepth-min 0.5057600140571594 ‚Äìdepth-max 16.525320053100586 ‚Äìvariable chl ‚Äìvariable po4 ‚Äìvariable no3 ‚Äìout-dir [OUTPUT_DIRECTORY] ‚Äìout-name [OUTPUT_FILENAME] ‚Äìuser [USERNAME] ‚Äìpwd [PASSWORD]\n\nAs you can see, we have to provide all the parameters in order for the request to work, with temporal and lat/lon ranges being required on all datasets and depth range required only for some. We also have to provide username and password (that we have after creating the Copernicus account) , as well as where we are going to store the data (output directory and name)."
  },
  {
    "objectID": "python-notebooks/01_MOTU_Downloader.html#import-necessary-libraries",
    "href": "python-notebooks/01_MOTU_Downloader.html#import-necessary-libraries",
    "title": "Download sliced data using MOTU-Client in Copernicus",
    "section": "Import necessary libraries",
    "text": "Import necessary libraries\n\nimport xarray as xr\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os, glob, subprocess"
  },
  {
    "objectID": "python-notebooks/01_MOTU_Downloader.html#download-data",
    "href": "python-notebooks/01_MOTU_Downloader.html#download-data",
    "title": "Download sliced data using MOTU-Client in Copernicus",
    "section": "Download data",
    "text": "Download data\n\ndef prompt_generator(params, temp_folder_path):\n    months = pd.date_range(params['coords']['date-min'], params['coords']['date-max'], freq='M')\n    prompts = []\n    \n    for m in months:\n        prompt = f\"motuclient --motu https://my.cmems-du.eu/motu-web/Motu --service-id {params['SERVICE_ID']}-TDS --product-id {params['PRODUCT_ID']} \" + \\\n        f\"--user {os.environ['motu_username']} --pwd {os.environ['motu_password']} --out-dir {temp_folder_path} --out-name {m.strftime('%Y%m')}.nc \"\n        prompt += f\"--date-min '{m.strftime('%Y-%m')}-01 00:00:00' --date-max '{m.strftime('%Y-%m-%d')} 23:59:00' \"\n        for var in params['variables']:\n            prompt += f'--variable {var} '\n        for coord_name, coord_value in params['coords'].items():\n            # since we already work \n            if coord_name not in ['date-min', 'date-max']:\n                prompt += f'--{coord_name} {coord_value} '\n        \n        prompts.append(prompt)\n        \n    return prompts\n\n\ndef download_motu_api(params):\n    # create new folder to store data\n    MAIN_FOLDER = 'demonstrated data/motu'\n    TEMP_FOLDER = params['PRODUCT_ID']\n    path_temp_folder = os.path.join(MAIN_FOLDER, TEMP_FOLDER)\n    \n    if not os.path.exists(path_temp_folder):\n        os.makedirs(path_temp_folder)\n\n    # generate prompts to feed into the downloader\n    prompts = prompt_generator(params, path_temp_folder)\n    for prompt in prompts:\n        process = subprocess.Popen(prompt.split())\n        process.wait()\n        \n\nNext, we‚Äôre going to initialize our parameters to download the data. The template is shown below, and werecommend you to choose a spatial range slightly larger than your actual region of interest due to cutoffs that may happen at the edges.\n\n# modify your parameters as you wish to feed into prompt generator\n# here's mine to download chlorophyll-a data\nvar_chlorophyll = {\n    'SERVICE_ID': 'OCEANCOLOUR_GLO_BGC_L4_MY_009_104',\n    'PRODUCT_ID': 'cmems_obs-oc_glo_bgc-plankton_my_l4-gapfree-multi-4km_P1D',\n    'variables': ['CHL', 'CHL_uncertainty'],\n    'coords': {\n        'longitude-min': 60, \n        'longitude-max': 80,\n        'latitude-min': 5,\n        'latitude-max': 25,\n        'date-min': '2000-01',\n        'date-max': '2002-01' # exclusive of last month, so in this case, we get data until DEC '02\n        # optional coordinate params for some datasets: 'depth-min', 'depth-max'\n        # consult data access page for more information\n    }\n}\n\nvar_sla = {\n    'SERVICE_ID': 'SEALEVEL_GLO_PHY_L4_MY_008_047',\n    'PRODUCT_ID': 'cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D',\n    'variables': ['adt', 'sla'],\n    'coords': {\n        'longitude-min': 60,\n        'longitude-max': 80,\n        'latitude-min': 5,\n        'latitude-max': 25,\n        'date-min': '2000-01',\n        'date-max': '2002-01' # exclusive of last month, so in this case, we get data until DEC '02\n    }\n}\n\n\ndownload_motu_api(var_sla)\n\n2023-07-11 04:31:55.819 [ INFO] Asynchronous mode set\n2023-07-11 04:31:55.819 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:32:04.748 [ INFO] Requesting file to download (this can take a while)...\n2023-07-11 04:32:14.896 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:32:32.754 [ INFO] The product is ready for download\n2023-07-11 04:32:32.754 [ INFO] Downloading file (this can take a while)...\n2023-07-11 04:32:33.680 [ INFO] File type: application/x-netcdf\n2023-07-11 04:32:33.680 [ INFO] File size: 3.3 MB (3260684 B)\n2023-07-11 04:32:33.680 [ INFO] Downloading file /home/jovyan/shared/minh-notebooks/motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200001.nc\n2023-07-11 04:32:34.029 [ INFO] Progress -   3.3 MB (2.0%)\n2023-07-11 04:32:34.204 [ INFO] Progress -   3.3 MB (4.0%)\n2023-07-11 04:32:34.379 [ INFO] Progress -   3.3 MB (6.0%)\n2023-07-11 04:32:34.380 [ INFO] Progress -   3.3 MB (8.0%)\n2023-07-11 04:32:34.380 [ INFO] Progress -   3.3 MB (10.0%)\n2023-07-11 04:32:34.555 [ INFO] Progress -   3.3 MB (12.1%)\n2023-07-11 04:32:34.555 [ INFO] Progress -   3.3 MB (14.1%)\n2023-07-11 04:32:34.556 [ INFO] Progress -   3.3 MB (16.1%)\n2023-07-11 04:32:34.556 [ INFO] Progress -   3.3 MB (18.1%)\n2023-07-11 04:32:34.556 [ INFO] Progress -   3.3 MB (20.1%)\n2023-07-11 04:32:34.556 [ INFO] Progress -   3.3 MB (22.1%)\n2023-07-11 04:32:34.557 [ INFO] Progress -   3.3 MB (24.1%)\n2023-07-11 04:32:34.730 [ INFO] Progress -   3.3 MB (26.1%)\n2023-07-11 04:32:34.731 [ INFO] Progress -   3.3 MB (28.1%)\n2023-07-11 04:32:34.731 [ INFO] Progress -   3.3 MB (30.1%)\n2023-07-11 04:32:34.732 [ INFO] Progress -   3.3 MB (32.2%)\n2023-07-11 04:32:34.732 [ INFO] Progress -   3.3 MB (34.2%)\n2023-07-11 04:32:34.733 [ INFO] Progress -   3.3 MB (36.2%)\n2023-07-11 04:32:34.733 [ INFO] Progress -   3.3 MB (38.2%)\n2023-07-11 04:32:34.733 [ INFO] Progress -   3.3 MB (40.2%)\n2023-07-11 04:32:34.905 [ INFO] Progress -   3.3 MB (42.2%)\n2023-07-11 04:32:34.906 [ INFO] Progress -   3.3 MB (44.2%)\n2023-07-11 04:32:34.906 [ INFO] Progress -   3.3 MB (46.2%)\n2023-07-11 04:32:34.907 [ INFO] Progress -   3.3 MB (48.2%)\n2023-07-11 04:32:34.907 [ INFO] Progress -   3.3 MB (50.2%)\n2023-07-11 04:32:34.907 [ INFO] Progress -   3.3 MB (52.3%)\n2023-07-11 04:32:34.908 [ INFO] Progress -   3.3 MB (54.3%)\n2023-07-11 04:32:34.908 [ INFO] Progress -   3.3 MB (56.3%)\n2023-07-11 04:32:34.908 [ INFO] Progress -   3.3 MB (58.3%)\n2023-07-11 04:32:34.908 [ INFO] Progress -   3.3 MB (60.3%)\n2023-07-11 04:32:34.909 [ INFO] Progress -   3.3 MB (62.3%)\n2023-07-11 04:32:34.909 [ INFO] Progress -   3.3 MB (64.3%)\n2023-07-11 04:32:34.910 [ INFO] Progress -   3.3 MB (66.3%)\n2023-07-11 04:32:34.910 [ INFO] Progress -   3.3 MB (68.3%)\n2023-07-11 04:32:34.910 [ INFO] Progress -   3.3 MB (70.3%)\n2023-07-11 04:32:34.910 [ INFO] Progress -   3.3 MB (72.4%)\n2023-07-11 04:32:34.911 [ INFO] Progress -   3.3 MB (74.4%)\n2023-07-11 04:32:34.911 [ INFO] Progress -   3.3 MB (76.4%)\n2023-07-11 04:32:34.911 [ INFO] Progress -   3.3 MB (78.4%)\n2023-07-11 04:32:34.911 [ INFO] Progress -   3.3 MB (80.4%)\n2023-07-11 04:32:34.912 [ INFO] Progress -   3.3 MB (82.4%)\n2023-07-11 04:32:35.080 [ INFO] Progress -   3.3 MB (84.4%)\n2023-07-11 04:32:35.081 [ INFO] Progress -   3.3 MB (86.4%)\n2023-07-11 04:32:35.082 [ INFO] Progress -   3.3 MB (88.4%)\n2023-07-11 04:32:35.082 [ INFO] Progress -   3.3 MB (90.4%)\n2023-07-11 04:32:35.082 [ INFO] Progress -   3.3 MB (92.5%)\n2023-07-11 04:32:35.082 [ INFO] Progress -   3.3 MB (94.5%)\n2023-07-11 04:32:35.083 [ INFO] Progress -   3.3 MB (96.5%)\n2023-07-11 04:32:35.083 [ INFO] Progress -   3.3 MB (98.5%)\n2023-07-11 04:32:35.084 [ INFO] Progress -   3.3 MB (100.0%)\n2023-07-11 04:32:35.084 [ INFO] Processing  time : 0:00:37.863885\n2023-07-11 04:32:35.084 [ INFO] Downloading time : 0:00:01.403882\n2023-07-11 04:32:35.084 [ INFO] Total time       : 0:00:39.267767\n2023-07-11 04:32:35.084 [ INFO] Download rate    : 1.4 MB/s\n2023-07-11 04:32:35.084 [ INFO] Save into        : motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200001.nc\n2023-07-11 04:32:35.115 [ INFO] Done\n2023-07-11 04:32:35.360 [ INFO] Asynchronous mode set\n2023-07-11 04:32:35.360 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:32:44.852 [ INFO] Requesting file to download (this can take a while)...\n2023-07-11 04:32:53.844 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:33:09.545 [ INFO] The product is ready for download\n2023-07-11 04:33:09.546 [ INFO] Downloading file (this can take a while)...\n2023-07-11 04:33:10.291 [ INFO] File type: application/x-netcdf\n2023-07-11 04:33:10.291 [ INFO] File size: 3.1 MB (3050724 B)\n2023-07-11 04:33:10.291 [ INFO] Downloading file /home/jovyan/shared/minh-notebooks/motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200002.nc\n2023-07-11 04:33:10.642 [ INFO] Progress -   3.1 MB (2.1%)\n2023-07-11 04:33:10.817 [ INFO] Progress -   3.1 MB (4.3%)\n2023-07-11 04:33:10.990 [ INFO] Progress -   3.1 MB (6.4%)\n2023-07-11 04:33:10.992 [ INFO] Progress -   3.1 MB (8.6%)\n2023-07-11 04:33:10.992 [ INFO] Progress -   3.1 MB (10.7%)\n2023-07-11 04:33:10.993 [ INFO] Progress -   3.1 MB (12.9%)\n2023-07-11 04:33:11.166 [ INFO] Progress -   3.1 MB (15.0%)\n2023-07-11 04:33:11.167 [ INFO] Progress -   3.1 MB (17.2%)\n2023-07-11 04:33:11.168 [ INFO] Progress -   3.1 MB (19.3%)\n2023-07-11 04:33:11.168 [ INFO] Progress -   3.1 MB (21.5%)\n2023-07-11 04:33:11.168 [ INFO] Progress -   3.1 MB (23.6%)\n2023-07-11 04:33:11.168 [ INFO] Progress -   3.1 MB (25.8%)\n2023-07-11 04:33:11.341 [ INFO] Progress -   3.1 MB (27.9%)\n2023-07-11 04:33:11.341 [ INFO] Progress -   3.1 MB (30.1%)\n2023-07-11 04:33:11.342 [ INFO] Progress -   3.1 MB (32.2%)\n2023-07-11 04:33:11.343 [ INFO] Progress -   3.1 MB (34.4%)\n2023-07-11 04:33:11.343 [ INFO] Progress -   3.1 MB (36.5%)\n2023-07-11 04:33:11.344 [ INFO] Progress -   3.1 MB (38.7%)\n2023-07-11 04:33:11.344 [ INFO] Progress -   3.1 MB (40.8%)\n2023-07-11 04:33:11.344 [ INFO] Progress -   3.1 MB (43.0%)\n2023-07-11 04:33:11.344 [ INFO] Progress -   3.1 MB (45.1%)\n2023-07-11 04:33:11.345 [ INFO] Progress -   3.1 MB (47.3%)\n2023-07-11 04:33:11.345 [ INFO] Progress -   3.1 MB (49.4%)\n2023-07-11 04:33:11.345 [ INFO] Progress -   3.1 MB (51.6%)\n2023-07-11 04:33:11.345 [ INFO] Progress -   3.1 MB (53.7%)\n2023-07-11 04:33:11.516 [ INFO] Progress -   3.1 MB (55.9%)\n2023-07-11 04:33:11.516 [ INFO] Progress -   3.1 MB (58.0%)\n2023-07-11 04:33:11.516 [ INFO] Progress -   3.1 MB (60.1%)\n2023-07-11 04:33:11.517 [ INFO] Progress -   3.1 MB (62.3%)\n2023-07-11 04:33:11.517 [ INFO] Progress -   3.1 MB (64.4%)\n2023-07-11 04:33:11.518 [ INFO] Progress -   3.1 MB (66.6%)\n2023-07-11 04:33:11.518 [ INFO] Progress -   3.1 MB (68.7%)\n2023-07-11 04:33:11.519 [ INFO] Progress -   3.1 MB (70.9%)\n2023-07-11 04:33:11.519 [ INFO] Progress -   3.1 MB (73.0%)\n2023-07-11 04:33:11.519 [ INFO] Progress -   3.1 MB (75.2%)\n2023-07-11 04:33:11.520 [ INFO] Progress -   3.1 MB (77.3%)\n2023-07-11 04:33:11.520 [ INFO] Progress -   3.1 MB (79.5%)\n2023-07-11 04:33:11.520 [ INFO] Progress -   3.1 MB (81.6%)\n2023-07-11 04:33:11.520 [ INFO] Progress -   3.1 MB (83.8%)\n2023-07-11 04:33:11.520 [ INFO] Progress -   3.1 MB (85.9%)\n2023-07-11 04:33:11.521 [ INFO] Progress -   3.1 MB (88.1%)\n2023-07-11 04:33:11.521 [ INFO] Progress -   3.1 MB (90.2%)\n2023-07-11 04:33:11.521 [ INFO] Progress -   3.1 MB (92.4%)\n2023-07-11 04:33:11.521 [ INFO] Progress -   3.1 MB (94.5%)\n2023-07-11 04:33:11.522 [ INFO] Progress -   3.1 MB (96.7%)\n2023-07-11 04:33:11.522 [ INFO] Progress -   3.1 MB (98.8%)\n2023-07-11 04:33:11.522 [ INFO] Progress -   3.1 MB (100.0%)\n2023-07-11 04:33:11.522 [ INFO] Processing  time : 0:00:34.933586\n2023-07-11 04:33:11.522 [ INFO] Downloading time : 0:00:01.231103\n2023-07-11 04:33:11.522 [ INFO] Total time       : 0:00:36.164689\n2023-07-11 04:33:11.522 [ INFO] Download rate    : 1.5 MB/s\n2023-07-11 04:33:11.522 [ INFO] Save into        : motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200002.nc\n2023-07-11 04:33:11.556 [ INFO] Done\n2023-07-11 04:33:11.798 [ INFO] Asynchronous mode set\n2023-07-11 04:33:11.798 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:33:19.952 [ INFO] Requesting file to download (this can take a while)...\n2023-07-11 04:33:28.668 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:33:44.971 [ INFO] The product is ready for download\n2023-07-11 04:33:44.971 [ INFO] Downloading file (this can take a while)...\n2023-07-11 04:33:45.715 [ INFO] File type: application/x-netcdf\n2023-07-11 04:33:45.715 [ INFO] File size: 3.3 MB (3260684 B)\n2023-07-11 04:33:45.715 [ INFO] Downloading file /home/jovyan/shared/minh-notebooks/motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200003.nc\n2023-07-11 04:33:46.064 [ INFO] Progress -   3.3 MB (2.0%)\n2023-07-11 04:33:46.239 [ INFO] Progress -   3.3 MB (4.0%)\n2023-07-11 04:33:46.415 [ INFO] Progress -   3.3 MB (6.0%)\n2023-07-11 04:33:46.415 [ INFO] Progress -   3.3 MB (8.0%)\n2023-07-11 04:33:46.415 [ INFO] Progress -   3.3 MB (10.0%)\n2023-07-11 04:33:46.590 [ INFO] Progress -   3.3 MB (12.1%)\n2023-07-11 04:33:46.590 [ INFO] Progress -   3.3 MB (14.1%)\n2023-07-11 04:33:46.591 [ INFO] Progress -   3.3 MB (16.1%)\n2023-07-11 04:33:46.591 [ INFO] Progress -   3.3 MB (18.1%)\n2023-07-11 04:33:46.591 [ INFO] Progress -   3.3 MB (20.1%)\n2023-07-11 04:33:46.591 [ INFO] Progress -   3.3 MB (22.1%)\n2023-07-11 04:33:46.765 [ INFO] Progress -   3.3 MB (24.1%)\n2023-07-11 04:33:46.765 [ INFO] Progress -   3.3 MB (26.1%)\n2023-07-11 04:33:46.765 [ INFO] Progress -   3.3 MB (28.1%)\n2023-07-11 04:33:46.766 [ INFO] Progress -   3.3 MB (30.1%)\n2023-07-11 04:33:46.766 [ INFO] Progress -   3.3 MB (32.2%)\n2023-07-11 04:33:46.767 [ INFO] Progress -   3.3 MB (34.2%)\n2023-07-11 04:33:46.767 [ INFO] Progress -   3.3 MB (36.2%)\n2023-07-11 04:33:46.768 [ INFO] Progress -   3.3 MB (38.2%)\n2023-07-11 04:33:46.768 [ INFO] Progress -   3.3 MB (40.2%)\n2023-07-11 04:33:46.768 [ INFO] Progress -   3.3 MB (42.2%)\n2023-07-11 04:33:46.768 [ INFO] Progress -   3.3 MB (44.2%)\n2023-07-11 04:33:46.769 [ INFO] Progress -   3.3 MB (46.2%)\n2023-07-11 04:33:46.769 [ INFO] Progress -   3.3 MB (48.2%)\n2023-07-11 04:33:46.940 [ INFO] Progress -   3.3 MB (50.2%)\n2023-07-11 04:33:46.940 [ INFO] Progress -   3.3 MB (52.3%)\n2023-07-11 04:33:46.940 [ INFO] Progress -   3.3 MB (54.3%)\n2023-07-11 04:33:46.941 [ INFO] Progress -   3.3 MB (56.3%)\n2023-07-11 04:33:46.941 [ INFO] Progress -   3.3 MB (58.3%)\n2023-07-11 04:33:46.941 [ INFO] Progress -   3.3 MB (60.3%)\n2023-07-11 04:33:46.942 [ INFO] Progress -   3.3 MB (62.3%)\n2023-07-11 04:33:46.942 [ INFO] Progress -   3.3 MB (64.3%)\n2023-07-11 04:33:46.943 [ INFO] Progress -   3.3 MB (66.3%)\n2023-07-11 04:33:46.943 [ INFO] Progress -   3.3 MB (68.3%)\n2023-07-11 04:33:46.943 [ INFO] Progress -   3.3 MB (70.3%)\n2023-07-11 04:33:46.944 [ INFO] Progress -   3.3 MB (72.4%)\n2023-07-11 04:33:46.944 [ INFO] Progress -   3.3 MB (74.4%)\n2023-07-11 04:33:46.944 [ INFO] Progress -   3.3 MB (76.4%)\n2023-07-11 04:33:46.945 [ INFO] Progress -   3.3 MB (78.4%)\n2023-07-11 04:33:46.945 [ INFO] Progress -   3.3 MB (80.4%)\n2023-07-11 04:33:46.945 [ INFO] Progress -   3.3 MB (82.4%)\n2023-07-11 04:33:46.945 [ INFO] Progress -   3.3 MB (84.4%)\n2023-07-11 04:33:46.946 [ INFO] Progress -   3.3 MB (86.4%)\n2023-07-11 04:33:46.946 [ INFO] Progress -   3.3 MB (88.4%)\n2023-07-11 04:33:46.946 [ INFO] Progress -   3.3 MB (90.4%)\n2023-07-11 04:33:46.946 [ INFO] Progress -   3.3 MB (92.5%)\n2023-07-11 04:33:46.947 [ INFO] Progress -   3.3 MB (94.5%)\n2023-07-11 04:33:46.947 [ INFO] Progress -   3.3 MB (96.5%)\n2023-07-11 04:33:46.988 [ INFO] Progress -   3.3 MB (98.5%)\n2023-07-11 04:33:47.115 [ INFO] Progress -   3.3 MB (100.0%)\n2023-07-11 04:33:47.115 [ INFO] Processing  time : 0:00:33.919354\n2023-07-11 04:33:47.115 [ INFO] Downloading time : 0:00:01.400042\n2023-07-11 04:33:47.115 [ INFO] Total time       : 0:00:35.319396\n2023-07-11 04:33:47.115 [ INFO] Download rate    : 1.5 MB/s\n2023-07-11 04:33:47.115 [ INFO] Save into        : motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200003.nc\n2023-07-11 04:33:47.158 [ INFO] Done\n2023-07-11 04:33:47.403 [ INFO] Asynchronous mode set\n2023-07-11 04:33:47.403 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:33:55.632 [ INFO] Requesting file to download (this can take a while)...\n2023-07-11 04:34:05.423 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:34:21.086 [ INFO] The product is ready for download\n2023-07-11 04:34:21.086 [ INFO] Downloading file (this can take a while)...\n2023-07-11 04:34:21.835 [ INFO] File type: application/x-netcdf\n2023-07-11 04:34:21.835 [ INFO] File size: 3.2 MB (3155704 B)\n2023-07-11 04:34:21.835 [ INFO] Downloading file /home/jovyan/shared/minh-notebooks/motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200004.nc\n2023-07-11 04:34:22.184 [ INFO] Progress -   3.2 MB (2.1%)\n2023-07-11 04:34:22.359 [ INFO] Progress -   3.2 MB (4.2%)\n2023-07-11 04:34:22.535 [ INFO] Progress -   3.2 MB (6.2%)\n2023-07-11 04:34:22.535 [ INFO] Progress -   3.2 MB (8.3%)\n2023-07-11 04:34:22.535 [ INFO] Progress -   3.2 MB (10.4%)\n2023-07-11 04:34:22.710 [ INFO] Progress -   3.2 MB (12.5%)\n2023-07-11 04:34:22.711 [ INFO] Progress -   3.2 MB (14.5%)\n2023-07-11 04:34:22.711 [ INFO] Progress -   3.2 MB (16.6%)\n2023-07-11 04:34:22.711 [ INFO] Progress -   3.2 MB (18.7%)\n2023-07-11 04:34:22.712 [ INFO] Progress -   3.2 MB (20.8%)\n2023-07-11 04:34:22.712 [ INFO] Progress -   3.2 MB (22.8%)\n2023-07-11 04:34:22.712 [ INFO] Progress -   3.2 MB (24.9%)\n2023-07-11 04:34:22.885 [ INFO] Progress -   3.2 MB (27.0%)\n2023-07-11 04:34:22.886 [ INFO] Progress -   3.2 MB (29.1%)\n2023-07-11 04:34:22.886 [ INFO] Progress -   3.2 MB (31.2%)\n2023-07-11 04:34:22.886 [ INFO] Progress -   3.2 MB (33.2%)\n2023-07-11 04:34:22.887 [ INFO] Progress -   3.2 MB (35.3%)\n2023-07-11 04:34:22.888 [ INFO] Progress -   3.2 MB (37.4%)\n2023-07-11 04:34:22.888 [ INFO] Progress -   3.2 MB (39.5%)\n2023-07-11 04:34:22.888 [ INFO] Progress -   3.2 MB (41.5%)\n2023-07-11 04:34:22.888 [ INFO] Progress -   3.2 MB (43.6%)\n2023-07-11 04:34:22.889 [ INFO] Progress -   3.2 MB (45.7%)\n2023-07-11 04:34:22.889 [ INFO] Progress -   3.2 MB (47.8%)\n2023-07-11 04:34:22.889 [ INFO] Progress -   3.2 MB (49.8%)\n2023-07-11 04:34:23.060 [ INFO] Progress -   3.2 MB (51.9%)\n2023-07-11 04:34:23.061 [ INFO] Progress -   3.2 MB (54.0%)\n2023-07-11 04:34:23.061 [ INFO] Progress -   3.2 MB (56.1%)\n2023-07-11 04:34:23.062 [ INFO] Progress -   3.2 MB (58.1%)\n2023-07-11 04:34:23.062 [ INFO] Progress -   3.2 MB (60.2%)\n2023-07-11 04:34:23.063 [ INFO] Progress -   3.2 MB (62.3%)\n2023-07-11 04:34:23.063 [ INFO] Progress -   3.2 MB (64.4%)\n2023-07-11 04:34:23.063 [ INFO] Progress -   3.2 MB (66.5%)\n2023-07-11 04:34:23.064 [ INFO] Progress -   3.2 MB (68.5%)\n2023-07-11 04:34:23.064 [ INFO] Progress -   3.2 MB (70.6%)\n2023-07-11 04:34:23.064 [ INFO] Progress -   3.2 MB (72.7%)\n2023-07-11 04:34:23.065 [ INFO] Progress -   3.2 MB (74.8%)\n2023-07-11 04:34:23.065 [ INFO] Progress -   3.2 MB (76.8%)\n2023-07-11 04:34:23.065 [ INFO] Progress -   3.2 MB (78.9%)\n2023-07-11 04:34:23.065 [ INFO] Progress -   3.2 MB (81.0%)\n2023-07-11 04:34:23.066 [ INFO] Progress -   3.2 MB (83.1%)\n2023-07-11 04:34:23.066 [ INFO] Progress -   3.2 MB (85.1%)\n2023-07-11 04:34:23.066 [ INFO] Progress -   3.2 MB (87.2%)\n2023-07-11 04:34:23.066 [ INFO] Progress -   3.2 MB (89.3%)\n2023-07-11 04:34:23.067 [ INFO] Progress -   3.2 MB (91.4%)\n2023-07-11 04:34:23.067 [ INFO] Progress -   3.2 MB (93.5%)\n2023-07-11 04:34:23.067 [ INFO] Progress -   3.2 MB (95.5%)\n2023-07-11 04:34:23.067 [ INFO] Progress -   3.2 MB (97.6%)\n2023-07-11 04:34:23.235 [ INFO] Progress -   3.2 MB (99.7%)\n2023-07-11 04:34:23.236 [ INFO] Progress -   3.2 MB (100.0%)\n2023-07-11 04:34:23.236 [ INFO] Processing  time : 0:00:34.434477\n2023-07-11 04:34:23.236 [ INFO] Downloading time : 0:00:01.401031\n2023-07-11 04:34:23.236 [ INFO] Total time       : 0:00:35.835508\n2023-07-11 04:34:23.236 [ INFO] Download rate    : 1.5 MB/s\n2023-07-11 04:34:23.236 [ INFO] Save into        : motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200004.nc\n2023-07-11 04:34:23.266 [ INFO] Done\n2023-07-11 04:34:23.512 [ INFO] Asynchronous mode set\n2023-07-11 04:34:23.513 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:34:30.732 [ INFO] Requesting file to download (this can take a while)...\n2023-07-11 04:34:39.973 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:34:55.062 [ INFO] The product is ready for download\n2023-07-11 04:34:55.062 [ INFO] Downloading file (this can take a while)...\n2023-07-11 04:34:55.808 [ INFO] File type: application/x-netcdf\n2023-07-11 04:34:55.808 [ INFO] File size: 3.3 MB (3260684 B)\n2023-07-11 04:34:55.808 [ INFO] Downloading file /home/jovyan/shared/minh-notebooks/motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200005.nc\n2023-07-11 04:34:56.158 [ INFO] Progress -   3.3 MB (2.0%)\n2023-07-11 04:34:56.333 [ INFO] Progress -   3.3 MB (4.0%)\n2023-07-11 04:34:56.508 [ INFO] Progress -   3.3 MB (6.0%)\n2023-07-11 04:34:56.509 [ INFO] Progress -   3.3 MB (8.0%)\n2023-07-11 04:34:56.683 [ INFO] Progress -   3.3 MB (10.0%)\n2023-07-11 04:34:56.684 [ INFO] Progress -   3.3 MB (12.1%)\n2023-07-11 04:34:56.684 [ INFO] Progress -   3.3 MB (14.1%)\n2023-07-11 04:34:56.684 [ INFO] Progress -   3.3 MB (16.1%)\n2023-07-11 04:34:56.685 [ INFO] Progress -   3.3 MB (18.1%)\n2023-07-11 04:34:56.859 [ INFO] Progress -   3.3 MB (20.1%)\n2023-07-11 04:34:56.859 [ INFO] Progress -   3.3 MB (22.1%)\n2023-07-11 04:34:56.860 [ INFO] Progress -   3.3 MB (24.1%)\n2023-07-11 04:34:56.860 [ INFO] Progress -   3.3 MB (26.1%)\n2023-07-11 04:34:56.860 [ INFO] Progress -   3.3 MB (28.1%)\n2023-07-11 04:34:56.860 [ INFO] Progress -   3.3 MB (30.1%)\n2023-07-11 04:34:56.861 [ INFO] Progress -   3.3 MB (32.2%)\n2023-07-11 04:34:56.862 [ INFO] Progress -   3.3 MB (34.2%)\n2023-07-11 04:34:56.862 [ INFO] Progress -   3.3 MB (36.2%)\n2023-07-11 04:34:56.862 [ INFO] Progress -   3.3 MB (38.2%)\n2023-07-11 04:34:57.034 [ INFO] Progress -   3.3 MB (40.2%)\n2023-07-11 04:34:57.034 [ INFO] Progress -   3.3 MB (42.2%)\n2023-07-11 04:34:57.035 [ INFO] Progress -   3.3 MB (44.2%)\n2023-07-11 04:34:57.035 [ INFO] Progress -   3.3 MB (46.2%)\n2023-07-11 04:34:57.035 [ INFO] Progress -   3.3 MB (48.2%)\n2023-07-11 04:34:57.035 [ INFO] Progress -   3.3 MB (50.2%)\n2023-07-11 04:34:57.036 [ INFO] Progress -   3.3 MB (52.3%)\n2023-07-11 04:34:57.036 [ INFO] Progress -   3.3 MB (54.3%)\n2023-07-11 04:34:57.036 [ INFO] Progress -   3.3 MB (56.3%)\n2023-07-11 04:34:57.036 [ INFO] Progress -   3.3 MB (58.3%)\n2023-07-11 04:34:57.037 [ INFO] Progress -   3.3 MB (60.3%)\n2023-07-11 04:34:57.037 [ INFO] Progress -   3.3 MB (62.3%)\n2023-07-11 04:34:57.037 [ INFO] Progress -   3.3 MB (64.3%)\n2023-07-11 04:34:57.038 [ INFO] Progress -   3.3 MB (66.3%)\n2023-07-11 04:34:57.038 [ INFO] Progress -   3.3 MB (68.3%)\n2023-07-11 04:34:57.039 [ INFO] Progress -   3.3 MB (70.3%)\n2023-07-11 04:34:57.039 [ INFO] Progress -   3.3 MB (72.4%)\n2023-07-11 04:34:57.039 [ INFO] Progress -   3.3 MB (74.4%)\n2023-07-11 04:34:57.040 [ INFO] Progress -   3.3 MB (76.4%)\n2023-07-11 04:34:57.040 [ INFO] Progress -   3.3 MB (78.4%)\n2023-07-11 04:34:57.209 [ INFO] Progress -   3.3 MB (80.4%)\n2023-07-11 04:34:57.209 [ INFO] Progress -   3.3 MB (82.4%)\n2023-07-11 04:34:57.210 [ INFO] Progress -   3.3 MB (84.4%)\n2023-07-11 04:34:57.210 [ INFO] Progress -   3.3 MB (86.4%)\n2023-07-11 04:34:57.210 [ INFO] Progress -   3.3 MB (88.4%)\n2023-07-11 04:34:57.210 [ INFO] Progress -   3.3 MB (90.4%)\n2023-07-11 04:34:57.211 [ INFO] Progress -   3.3 MB (92.5%)\n2023-07-11 04:34:57.211 [ INFO] Progress -   3.3 MB (94.5%)\n2023-07-11 04:34:57.211 [ INFO] Progress -   3.3 MB (96.5%)\n2023-07-11 04:34:57.212 [ INFO] Progress -   3.3 MB (98.5%)\n2023-07-11 04:34:57.212 [ INFO] Progress -   3.3 MB (100.0%)\n2023-07-11 04:34:57.213 [ INFO] Processing  time : 0:00:32.298162\n2023-07-11 04:34:57.213 [ INFO] Downloading time : 0:00:01.404053\n2023-07-11 04:34:57.213 [ INFO] Total time       : 0:00:33.702215\n2023-07-11 04:34:57.213 [ INFO] Download rate    : 1.5 MB/s\n2023-07-11 04:34:57.213 [ INFO] Save into        : motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200005.nc\n2023-07-11 04:34:57.242 [ INFO] Done\n2023-07-11 04:34:57.482 [ INFO] Asynchronous mode set\n2023-07-11 04:34:57.482 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:35:05.474 [ INFO] Requesting file to download (this can take a while)...\n2023-07-11 04:35:13.265 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:35:29.833 [ INFO] The product is ready for download\n2023-07-11 04:35:29.833 [ INFO] Downloading file (this can take a while)...\n2023-07-11 04:35:30.583 [ INFO] File type: application/x-netcdf\n2023-07-11 04:35:30.583 [ INFO] File size: 3.2 MB (3155704 B)\n2023-07-11 04:35:30.583 [ INFO] Downloading file /home/jovyan/shared/minh-notebooks/motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200006.nc\n2023-07-11 04:35:30.932 [ INFO] Progress -   3.2 MB (2.1%)\n2023-07-11 04:35:31.107 [ INFO] Progress -   3.2 MB (4.2%)\n2023-07-11 04:35:31.282 [ INFO] Progress -   3.2 MB (6.2%)\n2023-07-11 04:35:31.283 [ INFO] Progress -   3.2 MB (8.3%)\n2023-07-11 04:35:31.283 [ INFO] Progress -   3.2 MB (10.4%)\n2023-07-11 04:35:31.458 [ INFO] Progress -   3.2 MB (12.5%)\n2023-07-11 04:35:31.458 [ INFO] Progress -   3.2 MB (14.5%)\n2023-07-11 04:35:31.458 [ INFO] Progress -   3.2 MB (16.6%)\n2023-07-11 04:35:31.459 [ INFO] Progress -   3.2 MB (18.7%)\n2023-07-11 04:35:31.459 [ INFO] Progress -   3.2 MB (20.8%)\n2023-07-11 04:35:31.633 [ INFO] Progress -   3.2 MB (22.8%)\n2023-07-11 04:35:31.633 [ INFO] Progress -   3.2 MB (24.9%)\n2023-07-11 04:35:31.634 [ INFO] Progress -   3.2 MB (27.0%)\n2023-07-11 04:35:31.634 [ INFO] Progress -   3.2 MB (29.1%)\n2023-07-11 04:35:31.634 [ INFO] Progress -   3.2 MB (31.2%)\n2023-07-11 04:35:31.634 [ INFO] Progress -   3.2 MB (33.2%)\n2023-07-11 04:35:31.635 [ INFO] Progress -   3.2 MB (35.3%)\n2023-07-11 04:35:31.636 [ INFO] Progress -   3.2 MB (37.4%)\n2023-07-11 04:35:31.636 [ INFO] Progress -   3.2 MB (39.5%)\n2023-07-11 04:35:31.636 [ INFO] Progress -   3.2 MB (41.5%)\n2023-07-11 04:35:31.636 [ INFO] Progress -   3.2 MB (43.6%)\n2023-07-11 04:35:31.808 [ INFO] Progress -   3.2 MB (45.7%)\n2023-07-11 04:35:31.808 [ INFO] Progress -   3.2 MB (47.8%)\n2023-07-11 04:35:31.808 [ INFO] Progress -   3.2 MB (49.8%)\n2023-07-11 04:35:31.809 [ INFO] Progress -   3.2 MB (51.9%)\n2023-07-11 04:35:31.809 [ INFO] Progress -   3.2 MB (54.0%)\n2023-07-11 04:35:31.809 [ INFO] Progress -   3.2 MB (56.1%)\n2023-07-11 04:35:31.810 [ INFO] Progress -   3.2 MB (58.1%)\n2023-07-11 04:35:31.810 [ INFO] Progress -   3.2 MB (60.2%)\n2023-07-11 04:35:31.810 [ INFO] Progress -   3.2 MB (62.3%)\n2023-07-11 04:35:31.810 [ INFO] Progress -   3.2 MB (64.4%)\n2023-07-11 04:35:31.811 [ INFO] Progress -   3.2 MB (66.5%)\n2023-07-11 04:35:31.811 [ INFO] Progress -   3.2 MB (68.5%)\n2023-07-11 04:35:31.812 [ INFO] Progress -   3.2 MB (70.6%)\n2023-07-11 04:35:31.812 [ INFO] Progress -   3.2 MB (72.7%)\n2023-07-11 04:35:31.812 [ INFO] Progress -   3.2 MB (74.8%)\n2023-07-11 04:35:31.812 [ INFO] Progress -   3.2 MB (76.8%)\n2023-07-11 04:35:31.813 [ INFO] Progress -   3.2 MB (78.9%)\n2023-07-11 04:35:31.813 [ INFO] Progress -   3.2 MB (81.0%)\n2023-07-11 04:35:31.813 [ INFO] Progress -   3.2 MB (83.1%)\n2023-07-11 04:35:31.813 [ INFO] Progress -   3.2 MB (85.1%)\n2023-07-11 04:35:31.814 [ INFO] Progress -   3.2 MB (87.2%)\n2023-07-11 04:35:31.983 [ INFO] Progress -   3.2 MB (89.3%)\n2023-07-11 04:35:31.983 [ INFO] Progress -   3.2 MB (91.4%)\n2023-07-11 04:35:31.983 [ INFO] Progress -   3.2 MB (93.5%)\n2023-07-11 04:35:31.984 [ INFO] Progress -   3.2 MB (95.5%)\n2023-07-11 04:35:31.984 [ INFO] Progress -   3.2 MB (97.6%)\n2023-07-11 04:35:31.984 [ INFO] Progress -   3.2 MB (99.7%)\n2023-07-11 04:35:31.985 [ INFO] Progress -   3.2 MB (100.0%)\n2023-07-11 04:35:31.985 [ INFO] Processing  time : 0:00:33.103400\n2023-07-11 04:35:31.985 [ INFO] Downloading time : 0:00:01.401763\n2023-07-11 04:35:31.985 [ INFO] Total time       : 0:00:34.505163\n2023-07-11 04:35:31.985 [ INFO] Download rate    : 1.5 MB/s\n2023-07-11 04:35:31.985 [ INFO] Save into        : motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200006.nc\n2023-07-11 04:35:32.015 [ INFO] Done\n2023-07-11 04:35:32.256 [ INFO] Asynchronous mode set\n2023-07-11 04:35:32.257 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:35:40.867 [ INFO] Requesting file to download (this can take a while)...\n2023-07-11 04:35:49.156 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:36:05.880 [ INFO] The product is ready for download\n2023-07-11 04:36:05.880 [ INFO] Downloading file (this can take a while)...\n2023-07-11 04:36:06.624 [ INFO] File type: application/x-netcdf\n2023-07-11 04:36:06.625 [ INFO] File size: 3.3 MB (3260684 B)\n2023-07-11 04:36:06.625 [ INFO] Downloading file /home/jovyan/shared/minh-notebooks/motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200007.nc\n2023-07-11 04:36:06.974 [ INFO] Progress -   3.3 MB (2.0%)\n2023-07-11 04:36:07.149 [ INFO] Progress -   3.3 MB (4.0%)\n2023-07-11 04:36:07.324 [ INFO] Progress -   3.3 MB (6.0%)\n2023-07-11 04:36:07.324 [ INFO] Progress -   3.3 MB (8.0%)\n2023-07-11 04:36:07.325 [ INFO] Progress -   3.3 MB (10.0%)\n2023-07-11 04:36:07.499 [ INFO] Progress -   3.3 MB (12.1%)\n2023-07-11 04:36:07.500 [ INFO] Progress -   3.3 MB (14.1%)\n2023-07-11 04:36:07.500 [ INFO] Progress -   3.3 MB (16.1%)\n2023-07-11 04:36:07.500 [ INFO] Progress -   3.3 MB (18.1%)\n2023-07-11 04:36:07.500 [ INFO] Progress -   3.3 MB (20.1%)\n2023-07-11 04:36:07.501 [ INFO] Progress -   3.3 MB (22.1%)\n2023-07-11 04:36:07.501 [ INFO] Progress -   3.3 MB (24.1%)\n2023-07-11 04:36:07.674 [ INFO] Progress -   3.3 MB (26.1%)\n2023-07-11 04:36:07.675 [ INFO] Progress -   3.3 MB (28.1%)\n2023-07-11 04:36:07.675 [ INFO] Progress -   3.3 MB (30.1%)\n2023-07-11 04:36:07.675 [ INFO] Progress -   3.3 MB (32.2%)\n2023-07-11 04:36:07.676 [ INFO] Progress -   3.3 MB (34.2%)\n2023-07-11 04:36:07.677 [ INFO] Progress -   3.3 MB (36.2%)\n2023-07-11 04:36:07.677 [ INFO] Progress -   3.3 MB (38.2%)\n2023-07-11 04:36:07.677 [ INFO] Progress -   3.3 MB (40.2%)\n2023-07-11 04:36:07.677 [ INFO] Progress -   3.3 MB (42.2%)\n2023-07-11 04:36:07.678 [ INFO] Progress -   3.3 MB (44.2%)\n2023-07-11 04:36:07.678 [ INFO] Progress -   3.3 MB (46.2%)\n2023-07-11 04:36:07.678 [ INFO] Progress -   3.3 MB (48.2%)\n2023-07-11 04:36:07.849 [ INFO] Progress -   3.3 MB (50.2%)\n2023-07-11 04:36:07.850 [ INFO] Progress -   3.3 MB (52.3%)\n2023-07-11 04:36:07.850 [ INFO] Progress -   3.3 MB (54.3%)\n2023-07-11 04:36:07.850 [ INFO] Progress -   3.3 MB (56.3%)\n2023-07-11 04:36:07.851 [ INFO] Progress -   3.3 MB (58.3%)\n2023-07-11 04:36:07.851 [ INFO] Progress -   3.3 MB (60.3%)\n2023-07-11 04:36:07.851 [ INFO] Progress -   3.3 MB (62.3%)\n2023-07-11 04:36:07.852 [ INFO] Progress -   3.3 MB (64.3%)\n2023-07-11 04:36:07.853 [ INFO] Progress -   3.3 MB (66.3%)\n2023-07-11 04:36:07.853 [ INFO] Progress -   3.3 MB (68.3%)\n2023-07-11 04:36:07.853 [ INFO] Progress -   3.3 MB (70.3%)\n2023-07-11 04:36:07.853 [ INFO] Progress -   3.3 MB (72.4%)\n2023-07-11 04:36:07.854 [ INFO] Progress -   3.3 MB (74.4%)\n2023-07-11 04:36:07.854 [ INFO] Progress -   3.3 MB (76.4%)\n2023-07-11 04:36:07.854 [ INFO] Progress -   3.3 MB (78.4%)\n2023-07-11 04:36:07.854 [ INFO] Progress -   3.3 MB (80.4%)\n2023-07-11 04:36:07.854 [ INFO] Progress -   3.3 MB (82.4%)\n2023-07-11 04:36:07.855 [ INFO] Progress -   3.3 MB (84.4%)\n2023-07-11 04:36:07.855 [ INFO] Progress -   3.3 MB (86.4%)\n2023-07-11 04:36:07.855 [ INFO] Progress -   3.3 MB (88.4%)\n2023-07-11 04:36:07.855 [ INFO] Progress -   3.3 MB (90.4%)\n2023-07-11 04:36:07.856 [ INFO] Progress -   3.3 MB (92.5%)\n2023-07-11 04:36:07.856 [ INFO] Progress -   3.3 MB (94.5%)\n2023-07-11 04:36:07.856 [ INFO] Progress -   3.3 MB (96.5%)\n2023-07-11 04:36:07.857 [ INFO] Progress -   3.3 MB (98.5%)\n2023-07-11 04:36:08.024 [ INFO] Progress -   3.3 MB (100.0%)\n2023-07-11 04:36:08.025 [ INFO] Processing  time : 0:00:34.370452\n2023-07-11 04:36:08.025 [ INFO] Downloading time : 0:00:01.399979\n2023-07-11 04:36:08.025 [ INFO] Total time       : 0:00:35.770431\n2023-07-11 04:36:08.025 [ INFO] Download rate    : 1.5 MB/s\n2023-07-11 04:36:08.025 [ INFO] Save into        : motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200007.nc\n2023-07-11 04:36:08.055 [ INFO] Done\n2023-07-11 04:36:08.293 [ INFO] Asynchronous mode set\n2023-07-11 04:36:08.294 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:36:16.318 [ INFO] Requesting file to download (this can take a while)...\n2023-07-11 04:36:24.794 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:36:40.691 [ INFO] The product is ready for download\n2023-07-11 04:36:40.691 [ INFO] Downloading file (this can take a while)...\n2023-07-11 04:36:41.620 [ INFO] File type: application/x-netcdf\n2023-07-11 04:36:41.620 [ INFO] File size: 3.3 MB (3260684 B)\n2023-07-11 04:36:41.620 [ INFO] Downloading file /home/jovyan/shared/minh-notebooks/motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200008.nc\n2023-07-11 04:36:41.794 [ INFO] Progress -   3.3 MB (2.0%)\n2023-07-11 04:36:41.969 [ INFO] Progress -   3.3 MB (4.0%)\n2023-07-11 04:36:42.144 [ INFO] Progress -   3.3 MB (6.0%)\n2023-07-11 04:36:42.145 [ INFO] Progress -   3.3 MB (8.0%)\n2023-07-11 04:36:42.145 [ INFO] Progress -   3.3 MB (10.0%)\n2023-07-11 04:36:42.319 [ INFO] Progress -   3.3 MB (12.1%)\n2023-07-11 04:36:42.320 [ INFO] Progress -   3.3 MB (14.1%)\n2023-07-11 04:36:42.320 [ INFO] Progress -   3.3 MB (16.1%)\n2023-07-11 04:36:42.320 [ INFO] Progress -   3.3 MB (18.1%)\n2023-07-11 04:36:42.321 [ INFO] Progress -   3.3 MB (20.1%)\n2023-07-11 04:36:42.321 [ INFO] Progress -   3.3 MB (22.1%)\n2023-07-11 04:36:42.321 [ INFO] Progress -   3.3 MB (24.1%)\n2023-07-11 04:36:42.494 [ INFO] Progress -   3.3 MB (26.1%)\n2023-07-11 04:36:42.495 [ INFO] Progress -   3.3 MB (28.1%)\n2023-07-11 04:36:42.495 [ INFO] Progress -   3.3 MB (30.1%)\n2023-07-11 04:36:42.495 [ INFO] Progress -   3.3 MB (32.2%)\n2023-07-11 04:36:42.496 [ INFO] Progress -   3.3 MB (34.2%)\n2023-07-11 04:36:42.497 [ INFO] Progress -   3.3 MB (36.2%)\n2023-07-11 04:36:42.497 [ INFO] Progress -   3.3 MB (38.2%)\n2023-07-11 04:36:42.497 [ INFO] Progress -   3.3 MB (40.2%)\n2023-07-11 04:36:42.497 [ INFO] Progress -   3.3 MB (42.2%)\n2023-07-11 04:36:42.498 [ INFO] Progress -   3.3 MB (44.2%)\n2023-07-11 04:36:42.498 [ INFO] Progress -   3.3 MB (46.2%)\n2023-07-11 04:36:42.498 [ INFO] Progress -   3.3 MB (48.2%)\n2023-07-11 04:36:42.669 [ INFO] Progress -   3.3 MB (50.2%)\n2023-07-11 04:36:42.670 [ INFO] Progress -   3.3 MB (52.3%)\n2023-07-11 04:36:42.670 [ INFO] Progress -   3.3 MB (54.3%)\n2023-07-11 04:36:42.670 [ INFO] Progress -   3.3 MB (56.3%)\n2023-07-11 04:36:42.671 [ INFO] Progress -   3.3 MB (58.3%)\n2023-07-11 04:36:42.671 [ INFO] Progress -   3.3 MB (60.3%)\n2023-07-11 04:36:42.671 [ INFO] Progress -   3.3 MB (62.3%)\n2023-07-11 04:36:42.672 [ INFO] Progress -   3.3 MB (64.3%)\n2023-07-11 04:36:42.672 [ INFO] Progress -   3.3 MB (66.3%)\n2023-07-11 04:36:42.673 [ INFO] Progress -   3.3 MB (68.3%)\n2023-07-11 04:36:42.673 [ INFO] Progress -   3.3 MB (70.3%)\n2023-07-11 04:36:42.673 [ INFO] Progress -   3.3 MB (72.4%)\n2023-07-11 04:36:42.673 [ INFO] Progress -   3.3 MB (74.4%)\n2023-07-11 04:36:42.674 [ INFO] Progress -   3.3 MB (76.4%)\n2023-07-11 04:36:42.674 [ INFO] Progress -   3.3 MB (78.4%)\n2023-07-11 04:36:42.674 [ INFO] Progress -   3.3 MB (80.4%)\n2023-07-11 04:36:42.674 [ INFO] Progress -   3.3 MB (82.4%)\n2023-07-11 04:36:42.675 [ INFO] Progress -   3.3 MB (84.4%)\n2023-07-11 04:36:42.675 [ INFO] Progress -   3.3 MB (86.4%)\n2023-07-11 04:36:42.675 [ INFO] Progress -   3.3 MB (88.4%)\n2023-07-11 04:36:42.675 [ INFO] Progress -   3.3 MB (90.4%)\n2023-07-11 04:36:42.676 [ INFO] Progress -   3.3 MB (92.5%)\n2023-07-11 04:36:42.676 [ INFO] Progress -   3.3 MB (94.5%)\n2023-07-11 04:36:42.676 [ INFO] Progress -   3.3 MB (96.5%)\n2023-07-11 04:36:42.677 [ INFO] Progress -   3.3 MB (98.5%)\n2023-07-11 04:36:42.845 [ INFO] Progress -   3.3 MB (100.0%)\n2023-07-11 04:36:42.845 [ INFO] Processing  time : 0:00:33.329086\n2023-07-11 04:36:42.845 [ INFO] Downloading time : 0:00:01.224631\n2023-07-11 04:36:42.845 [ INFO] Total time       : 0:00:34.553717\n2023-07-11 04:36:42.845 [ INFO] Download rate    : 1.5 MB/s\n2023-07-11 04:36:42.845 [ INFO] Save into        : motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200008.nc\n2023-07-11 04:36:42.876 [ INFO] Done\n2023-07-11 04:36:43.117 [ INFO] Asynchronous mode set\n2023-07-11 04:36:43.118 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:36:50.965 [ INFO] Requesting file to download (this can take a while)...\n2023-07-11 04:36:59.415 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:37:13.369 [ INFO] The product is ready for download\n2023-07-11 04:37:13.369 [ INFO] Downloading file (this can take a while)...\n2023-07-11 04:37:14.112 [ INFO] File type: application/x-netcdf\n2023-07-11 04:37:14.112 [ INFO] File size: 3.2 MB (3155704 B)\n2023-07-11 04:37:14.113 [ INFO] Downloading file /home/jovyan/shared/minh-notebooks/motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200009.nc\n2023-07-11 04:37:14.462 [ INFO] Progress -   3.2 MB (2.1%)\n2023-07-11 04:37:14.637 [ INFO] Progress -   3.2 MB (4.2%)\n2023-07-11 04:37:14.812 [ INFO] Progress -   3.2 MB (6.2%)\n2023-07-11 04:37:14.812 [ INFO] Progress -   3.2 MB (8.3%)\n2023-07-11 04:37:14.813 [ INFO] Progress -   3.2 MB (10.4%)\n2023-07-11 04:37:14.987 [ INFO] Progress -   3.2 MB (12.5%)\n2023-07-11 04:37:14.987 [ INFO] Progress -   3.2 MB (14.5%)\n2023-07-11 04:37:14.988 [ INFO] Progress -   3.2 MB (16.6%)\n2023-07-11 04:37:14.988 [ INFO] Progress -   3.2 MB (18.7%)\n2023-07-11 04:37:14.988 [ INFO] Progress -   3.2 MB (20.8%)\n2023-07-11 04:37:14.988 [ INFO] Progress -   3.2 MB (22.8%)\n2023-07-11 04:37:15.162 [ INFO] Progress -   3.2 MB (24.9%)\n2023-07-11 04:37:15.163 [ INFO] Progress -   3.2 MB (27.0%)\n2023-07-11 04:37:15.163 [ INFO] Progress -   3.2 MB (29.1%)\n2023-07-11 04:37:15.163 [ INFO] Progress -   3.2 MB (31.2%)\n2023-07-11 04:37:15.164 [ INFO] Progress -   3.2 MB (33.2%)\n2023-07-11 04:37:15.164 [ INFO] Progress -   3.2 MB (35.3%)\n2023-07-11 04:37:15.165 [ INFO] Progress -   3.2 MB (37.4%)\n2023-07-11 04:37:15.165 [ INFO] Progress -   3.2 MB (39.5%)\n2023-07-11 04:37:15.165 [ INFO] Progress -   3.2 MB (41.5%)\n2023-07-11 04:37:15.165 [ INFO] Progress -   3.2 MB (43.6%)\n2023-07-11 04:37:15.166 [ INFO] Progress -   3.2 MB (45.7%)\n2023-07-11 04:37:15.337 [ INFO] Progress -   3.2 MB (47.8%)\n2023-07-11 04:37:15.337 [ INFO] Progress -   3.2 MB (49.8%)\n2023-07-11 04:37:15.338 [ INFO] Progress -   3.2 MB (51.9%)\n2023-07-11 04:37:15.338 [ INFO] Progress -   3.2 MB (54.0%)\n2023-07-11 04:37:15.338 [ INFO] Progress -   3.2 MB (56.1%)\n2023-07-11 04:37:15.338 [ INFO] Progress -   3.2 MB (58.1%)\n2023-07-11 04:37:15.339 [ INFO] Progress -   3.2 MB (60.2%)\n2023-07-11 04:37:15.339 [ INFO] Progress -   3.2 MB (62.3%)\n2023-07-11 04:37:15.339 [ INFO] Progress -   3.2 MB (64.4%)\n2023-07-11 04:37:15.339 [ INFO] Progress -   3.2 MB (66.5%)\n2023-07-11 04:37:15.340 [ INFO] Progress -   3.2 MB (68.5%)\n2023-07-11 04:37:15.340 [ INFO] Progress -   3.2 MB (70.6%)\n2023-07-11 04:37:15.341 [ INFO] Progress -   3.2 MB (72.7%)\n2023-07-11 04:37:15.341 [ INFO] Progress -   3.2 MB (74.8%)\n2023-07-11 04:37:15.341 [ INFO] Progress -   3.2 MB (76.8%)\n2023-07-11 04:37:15.341 [ INFO] Progress -   3.2 MB (78.9%)\n2023-07-11 04:37:15.342 [ INFO] Progress -   3.2 MB (81.0%)\n2023-07-11 04:37:15.342 [ INFO] Progress -   3.2 MB (83.1%)\n2023-07-11 04:37:15.342 [ INFO] Progress -   3.2 MB (85.1%)\n2023-07-11 04:37:15.342 [ INFO] Progress -   3.2 MB (87.2%)\n2023-07-11 04:37:15.342 [ INFO] Progress -   3.2 MB (89.3%)\n2023-07-11 04:37:15.343 [ INFO] Progress -   3.2 MB (91.4%)\n2023-07-11 04:37:15.343 [ INFO] Progress -   3.2 MB (93.5%)\n2023-07-11 04:37:15.512 [ INFO] Progress -   3.2 MB (95.5%)\n2023-07-11 04:37:15.512 [ INFO] Progress -   3.2 MB (97.6%)\n2023-07-11 04:37:15.513 [ INFO] Progress -   3.2 MB (99.7%)\n2023-07-11 04:37:15.513 [ INFO] Progress -   3.2 MB (100.0%)\n2023-07-11 04:37:15.513 [ INFO] Processing  time : 0:00:30.997472\n2023-07-11 04:37:15.513 [ INFO] Downloading time : 0:00:01.400914\n2023-07-11 04:37:15.513 [ INFO] Total time       : 0:00:32.398386\n2023-07-11 04:37:15.513 [ INFO] Download rate    : 1.5 MB/s\n2023-07-11 04:37:15.514 [ INFO] Save into        : motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200009.nc\n2023-07-11 04:37:15.543 [ INFO] Done\n2023-07-11 04:37:15.783 [ INFO] Asynchronous mode set\n2023-07-11 04:37:15.783 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:37:20.984 [ INFO] Requesting file to download (this can take a while)...\n2023-07-11 04:37:30.393 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:37:47.071 [ INFO] The product is ready for download\n2023-07-11 04:37:47.071 [ INFO] Downloading file (this can take a while)...\n2023-07-11 04:37:47.821 [ INFO] File type: application/x-netcdf\n2023-07-11 04:37:47.821 [ INFO] File size: 3.3 MB (3260684 B)\n2023-07-11 04:37:47.821 [ INFO] Downloading file /home/jovyan/shared/minh-notebooks/motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200010.nc\n2023-07-11 04:37:48.170 [ INFO] Progress -   3.3 MB (2.0%)\n2023-07-11 04:37:48.346 [ INFO] Progress -   3.3 MB (4.0%)\n2023-07-11 04:37:48.520 [ INFO] Progress -   3.3 MB (6.0%)\n2023-07-11 04:37:48.521 [ INFO] Progress -   3.3 MB (8.0%)\n2023-07-11 04:37:48.521 [ INFO] Progress -   3.3 MB (10.0%)\n2023-07-11 04:37:48.695 [ INFO] Progress -   3.3 MB (12.1%)\n2023-07-11 04:37:48.696 [ INFO] Progress -   3.3 MB (14.1%)\n2023-07-11 04:37:48.696 [ INFO] Progress -   3.3 MB (16.1%)\n2023-07-11 04:37:48.696 [ INFO] Progress -   3.3 MB (18.1%)\n2023-07-11 04:37:48.697 [ INFO] Progress -   3.3 MB (20.1%)\n2023-07-11 04:37:48.697 [ INFO] Progress -   3.3 MB (22.1%)\n2023-07-11 04:37:48.697 [ INFO] Progress -   3.3 MB (24.1%)\n2023-07-11 04:37:48.871 [ INFO] Progress -   3.3 MB (26.1%)\n2023-07-11 04:37:48.871 [ INFO] Progress -   3.3 MB (28.1%)\n2023-07-11 04:37:48.872 [ INFO] Progress -   3.3 MB (30.1%)\n2023-07-11 04:37:48.872 [ INFO] Progress -   3.3 MB (32.2%)\n2023-07-11 04:37:48.873 [ INFO] Progress -   3.3 MB (34.2%)\n2023-07-11 04:37:48.873 [ INFO] Progress -   3.3 MB (36.2%)\n2023-07-11 04:37:48.873 [ INFO] Progress -   3.3 MB (38.2%)\n2023-07-11 04:37:48.874 [ INFO] Progress -   3.3 MB (40.2%)\n2023-07-11 04:37:48.874 [ INFO] Progress -   3.3 MB (42.2%)\n2023-07-11 04:37:48.874 [ INFO] Progress -   3.3 MB (44.2%)\n2023-07-11 04:37:48.874 [ INFO] Progress -   3.3 MB (46.2%)\n2023-07-11 04:37:48.874 [ INFO] Progress -   3.3 MB (48.2%)\n2023-07-11 04:37:49.046 [ INFO] Progress -   3.3 MB (50.2%)\n2023-07-11 04:37:49.046 [ INFO] Progress -   3.3 MB (52.3%)\n2023-07-11 04:37:49.046 [ INFO] Progress -   3.3 MB (54.3%)\n2023-07-11 04:37:49.047 [ INFO] Progress -   3.3 MB (56.3%)\n2023-07-11 04:37:49.047 [ INFO] Progress -   3.3 MB (58.3%)\n2023-07-11 04:37:49.047 [ INFO] Progress -   3.3 MB (60.3%)\n2023-07-11 04:37:49.048 [ INFO] Progress -   3.3 MB (62.3%)\n2023-07-11 04:37:49.048 [ INFO] Progress -   3.3 MB (64.3%)\n2023-07-11 04:37:49.049 [ INFO] Progress -   3.3 MB (66.3%)\n2023-07-11 04:37:49.049 [ INFO] Progress -   3.3 MB (68.3%)\n2023-07-11 04:37:49.049 [ INFO] Progress -   3.3 MB (70.3%)\n2023-07-11 04:37:49.049 [ INFO] Progress -   3.3 MB (72.4%)\n2023-07-11 04:37:49.050 [ INFO] Progress -   3.3 MB (74.4%)\n2023-07-11 04:37:49.050 [ INFO] Progress -   3.3 MB (76.4%)\n2023-07-11 04:37:49.050 [ INFO] Progress -   3.3 MB (78.4%)\n2023-07-11 04:37:49.050 [ INFO] Progress -   3.3 MB (80.4%)\n2023-07-11 04:37:49.051 [ INFO] Progress -   3.3 MB (82.4%)\n2023-07-11 04:37:49.051 [ INFO] Progress -   3.3 MB (84.4%)\n2023-07-11 04:37:49.051 [ INFO] Progress -   3.3 MB (86.4%)\n2023-07-11 04:37:49.051 [ INFO] Progress -   3.3 MB (88.4%)\n2023-07-11 04:37:49.051 [ INFO] Progress -   3.3 MB (90.4%)\n2023-07-11 04:37:49.052 [ INFO] Progress -   3.3 MB (92.5%)\n2023-07-11 04:37:49.052 [ INFO] Progress -   3.3 MB (94.5%)\n2023-07-11 04:37:49.052 [ INFO] Progress -   3.3 MB (96.5%)\n2023-07-11 04:37:49.053 [ INFO] Progress -   3.3 MB (98.5%)\n2023-07-11 04:37:49.053 [ INFO] Progress -   3.3 MB (100.0%)\n2023-07-11 04:37:49.053 [ INFO] Processing  time : 0:00:32.040412\n2023-07-11 04:37:49.053 [ INFO] Downloading time : 0:00:01.232141\n2023-07-11 04:37:49.053 [ INFO] Total time       : 0:00:33.272553\n2023-07-11 04:37:49.053 [ INFO] Download rate    : 1.6 MB/s\n2023-07-11 04:37:49.054 [ INFO] Save into        : motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200010.nc\n2023-07-11 04:37:49.084 [ INFO] Done\n2023-07-11 04:37:49.325 [ INFO] Asynchronous mode set\n2023-07-11 04:37:49.325 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:37:59.411 [ INFO] Requesting file to download (this can take a while)...\n2023-07-11 04:38:06.403 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:38:22.049 [ INFO] The product is ready for download\n2023-07-11 04:38:22.049 [ INFO] Downloading file (this can take a while)...\n2023-07-11 04:38:22.795 [ INFO] File type: application/x-netcdf\n2023-07-11 04:38:22.795 [ INFO] File size: 3.2 MB (3155704 B)\n2023-07-11 04:38:22.795 [ INFO] Downloading file /home/jovyan/shared/minh-notebooks/motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200011.nc\n2023-07-11 04:38:23.144 [ INFO] Progress -   3.2 MB (2.1%)\n2023-07-11 04:38:23.319 [ INFO] Progress -   3.2 MB (4.2%)\n2023-07-11 04:38:23.494 [ INFO] Progress -   3.2 MB (6.2%)\n2023-07-11 04:38:23.495 [ INFO] Progress -   3.2 MB (8.3%)\n2023-07-11 04:38:23.495 [ INFO] Progress -   3.2 MB (10.4%)\n2023-07-11 04:38:23.669 [ INFO] Progress -   3.2 MB (12.5%)\n2023-07-11 04:38:23.670 [ INFO] Progress -   3.2 MB (14.5%)\n2023-07-11 04:38:23.670 [ INFO] Progress -   3.2 MB (16.6%)\n2023-07-11 04:38:23.670 [ INFO] Progress -   3.2 MB (18.7%)\n2023-07-11 04:38:23.671 [ INFO] Progress -   3.2 MB (20.8%)\n2023-07-11 04:38:23.671 [ INFO] Progress -   3.2 MB (22.8%)\n2023-07-11 04:38:23.844 [ INFO] Progress -   3.2 MB (24.9%)\n2023-07-11 04:38:23.845 [ INFO] Progress -   3.2 MB (27.0%)\n2023-07-11 04:38:23.845 [ INFO] Progress -   3.2 MB (29.1%)\n2023-07-11 04:38:23.845 [ INFO] Progress -   3.2 MB (31.2%)\n2023-07-11 04:38:23.846 [ INFO] Progress -   3.2 MB (33.2%)\n2023-07-11 04:38:23.847 [ INFO] Progress -   3.2 MB (35.3%)\n2023-07-11 04:38:23.847 [ INFO] Progress -   3.2 MB (37.4%)\n2023-07-11 04:38:23.847 [ INFO] Progress -   3.2 MB (39.5%)\n2023-07-11 04:38:23.847 [ INFO] Progress -   3.2 MB (41.5%)\n2023-07-11 04:38:23.848 [ INFO] Progress -   3.2 MB (43.6%)\n2023-07-11 04:38:23.848 [ INFO] Progress -   3.2 MB (45.7%)\n2023-07-11 04:38:23.848 [ INFO] Progress -   3.2 MB (47.8%)\n2023-07-11 04:38:24.019 [ INFO] Progress -   3.2 MB (49.8%)\n2023-07-11 04:38:24.020 [ INFO] Progress -   3.2 MB (51.9%)\n2023-07-11 04:38:24.020 [ INFO] Progress -   3.2 MB (54.0%)\n2023-07-11 04:38:24.020 [ INFO] Progress -   3.2 MB (56.1%)\n2023-07-11 04:38:24.021 [ INFO] Progress -   3.2 MB (58.1%)\n2023-07-11 04:38:24.021 [ INFO] Progress -   3.2 MB (60.2%)\n2023-07-11 04:38:24.021 [ INFO] Progress -   3.2 MB (62.3%)\n2023-07-11 04:38:24.021 [ INFO] Progress -   3.2 MB (64.4%)\n2023-07-11 04:38:24.022 [ INFO] Progress -   3.2 MB (66.5%)\n2023-07-11 04:38:24.022 [ INFO] Progress -   3.2 MB (68.5%)\n2023-07-11 04:38:24.023 [ INFO] Progress -   3.2 MB (70.6%)\n2023-07-11 04:38:24.023 [ INFO] Progress -   3.2 MB (72.7%)\n2023-07-11 04:38:24.023 [ INFO] Progress -   3.2 MB (74.8%)\n2023-07-11 04:38:24.023 [ INFO] Progress -   3.2 MB (76.8%)\n2023-07-11 04:38:24.023 [ INFO] Progress -   3.2 MB (78.9%)\n2023-07-11 04:38:24.024 [ INFO] Progress -   3.2 MB (81.0%)\n2023-07-11 04:38:24.024 [ INFO] Progress -   3.2 MB (83.1%)\n2023-07-11 04:38:24.024 [ INFO] Progress -   3.2 MB (85.1%)\n2023-07-11 04:38:24.024 [ INFO] Progress -   3.2 MB (87.2%)\n2023-07-11 04:38:24.025 [ INFO] Progress -   3.2 MB (89.3%)\n2023-07-11 04:38:24.025 [ INFO] Progress -   3.2 MB (91.4%)\n2023-07-11 04:38:24.025 [ INFO] Progress -   3.2 MB (93.5%)\n2023-07-11 04:38:24.025 [ INFO] Progress -   3.2 MB (95.5%)\n2023-07-11 04:38:24.025 [ INFO] Progress -   3.2 MB (97.6%)\n2023-07-11 04:38:24.194 [ INFO] Progress -   3.2 MB (99.7%)\n2023-07-11 04:38:24.195 [ INFO] Progress -   3.2 MB (100.0%)\n2023-07-11 04:38:24.195 [ INFO] Processing  time : 0:00:33.470549\n2023-07-11 04:38:24.195 [ INFO] Downloading time : 0:00:01.400394\n2023-07-11 04:38:24.195 [ INFO] Total time       : 0:00:34.870943\n2023-07-11 04:38:24.195 [ INFO] Download rate    : 1.5 MB/s\n2023-07-11 04:38:24.195 [ INFO] Save into        : motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200011.nc\n2023-07-11 04:38:24.225 [ INFO] Done\n2023-07-11 04:38:24.465 [ INFO] Asynchronous mode set\n2023-07-11 04:38:24.465 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:38:30.868 [ INFO] Requesting file to download (this can take a while)...\n2023-07-11 04:38:40.474 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:38:55.592 [ INFO] The product is ready for download\n2023-07-11 04:38:55.592 [ INFO] Downloading file (this can take a while)...\n2023-07-11 04:38:56.336 [ INFO] File type: application/x-netcdf\n2023-07-11 04:38:56.336 [ INFO] File size: 3.3 MB (3260684 B)\n2023-07-11 04:38:56.336 [ INFO] Downloading file /home/jovyan/shared/minh-notebooks/motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200012.nc\n2023-07-11 04:38:56.687 [ INFO] Progress -   3.3 MB (2.0%)\n2023-07-11 04:38:56.862 [ INFO] Progress -   3.3 MB (4.0%)\n2023-07-11 04:38:57.035 [ INFO] Progress -   3.3 MB (6.0%)\n2023-07-11 04:38:57.037 [ INFO] Progress -   3.3 MB (8.0%)\n2023-07-11 04:38:57.037 [ INFO] Progress -   3.3 MB (10.0%)\n2023-07-11 04:38:57.037 [ INFO] Progress -   3.3 MB (12.1%)\n2023-07-11 04:38:57.211 [ INFO] Progress -   3.3 MB (14.1%)\n2023-07-11 04:38:57.212 [ INFO] Progress -   3.3 MB (16.1%)\n2023-07-11 04:38:57.212 [ INFO] Progress -   3.3 MB (18.1%)\n2023-07-11 04:38:57.212 [ INFO] Progress -   3.3 MB (20.1%)\n2023-07-11 04:38:57.213 [ INFO] Progress -   3.3 MB (22.1%)\n2023-07-11 04:38:57.213 [ INFO] Progress -   3.3 MB (24.1%)\n2023-07-11 04:38:57.386 [ INFO] Progress -   3.3 MB (26.1%)\n2023-07-11 04:38:57.386 [ INFO] Progress -   3.3 MB (28.1%)\n2023-07-11 04:38:57.386 [ INFO] Progress -   3.3 MB (30.1%)\n2023-07-11 04:38:57.387 [ INFO] Progress -   3.3 MB (32.2%)\n2023-07-11 04:38:57.388 [ INFO] Progress -   3.3 MB (34.2%)\n2023-07-11 04:38:57.388 [ INFO] Progress -   3.3 MB (36.2%)\n2023-07-11 04:38:57.388 [ INFO] Progress -   3.3 MB (38.2%)\n2023-07-11 04:38:57.389 [ INFO] Progress -   3.3 MB (40.2%)\n2023-07-11 04:38:57.389 [ INFO] Progress -   3.3 MB (42.2%)\n2023-07-11 04:38:57.389 [ INFO] Progress -   3.3 MB (44.2%)\n2023-07-11 04:38:57.389 [ INFO] Progress -   3.3 MB (46.2%)\n2023-07-11 04:38:57.390 [ INFO] Progress -   3.3 MB (48.2%)\n2023-07-11 04:38:57.390 [ INFO] Progress -   3.3 MB (50.2%)\n2023-07-11 04:38:57.390 [ INFO] Progress -   3.3 MB (52.3%)\n2023-07-11 04:38:57.561 [ INFO] Progress -   3.3 MB (54.3%)\n2023-07-11 04:38:57.561 [ INFO] Progress -   3.3 MB (56.3%)\n2023-07-11 04:38:57.562 [ INFO] Progress -   3.3 MB (58.3%)\n2023-07-11 04:38:57.562 [ INFO] Progress -   3.3 MB (60.3%)\n2023-07-11 04:38:57.562 [ INFO] Progress -   3.3 MB (62.3%)\n2023-07-11 04:38:57.562 [ INFO] Progress -   3.3 MB (64.3%)\n2023-07-11 04:38:57.563 [ INFO] Progress -   3.3 MB (66.3%)\n2023-07-11 04:38:57.564 [ INFO] Progress -   3.3 MB (68.3%)\n2023-07-11 04:38:57.564 [ INFO] Progress -   3.3 MB (70.3%)\n2023-07-11 04:38:57.564 [ INFO] Progress -   3.3 MB (72.4%)\n2023-07-11 04:38:57.565 [ INFO] Progress -   3.3 MB (74.4%)\n2023-07-11 04:38:57.565 [ INFO] Progress -   3.3 MB (76.4%)\n2023-07-11 04:38:57.565 [ INFO] Progress -   3.3 MB (78.4%)\n2023-07-11 04:38:57.565 [ INFO] Progress -   3.3 MB (80.4%)\n2023-07-11 04:38:57.566 [ INFO] Progress -   3.3 MB (82.4%)\n2023-07-11 04:38:57.566 [ INFO] Progress -   3.3 MB (84.4%)\n2023-07-11 04:38:57.566 [ INFO] Progress -   3.3 MB (86.4%)\n2023-07-11 04:38:57.566 [ INFO] Progress -   3.3 MB (88.4%)\n2023-07-11 04:38:57.567 [ INFO] Progress -   3.3 MB (90.4%)\n2023-07-11 04:38:57.567 [ INFO] Progress -   3.3 MB (92.5%)\n2023-07-11 04:38:57.567 [ INFO] Progress -   3.3 MB (94.5%)\n2023-07-11 04:38:57.567 [ INFO] Progress -   3.3 MB (96.5%)\n2023-07-11 04:38:57.568 [ INFO] Progress -   3.3 MB (98.5%)\n2023-07-11 04:38:57.569 [ INFO] Progress -   3.3 MB (100.0%)\n2023-07-11 04:38:57.569 [ INFO] Processing  time : 0:00:31.874707\n2023-07-11 04:38:57.569 [ INFO] Downloading time : 0:00:01.232594\n2023-07-11 04:38:57.569 [ INFO] Total time       : 0:00:33.107301\n2023-07-11 04:38:57.569 [ INFO] Download rate    : 1.6 MB/s\n2023-07-11 04:38:57.569 [ INFO] Save into        : motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200012.nc\n2023-07-11 04:38:57.599 [ INFO] Done\n2023-07-11 04:38:57.840 [ INFO] Asynchronous mode set\n2023-07-11 04:38:57.840 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:39:04.934 [ INFO] Requesting file to download (this can take a while)...\n2023-07-11 04:39:11.874 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:39:27.152 [ INFO] The product is ready for download\n2023-07-11 04:39:27.153 [ INFO] Downloading file (this can take a while)...\n2023-07-11 04:39:27.897 [ INFO] File type: application/x-netcdf\n2023-07-11 04:39:27.897 [ INFO] File size: 3.3 MB (3260684 B)\n2023-07-11 04:39:27.897 [ INFO] Downloading file /home/jovyan/shared/minh-notebooks/motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200101.nc\n2023-07-11 04:39:28.247 [ INFO] Progress -   3.3 MB (2.0%)\n2023-07-11 04:39:28.422 [ INFO] Progress -   3.3 MB (4.0%)\n2023-07-11 04:39:28.597 [ INFO] Progress -   3.3 MB (6.0%)\n2023-07-11 04:39:28.597 [ INFO] Progress -   3.3 MB (8.0%)\n2023-07-11 04:39:28.772 [ INFO] Progress -   3.3 MB (10.0%)\n2023-07-11 04:39:28.772 [ INFO] Progress -   3.3 MB (12.1%)\n2023-07-11 04:39:28.772 [ INFO] Progress -   3.3 MB (14.1%)\n2023-07-11 04:39:28.773 [ INFO] Progress -   3.3 MB (16.1%)\n2023-07-11 04:39:28.773 [ INFO] Progress -   3.3 MB (18.1%)\n2023-07-11 04:39:28.947 [ INFO] Progress -   3.3 MB (20.1%)\n2023-07-11 04:39:28.947 [ INFO] Progress -   3.3 MB (22.1%)\n2023-07-11 04:39:28.947 [ INFO] Progress -   3.3 MB (24.1%)\n2023-07-11 04:39:28.948 [ INFO] Progress -   3.3 MB (26.1%)\n2023-07-11 04:39:28.948 [ INFO] Progress -   3.3 MB (28.1%)\n2023-07-11 04:39:28.948 [ INFO] Progress -   3.3 MB (30.1%)\n2023-07-11 04:39:29.122 [ INFO] Progress -   3.3 MB (32.2%)\n2023-07-11 04:39:29.123 [ INFO] Progress -   3.3 MB (34.2%)\n2023-07-11 04:39:29.123 [ INFO] Progress -   3.3 MB (36.2%)\n2023-07-11 04:39:29.124 [ INFO] Progress -   3.3 MB (38.2%)\n2023-07-11 04:39:29.124 [ INFO] Progress -   3.3 MB (40.2%)\n2023-07-11 04:39:29.124 [ INFO] Progress -   3.3 MB (42.2%)\n2023-07-11 04:39:29.124 [ INFO] Progress -   3.3 MB (44.2%)\n2023-07-11 04:39:29.125 [ INFO] Progress -   3.3 MB (46.2%)\n2023-07-11 04:39:29.125 [ INFO] Progress -   3.3 MB (48.2%)\n2023-07-11 04:39:29.125 [ INFO] Progress -   3.3 MB (50.2%)\n2023-07-11 04:39:29.125 [ INFO] Progress -   3.3 MB (52.3%)\n2023-07-11 04:39:29.125 [ INFO] Progress -   3.3 MB (54.3%)\n2023-07-11 04:39:29.126 [ INFO] Progress -   3.3 MB (56.3%)\n2023-07-11 04:39:29.126 [ INFO] Progress -   3.3 MB (58.3%)\n2023-07-11 04:39:29.297 [ INFO] Progress -   3.3 MB (60.3%)\n2023-07-11 04:39:29.297 [ INFO] Progress -   3.3 MB (62.3%)\n2023-07-11 04:39:29.297 [ INFO] Progress -   3.3 MB (64.3%)\n2023-07-11 04:39:29.298 [ INFO] Progress -   3.3 MB (66.3%)\n2023-07-11 04:39:29.299 [ INFO] Progress -   3.3 MB (68.3%)\n2023-07-11 04:39:29.299 [ INFO] Progress -   3.3 MB (70.3%)\n2023-07-11 04:39:29.299 [ INFO] Progress -   3.3 MB (72.4%)\n2023-07-11 04:39:29.300 [ INFO] Progress -   3.3 MB (74.4%)\n2023-07-11 04:39:29.300 [ INFO] Progress -   3.3 MB (76.4%)\n2023-07-11 04:39:29.300 [ INFO] Progress -   3.3 MB (78.4%)\n2023-07-11 04:39:29.300 [ INFO] Progress -   3.3 MB (80.4%)\n2023-07-11 04:39:29.301 [ INFO] Progress -   3.3 MB (82.4%)\n2023-07-11 04:39:29.301 [ INFO] Progress -   3.3 MB (84.4%)\n2023-07-11 04:39:29.301 [ INFO] Progress -   3.3 MB (86.4%)\n2023-07-11 04:39:29.301 [ INFO] Progress -   3.3 MB (88.4%)\n2023-07-11 04:39:29.301 [ INFO] Progress -   3.3 MB (90.4%)\n2023-07-11 04:39:29.302 [ INFO] Progress -   3.3 MB (92.5%)\n2023-07-11 04:39:29.302 [ INFO] Progress -   3.3 MB (94.5%)\n2023-07-11 04:39:29.302 [ INFO] Progress -   3.3 MB (96.5%)\n2023-07-11 04:39:29.303 [ INFO] Progress -   3.3 MB (98.5%)\n2023-07-11 04:39:29.303 [ INFO] Progress -   3.3 MB (100.0%)\n2023-07-11 04:39:29.303 [ INFO] Processing  time : 0:00:30.059472\n2023-07-11 04:39:29.303 [ INFO] Downloading time : 0:00:01.406073\n2023-07-11 04:39:29.303 [ INFO] Total time       : 0:00:31.465545\n2023-07-11 04:39:29.303 [ INFO] Download rate    : 1.5 MB/s\n2023-07-11 04:39:29.304 [ INFO] Save into        : motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200101.nc\n2023-07-11 04:39:29.334 [ INFO] Done\n2023-07-11 04:39:29.573 [ INFO] Asynchronous mode set\n2023-07-11 04:39:29.574 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:39:36.995 [ INFO] Requesting file to download (this can take a while)...\n2023-07-11 04:39:45.826 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:40:00.771 [ INFO] The product is ready for download\n2023-07-11 04:40:00.771 [ INFO] Downloading file (this can take a while)...\n2023-07-11 04:40:01.520 [ INFO] File type: application/x-netcdf\n2023-07-11 04:40:01.520 [ INFO] File size: 2.9 MB (2945744 B)\n2023-07-11 04:40:01.521 [ INFO] Downloading file /home/jovyan/shared/minh-notebooks/motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200102.nc\n2023-07-11 04:40:01.870 [ INFO] Progress -   2.9 MB (2.2%)\n2023-07-11 04:40:02.045 [ INFO] Progress -   2.9 MB (4.4%)\n2023-07-11 04:40:02.220 [ INFO] Progress -   2.9 MB (6.7%)\n2023-07-11 04:40:02.220 [ INFO] Progress -   2.9 MB (8.9%)\n2023-07-11 04:40:02.221 [ INFO] Progress -   2.9 MB (11.1%)\n2023-07-11 04:40:02.221 [ INFO] Progress -   2.9 MB (13.3%)\n2023-07-11 04:40:02.395 [ INFO] Progress -   2.9 MB (15.6%)\n2023-07-11 04:40:02.396 [ INFO] Progress -   2.9 MB (17.8%)\n2023-07-11 04:40:02.396 [ INFO] Progress -   2.9 MB (20.0%)\n2023-07-11 04:40:02.396 [ INFO] Progress -   2.9 MB (22.2%)\n2023-07-11 04:40:02.397 [ INFO] Progress -   2.9 MB (24.5%)\n2023-07-11 04:40:02.397 [ INFO] Progress -   2.9 MB (26.7%)\n2023-07-11 04:40:02.570 [ INFO] Progress -   2.9 MB (28.9%)\n2023-07-11 04:40:02.571 [ INFO] Progress -   2.9 MB (31.1%)\n2023-07-11 04:40:02.571 [ INFO] Progress -   2.9 MB (33.4%)\n2023-07-11 04:40:02.571 [ INFO] Progress -   2.9 MB (35.6%)\n2023-07-11 04:40:02.572 [ INFO] Progress -   2.9 MB (37.8%)\n2023-07-11 04:40:02.573 [ INFO] Progress -   2.9 MB (40.0%)\n2023-07-11 04:40:02.573 [ INFO] Progress -   2.9 MB (42.3%)\n2023-07-11 04:40:02.573 [ INFO] Progress -   2.9 MB (44.5%)\n2023-07-11 04:40:02.573 [ INFO] Progress -   2.9 MB (46.7%)\n2023-07-11 04:40:02.573 [ INFO] Progress -   2.9 MB (48.9%)\n2023-07-11 04:40:02.574 [ INFO] Progress -   2.9 MB (51.2%)\n2023-07-11 04:40:02.574 [ INFO] Progress -   2.9 MB (53.4%)\n2023-07-11 04:40:02.745 [ INFO] Progress -   2.9 MB (55.6%)\n2023-07-11 04:40:02.746 [ INFO] Progress -   2.9 MB (57.8%)\n2023-07-11 04:40:02.746 [ INFO] Progress -   2.9 MB (60.1%)\n2023-07-11 04:40:02.747 [ INFO] Progress -   2.9 MB (62.3%)\n2023-07-11 04:40:02.747 [ INFO] Progress -   2.9 MB (64.5%)\n2023-07-11 04:40:02.747 [ INFO] Progress -   2.9 MB (66.7%)\n2023-07-11 04:40:02.747 [ INFO] Progress -   2.9 MB (69.0%)\n2023-07-11 04:40:02.748 [ INFO] Progress -   2.9 MB (71.2%)\n2023-07-11 04:40:02.748 [ INFO] Progress -   2.9 MB (73.4%)\n2023-07-11 04:40:02.749 [ INFO] Progress -   2.9 MB (75.6%)\n2023-07-11 04:40:02.749 [ INFO] Progress -   2.9 MB (77.9%)\n2023-07-11 04:40:02.749 [ INFO] Progress -   2.9 MB (80.1%)\n2023-07-11 04:40:02.749 [ INFO] Progress -   2.9 MB (82.3%)\n2023-07-11 04:40:02.750 [ INFO] Progress -   2.9 MB (84.5%)\n2023-07-11 04:40:02.750 [ INFO] Progress -   2.9 MB (86.8%)\n2023-07-11 04:40:02.750 [ INFO] Progress -   2.9 MB (89.0%)\n2023-07-11 04:40:02.750 [ INFO] Progress -   2.9 MB (91.2%)\n2023-07-11 04:40:02.751 [ INFO] Progress -   2.9 MB (93.4%)\n2023-07-11 04:40:02.751 [ INFO] Progress -   2.9 MB (95.7%)\n2023-07-11 04:40:02.751 [ INFO] Progress -   2.9 MB (97.9%)\n2023-07-11 04:40:02.752 [ INFO] Progress -   2.9 MB (100.0%)\n2023-07-11 04:40:02.752 [ INFO] Processing  time : 0:00:31.949474\n2023-07-11 04:40:02.752 [ INFO] Downloading time : 0:00:01.231045\n2023-07-11 04:40:02.752 [ INFO] Total time       : 0:00:33.180519\n2023-07-11 04:40:02.752 [ INFO] Download rate    : 1.5 MB/s\n2023-07-11 04:40:02.752 [ INFO] Save into        : motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200102.nc\n2023-07-11 04:40:02.784 [ INFO] Done\n2023-07-11 04:40:03.024 [ INFO] Asynchronous mode set\n2023-07-11 04:40:03.024 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:40:12.056 [ INFO] Requesting file to download (this can take a while)...\n2023-07-11 04:40:18.657 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:40:33.365 [ INFO] The product is ready for download\n2023-07-11 04:40:33.365 [ INFO] Downloading file (this can take a while)...\n2023-07-11 04:40:34.109 [ INFO] File type: application/x-netcdf\n2023-07-11 04:40:34.109 [ INFO] File size: 3.3 MB (3260684 B)\n2023-07-11 04:40:34.109 [ INFO] Downloading file /home/jovyan/shared/minh-notebooks/motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200103.nc\n2023-07-11 04:40:34.460 [ INFO] Progress -   3.3 MB (2.0%)\n2023-07-11 04:40:34.635 [ INFO] Progress -   3.3 MB (4.0%)\n2023-07-11 04:40:34.808 [ INFO] Progress -   3.3 MB (6.0%)\n2023-07-11 04:40:34.810 [ INFO] Progress -   3.3 MB (8.0%)\n2023-07-11 04:40:34.983 [ INFO] Progress -   3.3 MB (10.0%)\n2023-07-11 04:40:34.984 [ INFO] Progress -   3.3 MB (12.1%)\n2023-07-11 04:40:34.984 [ INFO] Progress -   3.3 MB (14.1%)\n2023-07-11 04:40:34.985 [ INFO] Progress -   3.3 MB (16.1%)\n2023-07-11 04:40:34.985 [ INFO] Progress -   3.3 MB (18.1%)\n2023-07-11 04:40:34.986 [ INFO] Progress -   3.3 MB (20.1%)\n2023-07-11 04:40:35.159 [ INFO] Progress -   3.3 MB (22.1%)\n2023-07-11 04:40:35.159 [ INFO] Progress -   3.3 MB (24.1%)\n2023-07-11 04:40:35.159 [ INFO] Progress -   3.3 MB (26.1%)\n2023-07-11 04:40:35.160 [ INFO] Progress -   3.3 MB (28.1%)\n2023-07-11 04:40:35.160 [ INFO] Progress -   3.3 MB (30.1%)\n2023-07-11 04:40:35.160 [ INFO] Progress -   3.3 MB (32.2%)\n2023-07-11 04:40:35.161 [ INFO] Progress -   3.3 MB (34.2%)\n2023-07-11 04:40:35.161 [ INFO] Progress -   3.3 MB (36.2%)\n2023-07-11 04:40:35.162 [ INFO] Progress -   3.3 MB (38.2%)\n2023-07-11 04:40:35.162 [ INFO] Progress -   3.3 MB (40.2%)\n2023-07-11 04:40:35.162 [ INFO] Progress -   3.3 MB (42.2%)\n2023-07-11 04:40:35.334 [ INFO] Progress -   3.3 MB (44.2%)\n2023-07-11 04:40:35.334 [ INFO] Progress -   3.3 MB (46.2%)\n2023-07-11 04:40:35.334 [ INFO] Progress -   3.3 MB (48.2%)\n2023-07-11 04:40:35.335 [ INFO] Progress -   3.3 MB (50.2%)\n2023-07-11 04:40:35.335 [ INFO] Progress -   3.3 MB (52.3%)\n2023-07-11 04:40:35.335 [ INFO] Progress -   3.3 MB (54.3%)\n2023-07-11 04:40:35.335 [ INFO] Progress -   3.3 MB (56.3%)\n2023-07-11 04:40:35.336 [ INFO] Progress -   3.3 MB (58.3%)\n2023-07-11 04:40:35.336 [ INFO] Progress -   3.3 MB (60.3%)\n2023-07-11 04:40:35.336 [ INFO] Progress -   3.3 MB (62.3%)\n2023-07-11 04:40:35.337 [ INFO] Progress -   3.3 MB (64.3%)\n2023-07-11 04:40:35.337 [ INFO] Progress -   3.3 MB (66.3%)\n2023-07-11 04:40:35.338 [ INFO] Progress -   3.3 MB (68.3%)\n2023-07-11 04:40:35.338 [ INFO] Progress -   3.3 MB (70.3%)\n2023-07-11 04:40:35.338 [ INFO] Progress -   3.3 MB (72.4%)\n2023-07-11 04:40:35.338 [ INFO] Progress -   3.3 MB (74.4%)\n2023-07-11 04:40:35.339 [ INFO] Progress -   3.3 MB (76.4%)\n2023-07-11 04:40:35.339 [ INFO] Progress -   3.3 MB (78.4%)\n2023-07-11 04:40:35.339 [ INFO] Progress -   3.3 MB (80.4%)\n2023-07-11 04:40:35.339 [ INFO] Progress -   3.3 MB (82.4%)\n2023-07-11 04:40:35.340 [ INFO] Progress -   3.3 MB (84.4%)\n2023-07-11 04:40:35.340 [ INFO] Progress -   3.3 MB (86.4%)\n2023-07-11 04:40:35.509 [ INFO] Progress -   3.3 MB (88.4%)\n2023-07-11 04:40:35.509 [ INFO] Progress -   3.3 MB (90.4%)\n2023-07-11 04:40:35.509 [ INFO] Progress -   3.3 MB (92.5%)\n2023-07-11 04:40:35.510 [ INFO] Progress -   3.3 MB (94.5%)\n2023-07-11 04:40:35.510 [ INFO] Progress -   3.3 MB (96.5%)\n2023-07-11 04:40:35.511 [ INFO] Progress -   3.3 MB (98.5%)\n2023-07-11 04:40:35.511 [ INFO] Progress -   3.3 MB (100.0%)\n2023-07-11 04:40:35.511 [ INFO] Processing  time : 0:00:31.085699\n2023-07-11 04:40:35.511 [ INFO] Downloading time : 0:00:01.401936\n2023-07-11 04:40:35.511 [ INFO] Total time       : 0:00:32.487635\n2023-07-11 04:40:35.511 [ INFO] Download rate    : 1.5 MB/s\n2023-07-11 04:40:35.511 [ INFO] Save into        : motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200103.nc\n2023-07-11 04:40:35.541 [ INFO] Done\n2023-07-11 04:40:35.783 [ INFO] Asynchronous mode set\n2023-07-11 04:40:35.783 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:40:42.416 [ INFO] Requesting file to download (this can take a while)...\n2023-07-11 04:40:51.415 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:41:07.906 [ INFO] The product is ready for download\n2023-07-11 04:41:07.906 [ INFO] Downloading file (this can take a while)...\n2023-07-11 04:41:08.658 [ INFO] File type: application/x-netcdf\n2023-07-11 04:41:08.658 [ INFO] File size: 3.2 MB (3155704 B)\n2023-07-11 04:41:08.658 [ INFO] Downloading file /home/jovyan/shared/minh-notebooks/motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200104.nc\n2023-07-11 04:41:09.007 [ INFO] Progress -   3.2 MB (2.1%)\n2023-07-11 04:41:09.182 [ INFO] Progress -   3.2 MB (4.2%)\n2023-07-11 04:41:09.183 [ INFO] Progress -   3.2 MB (6.2%)\n2023-07-11 04:41:09.357 [ INFO] Progress -   3.2 MB (8.3%)\n2023-07-11 04:41:09.358 [ INFO] Progress -   3.2 MB (10.4%)\n2023-07-11 04:41:09.358 [ INFO] Progress -   3.2 MB (12.5%)\n2023-07-11 04:41:09.532 [ INFO] Progress -   3.2 MB (14.5%)\n2023-07-11 04:41:09.532 [ INFO] Progress -   3.2 MB (16.6%)\n2023-07-11 04:41:09.533 [ INFO] Progress -   3.2 MB (18.7%)\n2023-07-11 04:41:09.533 [ INFO] Progress -   3.2 MB (20.8%)\n2023-07-11 04:41:09.533 [ INFO] Progress -   3.2 MB (22.8%)\n2023-07-11 04:41:09.534 [ INFO] Progress -   3.2 MB (24.9%)\n2023-07-11 04:41:09.534 [ INFO] Progress -   3.2 MB (27.0%)\n2023-07-11 04:41:09.707 [ INFO] Progress -   3.2 MB (29.1%)\n2023-07-11 04:41:09.708 [ INFO] Progress -   3.2 MB (31.2%)\n2023-07-11 04:41:09.708 [ INFO] Progress -   3.2 MB (33.2%)\n2023-07-11 04:41:09.709 [ INFO] Progress -   3.2 MB (35.3%)\n2023-07-11 04:41:09.709 [ INFO] Progress -   3.2 MB (37.4%)\n2023-07-11 04:41:09.710 [ INFO] Progress -   3.2 MB (39.5%)\n2023-07-11 04:41:09.710 [ INFO] Progress -   3.2 MB (41.5%)\n2023-07-11 04:41:09.710 [ INFO] Progress -   3.2 MB (43.6%)\n2023-07-11 04:41:09.710 [ INFO] Progress -   3.2 MB (45.7%)\n2023-07-11 04:41:09.710 [ INFO] Progress -   3.2 MB (47.8%)\n2023-07-11 04:41:09.711 [ INFO] Progress -   3.2 MB (49.8%)\n2023-07-11 04:41:09.711 [ INFO] Progress -   3.2 MB (51.9%)\n2023-07-11 04:41:09.711 [ INFO] Progress -   3.2 MB (54.0%)\n2023-07-11 04:41:09.711 [ INFO] Progress -   3.2 MB (56.1%)\n2023-07-11 04:41:09.882 [ INFO] Progress -   3.2 MB (58.1%)\n2023-07-11 04:41:09.883 [ INFO] Progress -   3.2 MB (60.2%)\n2023-07-11 04:41:09.883 [ INFO] Progress -   3.2 MB (62.3%)\n2023-07-11 04:41:09.883 [ INFO] Progress -   3.2 MB (64.4%)\n2023-07-11 04:41:09.884 [ INFO] Progress -   3.2 MB (66.5%)\n2023-07-11 04:41:09.884 [ INFO] Progress -   3.2 MB (68.5%)\n2023-07-11 04:41:09.885 [ INFO] Progress -   3.2 MB (70.6%)\n2023-07-11 04:41:09.885 [ INFO] Progress -   3.2 MB (72.7%)\n2023-07-11 04:41:09.885 [ INFO] Progress -   3.2 MB (74.8%)\n2023-07-11 04:41:09.885 [ INFO] Progress -   3.2 MB (76.8%)\n2023-07-11 04:41:09.886 [ INFO] Progress -   3.2 MB (78.9%)\n2023-07-11 04:41:09.886 [ INFO] Progress -   3.2 MB (81.0%)\n2023-07-11 04:41:09.886 [ INFO] Progress -   3.2 MB (83.1%)\n2023-07-11 04:41:09.886 [ INFO] Progress -   3.2 MB (85.1%)\n2023-07-11 04:41:09.887 [ INFO] Progress -   3.2 MB (87.2%)\n2023-07-11 04:41:09.887 [ INFO] Progress -   3.2 MB (89.3%)\n2023-07-11 04:41:09.887 [ INFO] Progress -   3.2 MB (91.4%)\n2023-07-11 04:41:09.887 [ INFO] Progress -   3.2 MB (93.5%)\n2023-07-11 04:41:09.888 [ INFO] Progress -   3.2 MB (95.5%)\n2023-07-11 04:41:09.888 [ INFO] Progress -   3.2 MB (97.6%)\n2023-07-11 04:41:09.888 [ INFO] Progress -   3.2 MB (99.7%)\n2023-07-11 04:41:09.889 [ INFO] Progress -   3.2 MB (100.0%)\n2023-07-11 04:41:09.889 [ INFO] Processing  time : 0:00:32.877246\n2023-07-11 04:41:09.889 [ INFO] Downloading time : 0:00:01.230603\n2023-07-11 04:41:09.889 [ INFO] Total time       : 0:00:34.107849\n2023-07-11 04:41:09.889 [ INFO] Download rate    : 1.6 MB/s\n2023-07-11 04:41:09.889 [ INFO] Save into        : motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200104.nc\n2023-07-11 04:41:09.919 [ INFO] Done\n2023-07-11 04:41:10.168 [ INFO] Asynchronous mode set\n2023-07-11 04:41:10.169 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:41:17.159 [ INFO] Requesting file to download (this can take a while)...\n2023-07-11 04:41:23.689 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:41:40.330 [ INFO] The product is ready for download\n2023-07-11 04:41:40.330 [ INFO] Downloading file (this can take a while)...\n2023-07-11 04:41:41.240 [ INFO] File type: application/x-netcdf\n2023-07-11 04:41:41.240 [ INFO] File size: 3.3 MB (3260684 B)\n2023-07-11 04:41:41.240 [ INFO] Downloading file /home/jovyan/shared/minh-notebooks/motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200105.nc\n2023-07-11 04:41:41.413 [ INFO] Progress -   3.3 MB (2.0%)\n2023-07-11 04:41:41.586 [ INFO] Progress -   3.3 MB (4.0%)\n2023-07-11 04:41:41.756 [ INFO] Progress -   3.3 MB (6.0%)\n2023-07-11 04:41:41.758 [ INFO] Progress -   3.3 MB (8.0%)\n2023-07-11 04:41:41.758 [ INFO] Progress -   3.3 MB (10.0%)\n2023-07-11 04:41:41.929 [ INFO] Progress -   3.3 MB (12.1%)\n2023-07-11 04:41:41.929 [ INFO] Progress -   3.3 MB (14.1%)\n2023-07-11 04:41:41.931 [ INFO] Progress -   3.3 MB (16.1%)\n2023-07-11 04:41:41.931 [ INFO] Progress -   3.3 MB (18.1%)\n2023-07-11 04:41:41.931 [ INFO] Progress -   3.3 MB (20.1%)\n2023-07-11 04:41:41.931 [ INFO] Progress -   3.3 MB (22.1%)\n2023-07-11 04:41:41.932 [ INFO] Progress -   3.3 MB (24.1%)\n2023-07-11 04:41:42.101 [ INFO] Progress -   3.3 MB (26.1%)\n2023-07-11 04:41:42.102 [ INFO] Progress -   3.3 MB (28.1%)\n2023-07-11 04:41:42.103 [ INFO] Progress -   3.3 MB (30.1%)\n2023-07-11 04:41:42.103 [ INFO] Progress -   3.3 MB (32.2%)\n2023-07-11 04:41:42.104 [ INFO] Progress -   3.3 MB (34.2%)\n2023-07-11 04:41:42.104 [ INFO] Progress -   3.3 MB (36.2%)\n2023-07-11 04:41:42.105 [ INFO] Progress -   3.3 MB (38.2%)\n2023-07-11 04:41:42.105 [ INFO] Progress -   3.3 MB (40.2%)\n2023-07-11 04:41:42.105 [ INFO] Progress -   3.3 MB (42.2%)\n2023-07-11 04:41:42.105 [ INFO] Progress -   3.3 MB (44.2%)\n2023-07-11 04:41:42.106 [ INFO] Progress -   3.3 MB (46.2%)\n2023-07-11 04:41:42.106 [ INFO] Progress -   3.3 MB (48.2%)\n2023-07-11 04:41:42.274 [ INFO] Progress -   3.3 MB (50.2%)\n2023-07-11 04:41:42.274 [ INFO] Progress -   3.3 MB (52.3%)\n2023-07-11 04:41:42.274 [ INFO] Progress -   3.3 MB (54.3%)\n2023-07-11 04:41:42.275 [ INFO] Progress -   3.3 MB (56.3%)\n2023-07-11 04:41:42.275 [ INFO] Progress -   3.3 MB (58.3%)\n2023-07-11 04:41:42.275 [ INFO] Progress -   3.3 MB (60.3%)\n2023-07-11 04:41:42.276 [ INFO] Progress -   3.3 MB (62.3%)\n2023-07-11 04:41:42.277 [ INFO] Progress -   3.3 MB (64.3%)\n2023-07-11 04:41:42.277 [ INFO] Progress -   3.3 MB (66.3%)\n2023-07-11 04:41:42.278 [ INFO] Progress -   3.3 MB (68.3%)\n2023-07-11 04:41:42.278 [ INFO] Progress -   3.3 MB (70.3%)\n2023-07-11 04:41:42.278 [ INFO] Progress -   3.3 MB (72.4%)\n2023-07-11 04:41:42.278 [ INFO] Progress -   3.3 MB (74.4%)\n2023-07-11 04:41:42.279 [ INFO] Progress -   3.3 MB (76.4%)\n2023-07-11 04:41:42.279 [ INFO] Progress -   3.3 MB (78.4%)\n2023-07-11 04:41:42.279 [ INFO] Progress -   3.3 MB (80.4%)\n2023-07-11 04:41:42.279 [ INFO] Progress -   3.3 MB (82.4%)\n2023-07-11 04:41:42.279 [ INFO] Progress -   3.3 MB (84.4%)\n2023-07-11 04:41:42.280 [ INFO] Progress -   3.3 MB (86.4%)\n2023-07-11 04:41:42.280 [ INFO] Progress -   3.3 MB (88.4%)\n2023-07-11 04:41:42.280 [ INFO] Progress -   3.3 MB (90.4%)\n2023-07-11 04:41:42.280 [ INFO] Progress -   3.3 MB (92.5%)\n2023-07-11 04:41:42.281 [ INFO] Progress -   3.3 MB (94.5%)\n2023-07-11 04:41:42.281 [ INFO] Progress -   3.3 MB (96.5%)\n2023-07-11 04:41:42.282 [ INFO] Progress -   3.3 MB (98.5%)\n2023-07-11 04:41:42.282 [ INFO] Progress -   3.3 MB (100.0%)\n2023-07-11 04:41:42.282 [ INFO] Processing  time : 0:00:31.073788\n2023-07-11 04:41:42.282 [ INFO] Downloading time : 0:00:01.041936\n2023-07-11 04:41:42.282 [ INFO] Total time       : 0:00:32.115724\n2023-07-11 04:41:42.282 [ INFO] Download rate    : 1.7 MB/s\n2023-07-11 04:41:42.282 [ INFO] Save into        : motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200105.nc\n2023-07-11 04:41:42.313 [ INFO] Done\n2023-07-11 04:41:42.557 [ INFO] Asynchronous mode set\n2023-07-11 04:41:42.557 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:41:51.498 [ INFO] Requesting file to download (this can take a while)...\n2023-07-11 04:41:58.720 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:42:13.967 [ INFO] The product is ready for download\n2023-07-11 04:42:13.967 [ INFO] Downloading file (this can take a while)...\n2023-07-11 04:42:14.703 [ INFO] File type: application/x-netcdf\n2023-07-11 04:42:14.703 [ INFO] File size: 3.2 MB (3155704 B)\n2023-07-11 04:42:14.704 [ INFO] Downloading file /home/jovyan/shared/minh-notebooks/motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200106.nc\n2023-07-11 04:42:15.048 [ INFO] Progress -   3.2 MB (2.1%)\n2023-07-11 04:42:15.220 [ INFO] Progress -   3.2 MB (4.2%)\n2023-07-11 04:42:15.221 [ INFO] Progress -   3.2 MB (6.2%)\n2023-07-11 04:42:15.392 [ INFO] Progress -   3.2 MB (8.3%)\n2023-07-11 04:42:15.393 [ INFO] Progress -   3.2 MB (10.4%)\n2023-07-11 04:42:15.394 [ INFO] Progress -   3.2 MB (12.5%)\n2023-07-11 04:42:15.564 [ INFO] Progress -   3.2 MB (14.5%)\n2023-07-11 04:42:15.565 [ INFO] Progress -   3.2 MB (16.6%)\n2023-07-11 04:42:15.565 [ INFO] Progress -   3.2 MB (18.7%)\n2023-07-11 04:42:15.565 [ INFO] Progress -   3.2 MB (20.8%)\n2023-07-11 04:42:15.566 [ INFO] Progress -   3.2 MB (22.8%)\n2023-07-11 04:42:15.566 [ INFO] Progress -   3.2 MB (24.9%)\n2023-07-11 04:42:15.566 [ INFO] Progress -   3.2 MB (27.0%)\n2023-07-11 04:42:15.736 [ INFO] Progress -   3.2 MB (29.1%)\n2023-07-11 04:42:15.737 [ INFO] Progress -   3.2 MB (31.2%)\n2023-07-11 04:42:15.737 [ INFO] Progress -   3.2 MB (33.2%)\n2023-07-11 04:42:15.739 [ INFO] Progress -   3.2 MB (35.3%)\n2023-07-11 04:42:15.739 [ INFO] Progress -   3.2 MB (37.4%)\n2023-07-11 04:42:15.739 [ INFO] Progress -   3.2 MB (39.5%)\n2023-07-11 04:42:15.739 [ INFO] Progress -   3.2 MB (41.5%)\n2023-07-11 04:42:15.740 [ INFO] Progress -   3.2 MB (43.6%)\n2023-07-11 04:42:15.740 [ INFO] Progress -   3.2 MB (45.7%)\n2023-07-11 04:42:15.740 [ INFO] Progress -   3.2 MB (47.8%)\n2023-07-11 04:42:15.740 [ INFO] Progress -   3.2 MB (49.8%)\n2023-07-11 04:42:15.741 [ INFO] Progress -   3.2 MB (51.9%)\n2023-07-11 04:42:15.741 [ INFO] Progress -   3.2 MB (54.0%)\n2023-07-11 04:42:15.741 [ INFO] Progress -   3.2 MB (56.1%)\n2023-07-11 04:42:15.909 [ INFO] Progress -   3.2 MB (58.1%)\n2023-07-11 04:42:15.909 [ INFO] Progress -   3.2 MB (60.2%)\n2023-07-11 04:42:15.910 [ INFO] Progress -   3.2 MB (62.3%)\n2023-07-11 04:42:15.910 [ INFO] Progress -   3.2 MB (64.4%)\n2023-07-11 04:42:15.910 [ INFO] Progress -   3.2 MB (66.5%)\n2023-07-11 04:42:15.911 [ INFO] Progress -   3.2 MB (68.5%)\n2023-07-11 04:42:15.911 [ INFO] Progress -   3.2 MB (70.6%)\n2023-07-11 04:42:15.911 [ INFO] Progress -   3.2 MB (72.7%)\n2023-07-11 04:42:15.912 [ INFO] Progress -   3.2 MB (74.8%)\n2023-07-11 04:42:15.912 [ INFO] Progress -   3.2 MB (76.8%)\n2023-07-11 04:42:15.912 [ INFO] Progress -   3.2 MB (78.9%)\n2023-07-11 04:42:15.912 [ INFO] Progress -   3.2 MB (81.0%)\n2023-07-11 04:42:15.913 [ INFO] Progress -   3.2 MB (83.1%)\n2023-07-11 04:42:15.913 [ INFO] Progress -   3.2 MB (85.1%)\n2023-07-11 04:42:15.913 [ INFO] Progress -   3.2 MB (87.2%)\n2023-07-11 04:42:15.913 [ INFO] Progress -   3.2 MB (89.3%)\n2023-07-11 04:42:15.914 [ INFO] Progress -   3.2 MB (91.4%)\n2023-07-11 04:42:15.914 [ INFO] Progress -   3.2 MB (93.5%)\n2023-07-11 04:42:15.914 [ INFO] Progress -   3.2 MB (95.5%)\n2023-07-11 04:42:15.914 [ INFO] Progress -   3.2 MB (97.6%)\n2023-07-11 04:42:15.914 [ INFO] Progress -   3.2 MB (99.7%)\n2023-07-11 04:42:15.915 [ INFO] Progress -   3.2 MB (100.0%)\n2023-07-11 04:42:15.915 [ INFO] Processing  time : 0:00:32.148979\n2023-07-11 04:42:15.915 [ INFO] Downloading time : 0:00:01.211730\n2023-07-11 04:42:15.915 [ INFO] Total time       : 0:00:33.360709\n2023-07-11 04:42:15.915 [ INFO] Download rate    : 1.6 MB/s\n2023-07-11 04:42:15.915 [ INFO] Save into        : motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200106.nc\n2023-07-11 04:42:15.945 [ INFO] Done\n2023-07-11 04:42:16.186 [ INFO] Asynchronous mode set\n2023-07-11 04:42:16.186 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:42:22.965 [ INFO] Requesting file to download (this can take a while)...\n2023-07-11 04:42:29.614 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:42:44.253 [ INFO] The product is ready for download\n2023-07-11 04:42:44.254 [ INFO] Downloading file (this can take a while)...\n2023-07-11 04:42:45.157 [ INFO] File type: application/x-netcdf\n2023-07-11 04:42:45.157 [ INFO] File size: 3.3 MB (3260684 B)\n2023-07-11 04:42:45.157 [ INFO] Downloading file /home/jovyan/shared/minh-notebooks/motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200107.nc\n2023-07-11 04:42:45.330 [ INFO] Progress -   3.3 MB (2.0%)\n2023-07-11 04:42:45.503 [ INFO] Progress -   3.3 MB (4.0%)\n2023-07-11 04:42:45.673 [ INFO] Progress -   3.3 MB (6.0%)\n2023-07-11 04:42:45.675 [ INFO] Progress -   3.3 MB (8.0%)\n2023-07-11 04:42:45.675 [ INFO] Progress -   3.3 MB (10.0%)\n2023-07-11 04:42:45.846 [ INFO] Progress -   3.3 MB (12.1%)\n2023-07-11 04:42:45.846 [ INFO] Progress -   3.3 MB (14.1%)\n2023-07-11 04:42:45.848 [ INFO] Progress -   3.3 MB (16.1%)\n2023-07-11 04:42:45.848 [ INFO] Progress -   3.3 MB (18.1%)\n2023-07-11 04:42:45.848 [ INFO] Progress -   3.3 MB (20.1%)\n2023-07-11 04:42:45.848 [ INFO] Progress -   3.3 MB (22.1%)\n2023-07-11 04:42:45.849 [ INFO] Progress -   3.3 MB (24.1%)\n2023-07-11 04:42:46.018 [ INFO] Progress -   3.3 MB (26.1%)\n2023-07-11 04:42:46.019 [ INFO] Progress -   3.3 MB (28.1%)\n2023-07-11 04:42:46.020 [ INFO] Progress -   3.3 MB (30.1%)\n2023-07-11 04:42:46.020 [ INFO] Progress -   3.3 MB (32.2%)\n2023-07-11 04:42:46.021 [ INFO] Progress -   3.3 MB (34.2%)\n2023-07-11 04:42:46.021 [ INFO] Progress -   3.3 MB (36.2%)\n2023-07-11 04:42:46.022 [ INFO] Progress -   3.3 MB (38.2%)\n2023-07-11 04:42:46.022 [ INFO] Progress -   3.3 MB (40.2%)\n2023-07-11 04:42:46.022 [ INFO] Progress -   3.3 MB (42.2%)\n2023-07-11 04:42:46.022 [ INFO] Progress -   3.3 MB (44.2%)\n2023-07-11 04:42:46.023 [ INFO] Progress -   3.3 MB (46.2%)\n2023-07-11 04:42:46.023 [ INFO] Progress -   3.3 MB (48.2%)\n2023-07-11 04:42:46.190 [ INFO] Progress -   3.3 MB (50.2%)\n2023-07-11 04:42:46.191 [ INFO] Progress -   3.3 MB (52.3%)\n2023-07-11 04:42:46.191 [ INFO] Progress -   3.3 MB (54.3%)\n2023-07-11 04:42:46.191 [ INFO] Progress -   3.3 MB (56.3%)\n2023-07-11 04:42:46.192 [ INFO] Progress -   3.3 MB (58.3%)\n2023-07-11 04:42:46.193 [ INFO] Progress -   3.3 MB (60.3%)\n2023-07-11 04:42:46.193 [ INFO] Progress -   3.3 MB (62.3%)\n2023-07-11 04:42:46.193 [ INFO] Progress -   3.3 MB (64.3%)\n2023-07-11 04:42:46.194 [ INFO] Progress -   3.3 MB (66.3%)\n2023-07-11 04:42:46.194 [ INFO] Progress -   3.3 MB (68.3%)\n2023-07-11 04:42:46.195 [ INFO] Progress -   3.3 MB (70.3%)\n2023-07-11 04:42:46.195 [ INFO] Progress -   3.3 MB (72.4%)\n2023-07-11 04:42:46.195 [ INFO] Progress -   3.3 MB (74.4%)\n2023-07-11 04:42:46.195 [ INFO] Progress -   3.3 MB (76.4%)\n2023-07-11 04:42:46.196 [ INFO] Progress -   3.3 MB (78.4%)\n2023-07-11 04:42:46.196 [ INFO] Progress -   3.3 MB (80.4%)\n2023-07-11 04:42:46.196 [ INFO] Progress -   3.3 MB (82.4%)\n2023-07-11 04:42:46.196 [ INFO] Progress -   3.3 MB (84.4%)\n2023-07-11 04:42:46.197 [ INFO] Progress -   3.3 MB (86.4%)\n2023-07-11 04:42:46.197 [ INFO] Progress -   3.3 MB (88.4%)\n2023-07-11 04:42:46.197 [ INFO] Progress -   3.3 MB (90.4%)\n2023-07-11 04:42:46.197 [ INFO] Progress -   3.3 MB (92.5%)\n2023-07-11 04:42:46.198 [ INFO] Progress -   3.3 MB (94.5%)\n2023-07-11 04:42:46.198 [ INFO] Progress -   3.3 MB (96.5%)\n2023-07-11 04:42:46.199 [ INFO] Progress -   3.3 MB (98.5%)\n2023-07-11 04:42:46.199 [ INFO] Progress -   3.3 MB (100.0%)\n2023-07-11 04:42:46.199 [ INFO] Processing  time : 0:00:28.973583\n2023-07-11 04:42:46.199 [ INFO] Downloading time : 0:00:01.042066\n2023-07-11 04:42:46.199 [ INFO] Total time       : 0:00:30.015649\n2023-07-11 04:42:46.199 [ INFO] Download rate    : 1.7 MB/s\n2023-07-11 04:42:46.199 [ INFO] Save into        : motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200107.nc\n2023-07-11 04:42:46.232 [ INFO] Done\n2023-07-11 04:42:46.476 [ INFO] Asynchronous mode set\n2023-07-11 04:42:46.476 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:42:52.332 [ INFO] Requesting file to download (this can take a while)...\n2023-07-11 04:42:59.632 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:43:13.124 [ INFO] The product is ready for download\n2023-07-11 04:43:13.124 [ INFO] Downloading file (this can take a while)...\n2023-07-11 04:43:13.856 [ INFO] File type: application/x-netcdf\n2023-07-11 04:43:13.857 [ INFO] File size: 3.3 MB (3260684 B)\n2023-07-11 04:43:13.857 [ INFO] Downloading file /home/jovyan/shared/minh-notebooks/motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200108.nc\n2023-07-11 04:43:14.200 [ INFO] Progress -   3.3 MB (2.0%)\n2023-07-11 04:43:14.373 [ INFO] Progress -   3.3 MB (4.0%)\n2023-07-11 04:43:14.545 [ INFO] Progress -   3.3 MB (6.0%)\n2023-07-11 04:43:14.546 [ INFO] Progress -   3.3 MB (8.0%)\n2023-07-11 04:43:14.718 [ INFO] Progress -   3.3 MB (10.0%)\n2023-07-11 04:43:14.719 [ INFO] Progress -   3.3 MB (12.1%)\n2023-07-11 04:43:14.719 [ INFO] Progress -   3.3 MB (14.1%)\n2023-07-11 04:43:14.719 [ INFO] Progress -   3.3 MB (16.1%)\n2023-07-11 04:43:14.720 [ INFO] Progress -   3.3 MB (18.1%)\n2023-07-11 04:43:14.890 [ INFO] Progress -   3.3 MB (20.1%)\n2023-07-11 04:43:14.891 [ INFO] Progress -   3.3 MB (22.1%)\n2023-07-11 04:43:14.891 [ INFO] Progress -   3.3 MB (24.1%)\n2023-07-11 04:43:14.892 [ INFO] Progress -   3.3 MB (26.1%)\n2023-07-11 04:43:14.893 [ INFO] Progress -   3.3 MB (28.1%)\n2023-07-11 04:43:14.893 [ INFO] Progress -   3.3 MB (30.1%)\n2023-07-11 04:43:14.893 [ INFO] Progress -   3.3 MB (32.2%)\n2023-07-11 04:43:14.895 [ INFO] Progress -   3.3 MB (34.2%)\n2023-07-11 04:43:14.895 [ INFO] Progress -   3.3 MB (36.2%)\n2023-07-11 04:43:15.062 [ INFO] Progress -   3.3 MB (38.2%)\n2023-07-11 04:43:15.063 [ INFO] Progress -   3.3 MB (40.2%)\n2023-07-11 04:43:15.064 [ INFO] Progress -   3.3 MB (42.2%)\n2023-07-11 04:43:15.064 [ INFO] Progress -   3.3 MB (44.2%)\n2023-07-11 04:43:15.064 [ INFO] Progress -   3.3 MB (46.2%)\n2023-07-11 04:43:15.064 [ INFO] Progress -   3.3 MB (48.2%)\n2023-07-11 04:43:15.065 [ INFO] Progress -   3.3 MB (50.2%)\n2023-07-11 04:43:15.065 [ INFO] Progress -   3.3 MB (52.3%)\n2023-07-11 04:43:15.066 [ INFO] Progress -   3.3 MB (54.3%)\n2023-07-11 04:43:15.066 [ INFO] Progress -   3.3 MB (56.3%)\n2023-07-11 04:43:15.067 [ INFO] Progress -   3.3 MB (58.3%)\n2023-07-11 04:43:15.067 [ INFO] Progress -   3.3 MB (60.3%)\n2023-07-11 04:43:15.067 [ INFO] Progress -   3.3 MB (62.3%)\n2023-07-11 04:43:15.068 [ INFO] Progress -   3.3 MB (64.3%)\n2023-07-11 04:43:15.068 [ INFO] Progress -   3.3 MB (66.3%)\n2023-07-11 04:43:15.069 [ INFO] Progress -   3.3 MB (68.3%)\n2023-07-11 04:43:15.069 [ INFO] Progress -   3.3 MB (70.3%)\n2023-07-11 04:43:15.069 [ INFO] Progress -   3.3 MB (72.4%)\n2023-07-11 04:43:15.069 [ INFO] Progress -   3.3 MB (74.4%)\n2023-07-11 04:43:15.070 [ INFO] Progress -   3.3 MB (76.4%)\n2023-07-11 04:43:15.235 [ INFO] Progress -   3.3 MB (78.4%)\n2023-07-11 04:43:15.236 [ INFO] Progress -   3.3 MB (80.4%)\n2023-07-11 04:43:15.236 [ INFO] Progress -   3.3 MB (82.4%)\n2023-07-11 04:43:15.236 [ INFO] Progress -   3.3 MB (84.4%)\n2023-07-11 04:43:15.236 [ INFO] Progress -   3.3 MB (86.4%)\n2023-07-11 04:43:15.237 [ INFO] Progress -   3.3 MB (88.4%)\n2023-07-11 04:43:15.237 [ INFO] Progress -   3.3 MB (90.4%)\n2023-07-11 04:43:15.237 [ INFO] Progress -   3.3 MB (92.5%)\n2023-07-11 04:43:15.237 [ INFO] Progress -   3.3 MB (94.5%)\n2023-07-11 04:43:15.238 [ INFO] Progress -   3.3 MB (96.5%)\n2023-07-11 04:43:15.239 [ INFO] Progress -   3.3 MB (98.5%)\n2023-07-11 04:43:15.239 [ INFO] Progress -   3.3 MB (100.0%)\n2023-07-11 04:43:15.239 [ INFO] Processing  time : 0:00:27.384750\n2023-07-11 04:43:15.239 [ INFO] Downloading time : 0:00:01.382641\n2023-07-11 04:43:15.239 [ INFO] Total time       : 0:00:28.767391\n2023-07-11 04:43:15.239 [ INFO] Download rate    : 1.5 MB/s\n2023-07-11 04:43:15.239 [ INFO] Save into        : motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200108.nc\n2023-07-11 04:43:15.269 [ INFO] Done\n2023-07-11 04:43:15.513 [ INFO] Asynchronous mode set\n2023-07-11 04:43:15.513 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:43:22.274 [ INFO] Requesting file to download (this can take a while)...\n2023-07-11 04:43:29.643 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:43:47.109 [ INFO] The product is ready for download\n2023-07-11 04:43:47.109 [ INFO] Downloading file (this can take a while)...\n2023-07-11 04:43:47.846 [ INFO] File type: application/x-netcdf\n2023-07-11 04:43:47.846 [ INFO] File size: 3.2 MB (3155704 B)\n2023-07-11 04:43:47.847 [ INFO] Downloading file /home/jovyan/shared/minh-notebooks/motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200109.nc\n2023-07-11 04:43:48.190 [ INFO] Progress -   3.2 MB (2.1%)\n2023-07-11 04:43:48.363 [ INFO] Progress -   3.2 MB (4.2%)\n2023-07-11 04:43:48.535 [ INFO] Progress -   3.2 MB (6.2%)\n2023-07-11 04:43:48.536 [ INFO] Progress -   3.2 MB (8.3%)\n2023-07-11 04:43:48.708 [ INFO] Progress -   3.2 MB (10.4%)\n2023-07-11 04:43:48.708 [ INFO] Progress -   3.2 MB (12.5%)\n2023-07-11 04:43:48.708 [ INFO] Progress -   3.2 MB (14.5%)\n2023-07-11 04:43:48.709 [ INFO] Progress -   3.2 MB (16.6%)\n2023-07-11 04:43:48.709 [ INFO] Progress -   3.2 MB (18.7%)\n2023-07-11 04:43:48.880 [ INFO] Progress -   3.2 MB (20.8%)\n2023-07-11 04:43:48.880 [ INFO] Progress -   3.2 MB (22.8%)\n2023-07-11 04:43:48.880 [ INFO] Progress -   3.2 MB (24.9%)\n2023-07-11 04:43:48.881 [ INFO] Progress -   3.2 MB (27.0%)\n2023-07-11 04:43:48.881 [ INFO] Progress -   3.2 MB (29.1%)\n2023-07-11 04:43:48.881 [ INFO] Progress -   3.2 MB (31.2%)\n2023-07-11 04:43:48.881 [ INFO] Progress -   3.2 MB (33.2%)\n2023-07-11 04:43:48.882 [ INFO] Progress -   3.2 MB (35.3%)\n2023-07-11 04:43:48.883 [ INFO] Progress -   3.2 MB (37.4%)\n2023-07-11 04:43:49.052 [ INFO] Progress -   3.2 MB (39.5%)\n2023-07-11 04:43:49.052 [ INFO] Progress -   3.2 MB (41.5%)\n2023-07-11 04:43:49.053 [ INFO] Progress -   3.2 MB (43.6%)\n2023-07-11 04:43:49.053 [ INFO] Progress -   3.2 MB (45.7%)\n2023-07-11 04:43:49.053 [ INFO] Progress -   3.2 MB (47.8%)\n2023-07-11 04:43:49.053 [ INFO] Progress -   3.2 MB (49.8%)\n2023-07-11 04:43:49.054 [ INFO] Progress -   3.2 MB (51.9%)\n2023-07-11 04:43:49.054 [ INFO] Progress -   3.2 MB (54.0%)\n2023-07-11 04:43:49.054 [ INFO] Progress -   3.2 MB (56.1%)\n2023-07-11 04:43:49.055 [ INFO] Progress -   3.2 MB (58.1%)\n2023-07-11 04:43:49.055 [ INFO] Progress -   3.2 MB (60.2%)\n2023-07-11 04:43:49.055 [ INFO] Progress -   3.2 MB (62.3%)\n2023-07-11 04:43:49.055 [ INFO] Progress -   3.2 MB (64.4%)\n2023-07-11 04:43:49.056 [ INFO] Progress -   3.2 MB (66.5%)\n2023-07-11 04:43:49.056 [ INFO] Progress -   3.2 MB (68.5%)\n2023-07-11 04:43:49.057 [ INFO] Progress -   3.2 MB (70.6%)\n2023-07-11 04:43:49.057 [ INFO] Progress -   3.2 MB (72.7%)\n2023-07-11 04:43:49.057 [ INFO] Progress -   3.2 MB (74.8%)\n2023-07-11 04:43:49.057 [ INFO] Progress -   3.2 MB (76.8%)\n2023-07-11 04:43:49.225 [ INFO] Progress -   3.2 MB (78.9%)\n2023-07-11 04:43:49.225 [ INFO] Progress -   3.2 MB (81.0%)\n2023-07-11 04:43:49.225 [ INFO] Progress -   3.2 MB (83.1%)\n2023-07-11 04:43:49.226 [ INFO] Progress -   3.2 MB (85.1%)\n2023-07-11 04:43:49.226 [ INFO] Progress -   3.2 MB (87.2%)\n2023-07-11 04:43:49.226 [ INFO] Progress -   3.2 MB (89.3%)\n2023-07-11 04:43:49.226 [ INFO] Progress -   3.2 MB (91.4%)\n2023-07-11 04:43:49.227 [ INFO] Progress -   3.2 MB (93.5%)\n2023-07-11 04:43:49.227 [ INFO] Progress -   3.2 MB (95.5%)\n2023-07-11 04:43:49.227 [ INFO] Progress -   3.2 MB (97.6%)\n2023-07-11 04:43:49.227 [ INFO] Progress -   3.2 MB (99.7%)\n2023-07-11 04:43:49.228 [ INFO] Progress -   3.2 MB (100.0%)\n2023-07-11 04:43:49.228 [ INFO] Processing  time : 0:00:32.336252\n2023-07-11 04:43:49.228 [ INFO] Downloading time : 0:00:01.381585\n2023-07-11 04:43:49.228 [ INFO] Total time       : 0:00:33.717837\n2023-07-11 04:43:49.228 [ INFO] Download rate    : 1.5 MB/s\n2023-07-11 04:43:49.228 [ INFO] Save into        : motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200109.nc\n2023-07-11 04:43:49.283 [ INFO] Done\n2023-07-11 04:43:49.521 [ INFO] Asynchronous mode set\n2023-07-11 04:43:49.521 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:43:58.915 [ INFO] Requesting file to download (this can take a while)...\n2023-07-11 04:44:08.361 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:44:23.488 [ INFO] The product is ready for download\n2023-07-11 04:44:23.488 [ INFO] Downloading file (this can take a while)...\n2023-07-11 04:44:24.223 [ INFO] File type: application/x-netcdf\n2023-07-11 04:44:24.223 [ INFO] File size: 3.3 MB (3260684 B)\n2023-07-11 04:44:24.223 [ INFO] Downloading file /home/jovyan/shared/minh-notebooks/motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200110.nc\n2023-07-11 04:44:24.568 [ INFO] Progress -   3.3 MB (2.0%)\n2023-07-11 04:44:24.740 [ INFO] Progress -   3.3 MB (4.0%)\n2023-07-11 04:44:24.741 [ INFO] Progress -   3.3 MB (6.0%)\n2023-07-11 04:44:24.913 [ INFO] Progress -   3.3 MB (8.0%)\n2023-07-11 04:44:24.913 [ INFO] Progress -   3.3 MB (10.0%)\n2023-07-11 04:44:24.914 [ INFO] Progress -   3.3 MB (12.1%)\n2023-07-11 04:44:25.084 [ INFO] Progress -   3.3 MB (14.1%)\n2023-07-11 04:44:25.085 [ INFO] Progress -   3.3 MB (16.1%)\n2023-07-11 04:44:25.086 [ INFO] Progress -   3.3 MB (18.1%)\n2023-07-11 04:44:25.086 [ INFO] Progress -   3.3 MB (20.1%)\n2023-07-11 04:44:25.086 [ INFO] Progress -   3.3 MB (22.1%)\n2023-07-11 04:44:25.087 [ INFO] Progress -   3.3 MB (24.1%)\n2023-07-11 04:44:25.087 [ INFO] Progress -   3.3 MB (26.1%)\n2023-07-11 04:44:25.257 [ INFO] Progress -   3.3 MB (28.1%)\n2023-07-11 04:44:25.257 [ INFO] Progress -   3.3 MB (30.1%)\n2023-07-11 04:44:25.258 [ INFO] Progress -   3.3 MB (32.2%)\n2023-07-11 04:44:25.259 [ INFO] Progress -   3.3 MB (34.2%)\n2023-07-11 04:44:25.259 [ INFO] Progress -   3.3 MB (36.2%)\n2023-07-11 04:44:25.260 [ INFO] Progress -   3.3 MB (38.2%)\n2023-07-11 04:44:25.260 [ INFO] Progress -   3.3 MB (40.2%)\n2023-07-11 04:44:25.260 [ INFO] Progress -   3.3 MB (42.2%)\n2023-07-11 04:44:25.260 [ INFO] Progress -   3.3 MB (44.2%)\n2023-07-11 04:44:25.261 [ INFO] Progress -   3.3 MB (46.2%)\n2023-07-11 04:44:25.261 [ INFO] Progress -   3.3 MB (48.2%)\n2023-07-11 04:44:25.261 [ INFO] Progress -   3.3 MB (50.2%)\n2023-07-11 04:44:25.261 [ INFO] Progress -   3.3 MB (52.3%)\n2023-07-11 04:44:25.429 [ INFO] Progress -   3.3 MB (54.3%)\n2023-07-11 04:44:25.429 [ INFO] Progress -   3.3 MB (56.3%)\n2023-07-11 04:44:25.430 [ INFO] Progress -   3.3 MB (58.3%)\n2023-07-11 04:44:25.430 [ INFO] Progress -   3.3 MB (60.3%)\n2023-07-11 04:44:25.430 [ INFO] Progress -   3.3 MB (62.3%)\n2023-07-11 04:44:25.431 [ INFO] Progress -   3.3 MB (64.3%)\n2023-07-11 04:44:25.432 [ INFO] Progress -   3.3 MB (66.3%)\n2023-07-11 04:44:25.432 [ INFO] Progress -   3.3 MB (68.3%)\n2023-07-11 04:44:25.432 [ INFO] Progress -   3.3 MB (70.3%)\n2023-07-11 04:44:25.432 [ INFO] Progress -   3.3 MB (72.4%)\n2023-07-11 04:44:25.433 [ INFO] Progress -   3.3 MB (74.4%)\n2023-07-11 04:44:25.433 [ INFO] Progress -   3.3 MB (76.4%)\n2023-07-11 04:44:25.433 [ INFO] Progress -   3.3 MB (78.4%)\n2023-07-11 04:44:25.433 [ INFO] Progress -   3.3 MB (80.4%)\n2023-07-11 04:44:25.434 [ INFO] Progress -   3.3 MB (82.4%)\n2023-07-11 04:44:25.434 [ INFO] Progress -   3.3 MB (84.4%)\n2023-07-11 04:44:25.434 [ INFO] Progress -   3.3 MB (86.4%)\n2023-07-11 04:44:25.435 [ INFO] Progress -   3.3 MB (88.4%)\n2023-07-11 04:44:25.435 [ INFO] Progress -   3.3 MB (90.4%)\n2023-07-11 04:44:25.435 [ INFO] Progress -   3.3 MB (92.5%)\n2023-07-11 04:44:25.435 [ INFO] Progress -   3.3 MB (94.5%)\n2023-07-11 04:44:25.436 [ INFO] Progress -   3.3 MB (96.5%)\n2023-07-11 04:44:25.436 [ INFO] Progress -   3.3 MB (98.5%)\n2023-07-11 04:44:25.437 [ INFO] Progress -   3.3 MB (100.0%)\n2023-07-11 04:44:25.437 [ INFO] Processing  time : 0:00:34.702582\n2023-07-11 04:44:25.437 [ INFO] Downloading time : 0:00:01.213662\n2023-07-11 04:44:25.437 [ INFO] Total time       : 0:00:35.916244\n2023-07-11 04:44:25.437 [ INFO] Download rate    : 1.7 MB/s\n2023-07-11 04:44:25.437 [ INFO] Save into        : motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200110.nc\n2023-07-11 04:44:25.467 [ INFO] Done\n2023-07-11 04:44:25.708 [ INFO] Asynchronous mode set\n2023-07-11 04:44:25.709 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:44:33.978 [ INFO] Requesting file to download (this can take a while)...\n2023-07-11 04:44:41.900 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:44:56.540 [ INFO] The product is ready for download\n2023-07-11 04:44:56.540 [ INFO] Downloading file (this can take a while)...\n2023-07-11 04:44:57.296 [ INFO] File type: application/x-netcdf\n2023-07-11 04:44:57.297 [ INFO] File size: 3.2 MB (3155704 B)\n2023-07-11 04:44:57.297 [ INFO] Downloading file /home/jovyan/shared/minh-notebooks/motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200111.nc\n2023-07-11 04:44:57.640 [ INFO] Progress -   3.2 MB (2.1%)\n2023-07-11 04:44:57.813 [ INFO] Progress -   3.2 MB (4.2%)\n2023-07-11 04:44:57.985 [ INFO] Progress -   3.2 MB (6.2%)\n2023-07-11 04:44:57.986 [ INFO] Progress -   3.2 MB (8.3%)\n2023-07-11 04:44:58.157 [ INFO] Progress -   3.2 MB (10.4%)\n2023-07-11 04:44:58.158 [ INFO] Progress -   3.2 MB (12.5%)\n2023-07-11 04:44:58.158 [ INFO] Progress -   3.2 MB (14.5%)\n2023-07-11 04:44:58.158 [ INFO] Progress -   3.2 MB (16.6%)\n2023-07-11 04:44:58.159 [ INFO] Progress -   3.2 MB (18.7%)\n2023-07-11 04:44:58.330 [ INFO] Progress -   3.2 MB (20.8%)\n2023-07-11 04:44:58.331 [ INFO] Progress -   3.2 MB (22.8%)\n2023-07-11 04:44:58.331 [ INFO] Progress -   3.2 MB (24.9%)\n2023-07-11 04:44:58.331 [ INFO] Progress -   3.2 MB (27.0%)\n2023-07-11 04:44:58.332 [ INFO] Progress -   3.2 MB (29.1%)\n2023-07-11 04:44:58.332 [ INFO] Progress -   3.2 MB (31.2%)\n2023-07-11 04:44:58.332 [ INFO] Progress -   3.2 MB (33.2%)\n2023-07-11 04:44:58.333 [ INFO] Progress -   3.2 MB (35.3%)\n2023-07-11 04:44:58.333 [ INFO] Progress -   3.2 MB (37.4%)\n2023-07-11 04:44:58.333 [ INFO] Progress -   3.2 MB (39.5%)\n2023-07-11 04:44:58.502 [ INFO] Progress -   3.2 MB (41.5%)\n2023-07-11 04:44:58.503 [ INFO] Progress -   3.2 MB (43.6%)\n2023-07-11 04:44:58.503 [ INFO] Progress -   3.2 MB (45.7%)\n2023-07-11 04:44:58.503 [ INFO] Progress -   3.2 MB (47.8%)\n2023-07-11 04:44:58.504 [ INFO] Progress -   3.2 MB (49.8%)\n2023-07-11 04:44:58.504 [ INFO] Progress -   3.2 MB (51.9%)\n2023-07-11 04:44:58.504 [ INFO] Progress -   3.2 MB (54.0%)\n2023-07-11 04:44:58.505 [ INFO] Progress -   3.2 MB (56.1%)\n2023-07-11 04:44:58.505 [ INFO] Progress -   3.2 MB (58.1%)\n2023-07-11 04:44:58.505 [ INFO] Progress -   3.2 MB (60.2%)\n2023-07-11 04:44:58.505 [ INFO] Progress -   3.2 MB (62.3%)\n2023-07-11 04:44:58.506 [ INFO] Progress -   3.2 MB (64.4%)\n2023-07-11 04:44:58.506 [ INFO] Progress -   3.2 MB (66.5%)\n2023-07-11 04:44:58.507 [ INFO] Progress -   3.2 MB (68.5%)\n2023-07-11 04:44:58.507 [ INFO] Progress -   3.2 MB (70.6%)\n2023-07-11 04:44:58.507 [ INFO] Progress -   3.2 MB (72.7%)\n2023-07-11 04:44:58.507 [ INFO] Progress -   3.2 MB (74.8%)\n2023-07-11 04:44:58.508 [ INFO] Progress -   3.2 MB (76.8%)\n2023-07-11 04:44:58.508 [ INFO] Progress -   3.2 MB (78.9%)\n2023-07-11 04:44:58.508 [ INFO] Progress -   3.2 MB (81.0%)\n2023-07-11 04:44:58.675 [ INFO] Progress -   3.2 MB (83.1%)\n2023-07-11 04:44:58.675 [ INFO] Progress -   3.2 MB (85.1%)\n2023-07-11 04:44:58.676 [ INFO] Progress -   3.2 MB (87.2%)\n2023-07-11 04:44:58.676 [ INFO] Progress -   3.2 MB (89.3%)\n2023-07-11 04:44:58.676 [ INFO] Progress -   3.2 MB (91.4%)\n2023-07-11 04:44:58.676 [ INFO] Progress -   3.2 MB (93.5%)\n2023-07-11 04:44:58.677 [ INFO] Progress -   3.2 MB (95.5%)\n2023-07-11 04:44:58.677 [ INFO] Progress -   3.2 MB (97.6%)\n2023-07-11 04:44:58.677 [ INFO] Progress -   3.2 MB (99.7%)\n2023-07-11 04:44:58.678 [ INFO] Progress -   3.2 MB (100.0%)\n2023-07-11 04:44:58.678 [ INFO] Processing  time : 0:00:31.590883\n2023-07-11 04:44:58.678 [ INFO] Downloading time : 0:00:01.381213\n2023-07-11 04:44:58.678 [ INFO] Total time       : 0:00:32.972096\n2023-07-11 04:44:58.678 [ INFO] Download rate    : 1.5 MB/s\n2023-07-11 04:44:58.678 [ INFO] Save into        : motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200111.nc\n2023-07-11 04:44:58.710 [ INFO] Done\n2023-07-11 04:44:58.961 [ INFO] Asynchronous mode set\n2023-07-11 04:44:58.961 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:45:04.292 [ INFO] Requesting file to download (this can take a while)...\n2023-07-11 04:45:11.513 [ INFO] Authenticating user mphan for service https://my.cmems-du.eu/motu-web/Motu\n2023-07-11 04:45:26.798 [ INFO] The product is ready for download\n2023-07-11 04:45:26.798 [ INFO] Downloading file (this can take a while)...\n2023-07-11 04:45:27.551 [ INFO] File type: application/x-netcdf\n2023-07-11 04:45:27.551 [ INFO] File size: 3.3 MB (3260684 B)\n2023-07-11 04:45:27.551 [ INFO] Downloading file /home/jovyan/shared/minh-notebooks/motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200112.nc\n2023-07-11 04:45:27.895 [ INFO] Progress -   3.3 MB (2.0%)\n2023-07-11 04:45:28.067 [ INFO] Progress -   3.3 MB (4.0%)\n2023-07-11 04:45:28.069 [ INFO] Progress -   3.3 MB (6.0%)\n2023-07-11 04:45:28.240 [ INFO] Progress -   3.3 MB (8.0%)\n2023-07-11 04:45:28.240 [ INFO] Progress -   3.3 MB (10.0%)\n2023-07-11 04:45:28.241 [ INFO] Progress -   3.3 MB (12.1%)\n2023-07-11 04:45:28.412 [ INFO] Progress -   3.3 MB (14.1%)\n2023-07-11 04:45:28.413 [ INFO] Progress -   3.3 MB (16.1%)\n2023-07-11 04:45:28.413 [ INFO] Progress -   3.3 MB (18.1%)\n2023-07-11 04:45:28.413 [ INFO] Progress -   3.3 MB (20.1%)\n2023-07-11 04:45:28.414 [ INFO] Progress -   3.3 MB (22.1%)\n2023-07-11 04:45:28.414 [ INFO] Progress -   3.3 MB (24.1%)\n2023-07-11 04:45:28.414 [ INFO] Progress -   3.3 MB (26.1%)\n2023-07-11 04:45:28.585 [ INFO] Progress -   3.3 MB (28.1%)\n2023-07-11 04:45:28.585 [ INFO] Progress -   3.3 MB (30.1%)\n2023-07-11 04:45:28.586 [ INFO] Progress -   3.3 MB (32.2%)\n2023-07-11 04:45:28.587 [ INFO] Progress -   3.3 MB (34.2%)\n2023-07-11 04:45:28.587 [ INFO] Progress -   3.3 MB (36.2%)\n2023-07-11 04:45:28.587 [ INFO] Progress -   3.3 MB (38.2%)\n2023-07-11 04:45:28.587 [ INFO] Progress -   3.3 MB (40.2%)\n2023-07-11 04:45:28.588 [ INFO] Progress -   3.3 MB (42.2%)\n2023-07-11 04:45:28.588 [ INFO] Progress -   3.3 MB (44.2%)\n2023-07-11 04:45:28.588 [ INFO] Progress -   3.3 MB (46.2%)\n2023-07-11 04:45:28.588 [ INFO] Progress -   3.3 MB (48.2%)\n2023-07-11 04:45:28.588 [ INFO] Progress -   3.3 MB (50.2%)\n2023-07-11 04:45:28.589 [ INFO] Progress -   3.3 MB (52.3%)\n2023-07-11 04:45:28.589 [ INFO] Progress -   3.3 MB (54.3%)\n2023-07-11 04:45:28.757 [ INFO] Progress -   3.3 MB (56.3%)\n2023-07-11 04:45:28.758 [ INFO] Progress -   3.3 MB (58.3%)\n2023-07-11 04:45:28.758 [ INFO] Progress -   3.3 MB (60.3%)\n2023-07-11 04:45:28.758 [ INFO] Progress -   3.3 MB (62.3%)\n2023-07-11 04:45:28.758 [ INFO] Progress -   3.3 MB (64.3%)\n2023-07-11 04:45:28.759 [ INFO] Progress -   3.3 MB (66.3%)\n2023-07-11 04:45:28.759 [ INFO] Progress -   3.3 MB (68.3%)\n2023-07-11 04:45:28.760 [ INFO] Progress -   3.3 MB (70.3%)\n2023-07-11 04:45:28.760 [ INFO] Progress -   3.3 MB (72.4%)\n2023-07-11 04:45:28.760 [ INFO] Progress -   3.3 MB (74.4%)\n2023-07-11 04:45:28.760 [ INFO] Progress -   3.3 MB (76.4%)\n2023-07-11 04:45:28.761 [ INFO] Progress -   3.3 MB (78.4%)\n2023-07-11 04:45:28.761 [ INFO] Progress -   3.3 MB (80.4%)\n2023-07-11 04:45:28.761 [ INFO] Progress -   3.3 MB (82.4%)\n2023-07-11 04:45:28.761 [ INFO] Progress -   3.3 MB (84.4%)\n2023-07-11 04:45:28.762 [ INFO] Progress -   3.3 MB (86.4%)\n2023-07-11 04:45:28.762 [ INFO] Progress -   3.3 MB (88.4%)\n2023-07-11 04:45:28.762 [ INFO] Progress -   3.3 MB (90.4%)\n2023-07-11 04:45:28.762 [ INFO] Progress -   3.3 MB (92.5%)\n2023-07-11 04:45:28.762 [ INFO] Progress -   3.3 MB (94.5%)\n2023-07-11 04:45:28.763 [ INFO] Progress -   3.3 MB (96.5%)\n2023-07-11 04:45:28.763 [ INFO] Progress -   3.3 MB (98.5%)\n2023-07-11 04:45:28.764 [ INFO] Progress -   3.3 MB (100.0%)\n2023-07-11 04:45:28.764 [ INFO] Processing  time : 0:00:28.592514\n2023-07-11 04:45:28.764 [ INFO] Downloading time : 0:00:01.212567\n2023-07-11 04:45:28.764 [ INFO] Total time       : 0:00:29.805081\n2023-07-11 04:45:28.764 [ INFO] Download rate    : 1.7 MB/s\n2023-07-11 04:45:28.764 [ INFO] Save into        : motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D/200112.nc\n2023-07-11 04:45:28.818 [ INFO] Done\n\n\nDISCLAIMER: Downloading time may be extremely slow due to processing time from Motu for each request. We can increase temporal range for each one (6 months or 1 year, for example) but it may exceed the size limit MOTU allows. Feel free to modify the prompt generation as it fits to balance speed and size.\n\n\n\nimage.png"
  },
  {
    "objectID": "python-notebooks/01_MOTU_Downloader.html#combine-data",
    "href": "python-notebooks/01_MOTU_Downloader.html#combine-data",
    "title": "Download sliced data using MOTU-Client in Copernicus",
    "section": "Combine data",
    "text": "Combine data\n\nds = xr.open_mfdataset('demonstrated data/motu/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D//*.nc')\n\n\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:    (time: 731, latitude: 81, longitude: 81)\nCoordinates:\n  * latitude   (latitude) float32 4.875 5.125 5.375 5.625 ... 24.38 24.62 24.88\n  * time       (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2001-12-31\n  * longitude  (longitude) float32 59.88 60.12 60.38 60.62 ... 79.38 79.62 79.88\nData variables:\n    adt        (time, latitude, longitude) float64 dask.array&lt;chunksize=(31, 81, 81), meta=np.ndarray&gt;\n    crs        (time) int32 -2147483647 -2147483647 ... -2147483647 -2147483647\n    sla        (time, latitude, longitude) float64 dask.array&lt;chunksize=(31, 81, 81), meta=np.ndarray&gt;\nAttributes: (12/45)\n    Conventions:                                    CF-1.6\n    FROM_ORIGINAL_FILE__Metadata_Conventions:       Unidata Dataset Discovery...\n    cdm_data_type:                                  Grid\n    comment:                                        Sea Surface Height measur...\n    contact:                                        servicedesk.cmems@mercato...\n    creator_email:                                  servicedesk.cmems@mercato...\n    ...                                             ...\n    time_coverage_duration:                         P1D\n    time_coverage_end:                              2022-08-04T12:00:00Z\n    time_coverage_resolution:                       P1D\n    time_coverage_start:                            2022-08-03T12:00:00Z\n    title:                                          DT merged all satellites ...\n    _CoordSysBuilder:                               ucar.nc2.dataset.conv.CF1...xarray.DatasetDimensions:time: 731latitude: 81longitude: 81Coordinates: (3)latitude(latitude)float324.875 5.125 5.375 ... 24.62 24.88axis :Ybounds :lat_bndslong_name :Latitudestandard_name :latitudeunits :degrees_northvalid_max :24.875valid_min :4.875_ChunkSizes :50_CoordinateAxisType :Latarray([ 4.875,  5.125,  5.375,  5.625,  5.875,  6.125,  6.375,  6.625,  6.875,\n        7.125,  7.375,  7.625,  7.875,  8.125,  8.375,  8.625,  8.875,  9.125,\n        9.375,  9.625,  9.875, 10.125, 10.375, 10.625, 10.875, 11.125, 11.375,\n       11.625, 11.875, 12.125, 12.375, 12.625, 12.875, 13.125, 13.375, 13.625,\n       13.875, 14.125, 14.375, 14.625, 14.875, 15.125, 15.375, 15.625, 15.875,\n       16.125, 16.375, 16.625, 16.875, 17.125, 17.375, 17.625, 17.875, 18.125,\n       18.375, 18.625, 18.875, 19.125, 19.375, 19.625, 19.875, 20.125, 20.375,\n       20.625, 20.875, 21.125, 21.375, 21.625, 21.875, 22.125, 22.375, 22.625,\n       22.875, 23.125, 23.375, 23.625, 23.875, 24.125, 24.375, 24.625, 24.875],\n      dtype=float32)time(time)datetime64[ns]2000-01-01 ... 2001-12-31axis :Tlong_name :Timestandard_name :time_ChunkSizes :1_CoordinateAxisType :Timevalid_min :18262.0valid_max :18292.0array(['2000-01-01T00:00:00.000000000', '2000-01-02T00:00:00.000000000',\n       '2000-01-03T00:00:00.000000000', ..., '2001-12-29T00:00:00.000000000',\n       '2001-12-30T00:00:00.000000000', '2001-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')longitude(longitude)float3259.88 60.12 60.38 ... 79.62 79.88axis :Xbounds :lon_bndslong_name :Longitudestandard_name :longitudeunits :degrees_eastvalid_max :79.875valid_min :59.875_ChunkSizes :50_CoordinateAxisType :Lonarray([59.875, 60.125, 60.375, 60.625, 60.875, 61.125, 61.375, 61.625, 61.875,\n       62.125, 62.375, 62.625, 62.875, 63.125, 63.375, 63.625, 63.875, 64.125,\n       64.375, 64.625, 64.875, 65.125, 65.375, 65.625, 65.875, 66.125, 66.375,\n       66.625, 66.875, 67.125, 67.375, 67.625, 67.875, 68.125, 68.375, 68.625,\n       68.875, 69.125, 69.375, 69.625, 69.875, 70.125, 70.375, 70.625, 70.875,\n       71.125, 71.375, 71.625, 71.875, 72.125, 72.375, 72.625, 72.875, 73.125,\n       73.375, 73.625, 73.875, 74.125, 74.375, 74.625, 74.875, 75.125, 75.375,\n       75.625, 75.875, 76.125, 76.375, 76.625, 76.875, 77.125, 77.375, 77.625,\n       77.875, 78.125, 78.375, 78.625, 78.875, 79.125, 79.375, 79.625, 79.875],\n      dtype=float32)Data variables: (3)adt(time, latitude, longitude)float64dask.array&lt;chunksize=(31, 81, 81), meta=np.ndarray&gt;comment :The absolute dynamic topography is the sea surface height above geoid; the adt is obtained as follows: adt=sla+mdt where mdt is the mean dynamic topography; see the product user manual for detailsgrid_mapping :crslong_name :Absolute dynamic topographystandard_name :sea_surface_height_above_geoidunits :m_ChunkSizes :[ 1 50 50]\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n36.59 MiB\n1.55 MiB\n\n\nShape\n(731, 81, 81)\n(31, 81, 81)\n\n\nDask graph\n24 chunks in 49 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n\n\n\n\ncrs\n\n\n(time)\n\n\nint32\n\n\n-2147483647 ... -2147483647\n\n\n\n\ncomment :\n\nThis is a container variable that describes the grid_mapping used by the data in this file. This variable does not contain any data; only information about the geographic coordinate system.\n\ninverse_flattening :\n\n298.257\n\ngrid_mapping_name :\n\nlatitude_longitude\n\nsemi_major_axis :\n\n6378136.3\n\n_CoordinateTransformType :\n\nProjection\n\n_CoordinateAxisTypes :\n\nGeoX GeoY\n\n\n\n\narray([-2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n...\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647, -2147483647, -2147483647, -2147483647, -2147483647,\n       -2147483647], dtype=int32)\n\n\nsla\n\n\n(time, latitude, longitude)\n\n\nfloat64\n\n\ndask.array&lt;chunksize=(31, 81, 81), meta=np.ndarray&gt;\n\n\n\n\nancillary_variables :\n\nerr_sla\n\ncomment :\n\nThe sea level anomaly is the sea surface height above mean sea surface; it is referenced to the [1993, 2012] period; see the product user manual for details\n\ngrid_mapping :\n\ncrs\n\nlong_name :\n\nSea level anomaly\n\nstandard_name :\n\nsea_surface_height_above_sea_level\n\nunits :\n\nm\n\n_ChunkSizes :\n\n[ 1 50 50]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n36.59 MiB\n1.55 MiB\n\n\nShape\n(731, 81, 81)\n(31, 81, 81)\n\n\nDask graph\n24 chunks in 49 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n\n\n\n\n\nAttributes: (45)Conventions :CF-1.6FROM_ORIGINAL_FILE__Metadata_Conventions :Unidata Dataset Discovery v1.0cdm_data_type :Gridcomment :Sea Surface Height measured by Altimetry and derived variablescontact :servicedesk.cmems@mercator-ocean.eucreator_email :servicedesk.cmems@mercator-ocean.eucreator_name :CMEMS - Sea Level Thematic Assembly Centercreator_url :http://marine.copernicus.eudate_created :2023-03-01T17:56:26Zdate_issued :2023-03-01T17:56:26Zdate_modified :2023-03-01T17:56:26ZFROM_ORIGINAL_FILE__geospatial_lat_max :89.875FROM_ORIGINAL_FILE__geospatial_lat_min :-89.875FROM_ORIGINAL_FILE__geospatial_lat_resolution :0.25FROM_ORIGINAL_FILE__geospatial_lat_units :degrees_northFROM_ORIGINAL_FILE__geospatial_lon_max :179.875FROM_ORIGINAL_FILE__geospatial_lon_min :-179.875FROM_ORIGINAL_FILE__geospatial_lon_resolution :0.25FROM_ORIGINAL_FILE__geospatial_lon_units :degrees_eastgeospatial_vertical_max :0.0geospatial_vertical_min :0.0geospatial_vertical_positive :downgeospatial_vertical_resolution :pointgeospatial_vertical_units :mhistory :2023-03-01 17:56:27Z: Creationinstitution :CLS, CNESkeywords :Oceans &gt; Ocean Topography &gt; Sea Surface Heightkeywords_vocabulary :NetCDF COARDS Climate and Forecast Standard Nameslicense :http://marine.copernicus.eu/web/27-service-commitments-and-licence.phpFROM_ORIGINAL_FILE__platform :Altika Drifting Phase, Cryosat-2 New Orbit, Haiyang-2B, Jason-3 Interleaved, Sentinel-3A, Sentinel-3B, Sentinel-6Aprocessing_level :L4FROM_ORIGINAL_FILE__product_version :vDec2021project :COPERNICUS MARINE ENVIRONMENT MONITORING SERVICE (CMEMS)references :http://marine.copernicus.euFROM_ORIGINAL_FILE__software_version :7.1_DUACS_DT2021_baselinesource :Altimetry measurementsssalto_duacs_comment :The reference mission used for the altimeter inter-calibration processing is Topex/Poseidon between 1993-01-01 and 2002-04-23, Jason-1 between 2002-04-24 and 2008-10-18, OSTM/Jason-2 between 2008-10-19 and 2016-06-25, Jason-3 between 2016-06-25 and 2022-02-09, and Sentinel-6A from 2022-02-10.standard_name_vocabulary :NetCDF Climate and Forecast (CF) Metadata Convention Standard Name Table v37summary :SSALTO/DUACS Delayed-Time Level-4 sea surface height and derived variables measured by multi-satellite altimetry observations over Global Ocean.time_coverage_duration :P1Dtime_coverage_end :2022-08-04T12:00:00Ztime_coverage_resolution :P1Dtime_coverage_start :2022-08-03T12:00:00Ztitle :DT merged all satellites Global Ocean Gridded SSALTO/DUACS Sea Surface Height L4 product and derived variables_CoordSysBuilder :ucar.nc2.dataset.conv.CF1Convention"
  },
  {
    "objectID": "python-notebooks/01_OPeNDAP_Downloader.html#import-necessary-libraries",
    "href": "python-notebooks/01_OPeNDAP_Downloader.html#import-necessary-libraries",
    "title": "Download Data using OPeNDAP",
    "section": "Import necessary libraries",
    "text": "Import necessary libraries\n\nimport xarray as xr\nimport getpass\nfrom pydap.client import open_url\nfrom pydap.cas.get_cookies import setup_session\n\n\nUSERNAME = 'mphan'\nPASSWORD = getpass.getpass('Enter your password: ')\n\nEnter your password:  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n\n\nFor this notebook we want to stream data from the Copernicus Marine Environment Monitoring Service‚Äôs Global Ocean Physics Analysis\n\n# change your Dataset ID accordingly\nDATASET_ID = 'cmems_mod_glo_phy_my_0.083_P1D-m'\n\nIn the function below, we utilize a PydapDataStore, an Xarray store object used for accessing OpenDAP datasets. For this dataset, you need to log in your credentials using the name and password provided above. Note that not all OpenDAP datasets will require the same steps, so you should look up appropriate methods to access data.\n\ndef copernicusmarine_datastore(dataset, username, password):\n    cas_url = 'https://cmems-cas.cls.fr/cas/login'\n    session = setup_session(cas_url, username, password)\n    session.cookies.set(\"CASTGC\", session.cookies.get_dict()['CASTGC'])\n    database = ['my', 'nrt']\n    url = f'https://{database[0]}.cmems-du.eu/thredds/dodsC/{dataset}'\n    try:\n        data_store = xr.backends.PydapDataStore(open_url(url, session=session, user_charset='utf-8')) # needs PyDAP &gt;= v3.3.0 see https://github.com/pydap/pydap/pull/223/commits \n    except:\n        url = f'https://{database[1]}.cmems-du.eu/thredds/dodsC/{dataset}'\n        data_store = xr.backends.PydapDataStore(open_url(url, session=session, user_charset='utf-8')) # needs PyDAP &gt;= v3.3.0 see https://github.com/pydap/pydap/pull/223/commits\n    return data_store\n\n\ndata_store = copernicusmarine_datastore(DATASET_ID, USERNAME, PASSWORD)\n\nWhen you open a dataset from PyDAP‚Äôs Data Store object, only the ‚Äúshell‚Äù of the dataset is streamed into our local machine. The rest of the data shall be remote, so if we want to process further outside of slicing/inspecting, we need to stream the additional data, which occupies lots of memory. The streaming/downloading speed is also very slow, so proceed with caution.\n\nDS = xr.open_dataset(data_store)\n\n\nDS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:    (longitude: 4320, latitude: 2041, depth: 50, time: 10227)\nCoordinates:\n  * longitude  (longitude) float32 -180.0 -179.9 -179.8 ... 179.8 179.8 179.9\n  * latitude   (latitude) float32 -80.0 -79.92 -79.83 ... 89.83 89.92 90.0\n  * depth      (depth) float32 0.494 1.541 2.646 ... 5.275e+03 5.728e+03\n  * time       (time) datetime64[ns] 1993-01-01T12:00:00 ... 2020-12-31T12:00:00\nData variables:\n    mlotst     (time, latitude, longitude) float32 ...\n    zos        (time, latitude, longitude) float32 ...\n    bottomT    (time, latitude, longitude) float32 ...\n    sithick    (time, latitude, longitude) float32 ...\n    siconc     (time, latitude, longitude) float32 ...\n    usi        (time, latitude, longitude) float32 ...\n    vsi        (time, latitude, longitude) float32 ...\n    thetao     (time, depth, latitude, longitude) float32 ...\n    so         (time, depth, latitude, longitude) float32 ...\n    uo         (time, depth, latitude, longitude) float32 ...\n    vo         (time, depth, latitude, longitude) float32 ...\nAttributes: (12/24)\n    title:              daily mean fields from Global Ocean Physics Analysis ...\n    easting:            longitude\n    northing:           latitude\n    history:            2022/05/25 21:54:07 MERCATOR OCEAN Netcdf creation\n    source:             MERCATOR GLORYS12V1\n    institution:        MERCATOR OCEAN\n    ...                 ...\n    longitude_min:      -180.0\n    longitude_max:      179.91667\n    latitude_min:       -80.0\n    latitude_max:       90.0\n    z_min:              0.494025\n    z_max:              5727.917xarray.DatasetDimensions:longitude: 4320latitude: 2041depth: 50time: 10227Coordinates: (4)longitude(longitude)float32-180.0 -179.9 ... 179.8 179.9valid_min :-180.0valid_max :179.91667step :0.08332825units :degrees_eastunit_long :Degrees Eastlong_name :Longitudestandard_name :longitudeaxis :X_ChunkSizes :4320array([-180.     , -179.91667, -179.83333, ...,  179.75   ,  179.83333,\n        179.91667], dtype=float32)latitude(latitude)float32-80.0 -79.92 -79.83 ... 89.92 90.0valid_min :-80.0valid_max :90.0step :0.08333588units :degrees_northunit_long :Degrees Northlong_name :Latitudestandard_name :latitudeaxis :Y_ChunkSizes :2041array([-80.      , -79.916664, -79.833336, ...,  89.833336,  89.916664,\n        90.      ], dtype=float32)depth(depth)float320.494 1.541 ... 5.275e+03 5.728e+03valid_min :0.494025valid_max :5727.917units :mpositive :downunit_long :Meterslong_name :Depthstandard_name :depthaxis :Z_ChunkSizes :50array([4.940250e-01, 1.541375e+00, 2.645669e+00, 3.819495e+00, 5.078224e+00,\n       6.440614e+00, 7.929560e+00, 9.572997e+00, 1.140500e+01, 1.346714e+01,\n       1.581007e+01, 1.849556e+01, 2.159882e+01, 2.521141e+01, 2.944473e+01,\n       3.443415e+01, 4.034405e+01, 4.737369e+01, 5.576429e+01, 6.580727e+01,\n       7.785385e+01, 9.232607e+01, 1.097293e+02, 1.306660e+02, 1.558507e+02,\n       1.861256e+02, 2.224752e+02, 2.660403e+02, 3.181274e+02, 3.802130e+02,\n       4.539377e+02, 5.410889e+02, 6.435668e+02, 7.633331e+02, 9.023393e+02,\n       1.062440e+03, 1.245291e+03, 1.452251e+03, 1.684284e+03, 1.941893e+03,\n       2.225078e+03, 2.533336e+03, 2.865703e+03, 3.220820e+03, 3.597032e+03,\n       3.992484e+03, 4.405224e+03, 4.833291e+03, 5.274784e+03, 5.727917e+03],\n      dtype=float32)time(time)datetime64[ns]1993-01-01T12:00:00 ... 2020-12-...long_name :Time (hours since 1950-01-01)standard_name :timeaxis :T_ChunkSizes :1024array(['1993-01-01T12:00:00.000000000', '1993-01-02T12:00:00.000000000',\n       '1993-01-03T12:00:00.000000000', ..., '2020-12-29T12:00:00.000000000',\n       '2020-12-30T12:00:00.000000000', '2020-12-31T12:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (11)mlotst(time, latitude, longitude)float32...long_name :Density ocean mixed layer thicknessstandard_name :ocean_mixed_layer_thickness_defined_by_sigma_thetaunits :munit_long :Metersvalid_min :1valid_max :6447cell_methods :area: mean_ChunkSizes :[1, 681, 1440][90172686240 values with dtype=float32]zos(time, latitude, longitude)float32...long_name :Sea surface heightstandard_name :sea_surface_height_above_geoidunits :munit_long :Metersvalid_min :-6181valid_max :5039cell_methods :area: mean_ChunkSizes :[1, 681, 1440][90172686240 values with dtype=float32]bottomT(time, latitude, longitude)float32...long_name :Sea floor potential temperaturestandard_name :sea_water_potential_temperature_at_sea_floorunits :degrees_Cunit_long :Degrees Celsiusvalid_min :-32757valid_max :21318cell_methods :area: mean_ChunkSizes :[1, 681, 1440][90172686240 values with dtype=float32]sithick(time, latitude, longitude)float32...long_name :Sea ice thicknessstandard_name :sea_ice_thicknessunits :munit_long :Metersvalid_min :1valid_max :4618cell_methods :area: mean where sea_ice_ChunkSizes :[1, 681, 1440][90172686240 values with dtype=float32]siconc(time, latitude, longitude)float32...long_name :Ice concentrationstandard_name :sea_ice_area_fractionunits :1unit_long :Fractionvalid_min :1valid_max :28060cell_methods :area: mean where sea_ice_ChunkSizes :[1, 681, 1440][90172686240 values with dtype=float32]usi(time, latitude, longitude)float32...long_name :Sea ice eastward velocitystandard_name :eastward_sea_ice_velocityunits :m s-1unit_long :Meters per secondvalid_min :-32707valid_max :32144cell_methods :area: mean where sea_ice_ChunkSizes :[1, 681, 1440][90172686240 values with dtype=float32]vsi(time, latitude, longitude)float32...long_name :Sea ice northward velocitystandard_name :northward_sea_ice_velocityunits :m s-1unit_long :Meters per secondvalid_min :-31907valid_max :31690cell_methods :area: mean where sea_ice_ChunkSizes :[1, 681, 1440][90172686240 values with dtype=float32]thetao(time, depth, latitude, longitude)float32...long_name :Temperaturestandard_name :sea_water_potential_temperatureunits :degrees_Cunit_long :Degrees Celsiusvalid_min :-32764valid_max :21298cell_methods :area: mean_ChunkSizes :[1, 7, 341, 720][4508634312000 values with dtype=float32]so(time, depth, latitude, longitude)float32...long_name :Salinitystandard_name :sea_water_salinityunits :1e-3unit_long :Practical Salinity Unitvalid_min :1valid_max :28336cell_methods :area: mean_ChunkSizes :[1, 7, 341, 720][4508634312000 values with dtype=float32]uo(time, depth, latitude, longitude)float32...long_name :Eastward velocitystandard_name :eastward_sea_water_velocityunits :m s-1unit_long :Meters per secondvalid_min :-3573valid_max :3559cell_methods :area: mean_ChunkSizes :[1, 7, 341, 720][4508634312000 values with dtype=float32]vo(time, depth, latitude, longitude)float32...long_name :Northward velocitystandard_name :northward_sea_water_velocityunits :m s-1unit_long :Meters per secondvalid_min :-3300valid_max :3399cell_methods :area: mean_ChunkSizes :[1, 7, 341, 720][4508634312000 values with dtype=float32]Indexes: (4)longitudePandasIndexPandasIndex(Index([            -180.0, -179.9166717529297, -179.8333282470703,\n                  -179.75, -179.6666717529297, -179.5833282470703,\n                   -179.5, -179.4166717529297, -179.3333282470703,\n                  -179.25,\n       ...\n        179.1666717529297,             179.25,  179.3333282470703,\n        179.4166717529297,              179.5,  179.5833282470703,\n        179.6666717529297,             179.75,  179.8333282470703,\n        179.9166717529297],\n      dtype='float32', name='longitude', length=4320))latitudePandasIndexPandasIndex(Index([             -80.0, -79.91666412353516, -79.83333587646484,\n                   -79.75, -79.66666412353516, -79.58333587646484,\n                    -79.5, -79.41666412353516, -79.33333587646484,\n                   -79.25,\n       ...\n                    89.25,  89.33333587646484,  89.41666412353516,\n                     89.5,  89.58333587646484,  89.66666412353516,\n                    89.75,  89.83333587646484,  89.91666412353516,\n                     90.0],\n      dtype='float32', name='latitude', length=2041))depthPandasIndexPandasIndex(Index([0.49402499198913574,  1.5413750410079956,  2.6456689834594727,\n        3.8194949626922607,   5.078224182128906,   6.440614223480225,\n          7.92956018447876,   9.572997093200684,  11.404999732971191,\n        13.467140197753906,  15.810070037841797,  18.495559692382812,\n        21.598819732666016,  25.211410522460938,  29.444730758666992,\n         34.43415069580078,  40.344051361083984,   47.37369155883789,\n         55.76428985595703,   65.80726623535156,   77.85385131835938,\n          92.3260726928711,  109.72930145263672,  130.66600036621094,\n        155.85069274902344,  186.12559509277344,  222.47520446777344,\n         266.0403137207031,   318.1274108886719,   380.2130126953125,\n         453.9377136230469,   541.0889282226562,   643.5667724609375,\n         763.3331298828125,   902.3392944335938,    1062.43994140625,\n            1245.291015625,     1452.2509765625,  1684.2840576171875,\n        1941.8929443359375,   2225.077880859375,        2533.3359375,\n         2865.702880859375,   3220.820068359375,   3597.031982421875,\n          3992.48388671875,    4405.22412109375,      4833.291015625,\n           5274.7841796875,     5727.9169921875],\n      dtype='float32', name='depth'))timePandasIndexPandasIndex(DatetimeIndex(['1993-01-01 12:00:00', '1993-01-02 12:00:00',\n               '1993-01-03 12:00:00', '1993-01-04 12:00:00',\n               '1993-01-05 12:00:00', '1993-01-06 12:00:00',\n               '1993-01-07 12:00:00', '1993-01-08 12:00:00',\n               '1993-01-09 12:00:00', '1993-01-10 12:00:00',\n               ...\n               '2020-12-22 12:00:00', '2020-12-23 12:00:00',\n               '2020-12-24 12:00:00', '2020-12-25 12:00:00',\n               '2020-12-26 12:00:00', '2020-12-27 12:00:00',\n               '2020-12-28 12:00:00', '2020-12-29 12:00:00',\n               '2020-12-30 12:00:00', '2020-12-31 12:00:00'],\n              dtype='datetime64[ns]', name='time', length=10227, freq=None))Attributes: (24)title :daily mean fields from Global Ocean Physics Analysis and Forecast updated Dailyeasting :longitudenorthing :latitudehistory :2022/05/25 21:54:07 MERCATOR OCEAN Netcdf creationsource :MERCATOR GLORYS12V1institution :MERCATOR OCEANreferences :http://www.mercator-ocean.frcomment :CMEMS productConventions :CF-1.4domain_name :GL12field_type :meanfield_date :2020-12-31 00:00:00field_julian_date :25932.0julian_day_unit :days since 1950-01-01 00:00:00forecast_range :forecast_type :bulletin_date :2021-01-06 00:00:00bulletin_type :operationallongitude_min :-180.0longitude_max :179.91667latitude_min :-80.0latitude_max :90.0z_min :0.494025z_max :5727.917\n\n\nWe can slice and get the portion of data we need. In consistency with the other notebooks, let‚Äôs get data from Jan-March 2003 for salinity at the most shallow level, within our region of interest (60-80 deg E, 5-25 deg N)\nDISCLAIMER: DO NOT LOAD DATASET IN ITS ENTIRETY BEFORE SLICING IF YOU DO NOT WANT TO OVERFLOW YOUR MEMORY. Actual data is only loaded when you slice the dataset.\n\nDS_sliced = DS['so'].isel(depth=0).sel(longitude=slice(60, 80), latitude=slice(5, 25), time=slice('2003-01', '2003-02'))\n\n\nDS_sliced\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'so' (time: 59, latitude: 241, longitude: 241)&gt;\narray([[[35.399944, 35.389263, ..., 33.65581 , 33.680225],\n        [35.425884, 35.407574, ..., 33.587147, 33.597828],\n        ...,\n        [36.52913 , 36.526077, ...,       nan,       nan],\n        [36.552017, 36.53981 , ...,       nan,       nan]],\n\n       [[35.384686, 35.380108, ..., 33.7611  , 33.78399 ],\n        [35.387737, 35.38316 , ..., 33.9259  , 34.067814],\n        ...,\n        [36.52913 , 36.527603, ...,       nan,       nan],\n        [36.545914, 36.54744 , ...,       nan,       nan]],\n\n       ...,\n\n       [[35.245827, 35.22599 , ..., 33.957947, 33.991516],\n        [35.254982, 35.24125 , ..., 33.898434, 33.935055],\n        ...,\n        [36.530655, 36.52913 , ...,       nan,       nan],\n        [36.552017, 36.57033 , ...,       nan,       nan]],\n\n       [[35.242775, 35.221413, ..., 34.06018 , 34.083073],\n        [35.26261 , 35.245827, ..., 34.0083  , 34.040344],\n        ...,\n        [36.562702, 36.57491 , ...,       nan,       nan],\n        [36.590168, 36.602375, ...,       nan,       nan]]], dtype=float32)\nCoordinates:\n  * longitude  (longitude) float32 60.0 60.08 60.17 60.25 ... 79.83 79.92 80.0\n  * latitude   (latitude) float32 5.0 5.083 5.167 5.25 ... 24.83 24.92 25.0\n    depth      float32 0.494\n  * time       (time) datetime64[ns] 2003-01-01T12:00:00 ... 2003-02-28T12:00:00\nAttributes:\n    long_name:      Salinity\n    standard_name:  sea_water_salinity\n    units:          1e-3\n    unit_long:      Practical Salinity Unit\n    valid_min:      1\n    valid_max:      28336\n    cell_methods:   area: mean\n    _ChunkSizes:    [1, 7, 341, 720]xarray.DataArray'so'time: 59latitude: 241longitude: 24135.4 35.39 35.38 35.37 35.37 35.37 35.36 ... nan nan nan nan nan nanarray([[[35.399944, 35.389263, ..., 33.65581 , 33.680225],\n        [35.425884, 35.407574, ..., 33.587147, 33.597828],\n        ...,\n        [36.52913 , 36.526077, ...,       nan,       nan],\n        [36.552017, 36.53981 , ...,       nan,       nan]],\n\n       [[35.384686, 35.380108, ..., 33.7611  , 33.78399 ],\n        [35.387737, 35.38316 , ..., 33.9259  , 34.067814],\n        ...,\n        [36.52913 , 36.527603, ...,       nan,       nan],\n        [36.545914, 36.54744 , ...,       nan,       nan]],\n\n       ...,\n\n       [[35.245827, 35.22599 , ..., 33.957947, 33.991516],\n        [35.254982, 35.24125 , ..., 33.898434, 33.935055],\n        ...,\n        [36.530655, 36.52913 , ...,       nan,       nan],\n        [36.552017, 36.57033 , ...,       nan,       nan]],\n\n       [[35.242775, 35.221413, ..., 34.06018 , 34.083073],\n        [35.26261 , 35.245827, ..., 34.0083  , 34.040344],\n        ...,\n        [36.562702, 36.57491 , ...,       nan,       nan],\n        [36.590168, 36.602375, ...,       nan,       nan]]], dtype=float32)Coordinates: (4)longitude(longitude)float3260.0 60.08 60.17 ... 79.92 80.0valid_min :-180.0valid_max :179.91667step :0.08332825units :degrees_eastunit_long :Degrees Eastlong_name :Longitudestandard_name :longitudeaxis :X_ChunkSizes :4320array([60.      , 60.083332, 60.166668, ..., 79.833336, 79.916664, 80.      ],\n      dtype=float32)latitude(latitude)float325.0 5.083 5.167 ... 24.92 25.0valid_min :-80.0valid_max :90.0step :0.08333588units :degrees_northunit_long :Degrees Northlong_name :Latitudestandard_name :latitudeaxis :Y_ChunkSizes :2041array([ 5.      ,  5.083333,  5.166667, ..., 24.833334, 24.916666, 25.      ],\n      dtype=float32)depth()float320.494valid_min :0.494025valid_max :5727.917units :mpositive :downunit_long :Meterslong_name :Depthstandard_name :depthaxis :Z_ChunkSizes :50array(0.494025, dtype=float32)time(time)datetime64[ns]2003-01-01T12:00:00 ... 2003-02-...long_name :Time (hours since 1950-01-01)standard_name :timeaxis :T_ChunkSizes :1024array(['2003-01-01T12:00:00.000000000', '2003-01-02T12:00:00.000000000',\n       '2003-01-03T12:00:00.000000000', '2003-01-04T12:00:00.000000000',\n       '2003-01-05T12:00:00.000000000', '2003-01-06T12:00:00.000000000',\n       '2003-01-07T12:00:00.000000000', '2003-01-08T12:00:00.000000000',\n       '2003-01-09T12:00:00.000000000', '2003-01-10T12:00:00.000000000',\n       '2003-01-11T12:00:00.000000000', '2003-01-12T12:00:00.000000000',\n       '2003-01-13T12:00:00.000000000', '2003-01-14T12:00:00.000000000',\n       '2003-01-15T12:00:00.000000000', '2003-01-16T12:00:00.000000000',\n       '2003-01-17T12:00:00.000000000', '2003-01-18T12:00:00.000000000',\n       '2003-01-19T12:00:00.000000000', '2003-01-20T12:00:00.000000000',\n       '2003-01-21T12:00:00.000000000', '2003-01-22T12:00:00.000000000',\n       '2003-01-23T12:00:00.000000000', '2003-01-24T12:00:00.000000000',\n       '2003-01-25T12:00:00.000000000', '2003-01-26T12:00:00.000000000',\n       '2003-01-27T12:00:00.000000000', '2003-01-28T12:00:00.000000000',\n       '2003-01-29T12:00:00.000000000', '2003-01-30T12:00:00.000000000',\n       '2003-01-31T12:00:00.000000000', '2003-02-01T12:00:00.000000000',\n       '2003-02-02T12:00:00.000000000', '2003-02-03T12:00:00.000000000',\n       '2003-02-04T12:00:00.000000000', '2003-02-05T12:00:00.000000000',\n       '2003-02-06T12:00:00.000000000', '2003-02-07T12:00:00.000000000',\n       '2003-02-08T12:00:00.000000000', '2003-02-09T12:00:00.000000000',\n       '2003-02-10T12:00:00.000000000', '2003-02-11T12:00:00.000000000',\n       '2003-02-12T12:00:00.000000000', '2003-02-13T12:00:00.000000000',\n       '2003-02-14T12:00:00.000000000', '2003-02-15T12:00:00.000000000',\n       '2003-02-16T12:00:00.000000000', '2003-02-17T12:00:00.000000000',\n       '2003-02-18T12:00:00.000000000', '2003-02-19T12:00:00.000000000',\n       '2003-02-20T12:00:00.000000000', '2003-02-21T12:00:00.000000000',\n       '2003-02-22T12:00:00.000000000', '2003-02-23T12:00:00.000000000',\n       '2003-02-24T12:00:00.000000000', '2003-02-25T12:00:00.000000000',\n       '2003-02-26T12:00:00.000000000', '2003-02-27T12:00:00.000000000',\n       '2003-02-28T12:00:00.000000000'], dtype='datetime64[ns]')Indexes: (3)longitudePandasIndexPandasIndex(Index([             60.0, 60.08333206176758, 60.16666793823242,\n                   60.25, 60.33333206176758, 60.41666793823242,\n                    60.5, 60.58333206176758, 60.66666793823242,\n                   60.75,\n       ...\n                   79.25, 79.33333587646484, 79.41666412353516,\n                    79.5, 79.58333587646484, 79.66666412353516,\n                   79.75, 79.83333587646484, 79.91666412353516,\n                    80.0],\n      dtype='float32', name='longitude', length=241))latitudePandasIndexPandasIndex(Index([              5.0, 5.083333492279053, 5.166666507720947,\n                    5.25, 5.333333492279053, 5.416666507720947,\n                     5.5, 5.583333492279053, 5.666666507720947,\n                    5.75,\n       ...\n                   24.25, 24.33333396911621, 24.41666603088379,\n                    24.5, 24.58333396911621, 24.66666603088379,\n                   24.75, 24.83333396911621, 24.91666603088379,\n                    25.0],\n      dtype='float32', name='latitude', length=241))timePandasIndexPandasIndex(DatetimeIndex(['2003-01-01 12:00:00', '2003-01-02 12:00:00',\n               '2003-01-03 12:00:00', '2003-01-04 12:00:00',\n               '2003-01-05 12:00:00', '2003-01-06 12:00:00',\n               '2003-01-07 12:00:00', '2003-01-08 12:00:00',\n               '2003-01-09 12:00:00', '2003-01-10 12:00:00',\n               '2003-01-11 12:00:00', '2003-01-12 12:00:00',\n               '2003-01-13 12:00:00', '2003-01-14 12:00:00',\n               '2003-01-15 12:00:00', '2003-01-16 12:00:00',\n               '2003-01-17 12:00:00', '2003-01-18 12:00:00',\n               '2003-01-19 12:00:00', '2003-01-20 12:00:00',\n               '2003-01-21 12:00:00', '2003-01-22 12:00:00',\n               '2003-01-23 12:00:00', '2003-01-24 12:00:00',\n               '2003-01-25 12:00:00', '2003-01-26 12:00:00',\n               '2003-01-27 12:00:00', '2003-01-28 12:00:00',\n               '2003-01-29 12:00:00', '2003-01-30 12:00:00',\n               '2003-01-31 12:00:00', '2003-02-01 12:00:00',\n               '2003-02-02 12:00:00', '2003-02-03 12:00:00',\n               '2003-02-04 12:00:00', '2003-02-05 12:00:00',\n               '2003-02-06 12:00:00', '2003-02-07 12:00:00',\n               '2003-02-08 12:00:00', '2003-02-09 12:00:00',\n               '2003-02-10 12:00:00', '2003-02-11 12:00:00',\n               '2003-02-12 12:00:00', '2003-02-13 12:00:00',\n               '2003-02-14 12:00:00', '2003-02-15 12:00:00',\n               '2003-02-16 12:00:00', '2003-02-17 12:00:00',\n               '2003-02-18 12:00:00', '2003-02-19 12:00:00',\n               '2003-02-20 12:00:00', '2003-02-21 12:00:00',\n               '2003-02-22 12:00:00', '2003-02-23 12:00:00',\n               '2003-02-24 12:00:00', '2003-02-25 12:00:00',\n               '2003-02-26 12:00:00', '2003-02-27 12:00:00',\n               '2003-02-28 12:00:00'],\n              dtype='datetime64[ns]', name='time', freq=None))Attributes: (8)long_name :Salinitystandard_name :sea_water_salinityunits :1e-3unit_long :Practical Salinity Unitvalid_min :1valid_max :28336cell_methods :area: mean_ChunkSizes :[1, 7, 341, 720]\n\n\nYou can get rid of the depth since it is an empty coordinate. Make sure to note the depth of the data sampled in your attribute when you work on the data, though!\n\nDS_sliced = DS_sliced.drop('depth')\n\n\nDS_sliced\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'so' (time: 59, latitude: 241, longitude: 241)&gt;\narray([[[35.399944, 35.389263, ..., 33.65581 , 33.680225],\n        [35.425884, 35.407574, ..., 33.587147, 33.597828],\n        ...,\n        [36.52913 , 36.526077, ...,       nan,       nan],\n        [36.552017, 36.53981 , ...,       nan,       nan]],\n\n       [[35.384686, 35.380108, ..., 33.7611  , 33.78399 ],\n        [35.387737, 35.38316 , ..., 33.9259  , 34.067814],\n        ...,\n        [36.52913 , 36.527603, ...,       nan,       nan],\n        [36.545914, 36.54744 , ...,       nan,       nan]],\n\n       ...,\n\n       [[35.245827, 35.22599 , ..., 33.957947, 33.991516],\n        [35.254982, 35.24125 , ..., 33.898434, 33.935055],\n        ...,\n        [36.530655, 36.52913 , ...,       nan,       nan],\n        [36.552017, 36.57033 , ...,       nan,       nan]],\n\n       [[35.242775, 35.221413, ..., 34.06018 , 34.083073],\n        [35.26261 , 35.245827, ..., 34.0083  , 34.040344],\n        ...,\n        [36.562702, 36.57491 , ...,       nan,       nan],\n        [36.590168, 36.602375, ...,       nan,       nan]]], dtype=float32)\nCoordinates:\n  * longitude  (longitude) float32 60.0 60.08 60.17 60.25 ... 79.83 79.92 80.0\n  * latitude   (latitude) float32 5.0 5.083 5.167 5.25 ... 24.83 24.92 25.0\n  * time       (time) datetime64[ns] 2003-01-01T12:00:00 ... 2003-02-28T12:00:00\nAttributes:\n    long_name:      Salinity\n    standard_name:  sea_water_salinity\n    units:          1e-3\n    unit_long:      Practical Salinity Unit\n    valid_min:      1\n    valid_max:      28336\n    cell_methods:   area: mean\n    _ChunkSizes:    [1, 7, 341, 720]xarray.DataArray'so'time: 59latitude: 241longitude: 24135.4 35.39 35.38 35.37 35.37 35.37 35.36 ... nan nan nan nan nan nanarray([[[35.399944, 35.389263, ..., 33.65581 , 33.680225],\n        [35.425884, 35.407574, ..., 33.587147, 33.597828],\n        ...,\n        [36.52913 , 36.526077, ...,       nan,       nan],\n        [36.552017, 36.53981 , ...,       nan,       nan]],\n\n       [[35.384686, 35.380108, ..., 33.7611  , 33.78399 ],\n        [35.387737, 35.38316 , ..., 33.9259  , 34.067814],\n        ...,\n        [36.52913 , 36.527603, ...,       nan,       nan],\n        [36.545914, 36.54744 , ...,       nan,       nan]],\n\n       ...,\n\n       [[35.245827, 35.22599 , ..., 33.957947, 33.991516],\n        [35.254982, 35.24125 , ..., 33.898434, 33.935055],\n        ...,\n        [36.530655, 36.52913 , ...,       nan,       nan],\n        [36.552017, 36.57033 , ...,       nan,       nan]],\n\n       [[35.242775, 35.221413, ..., 34.06018 , 34.083073],\n        [35.26261 , 35.245827, ..., 34.0083  , 34.040344],\n        ...,\n        [36.562702, 36.57491 , ...,       nan,       nan],\n        [36.590168, 36.602375, ...,       nan,       nan]]], dtype=float32)Coordinates: (3)longitude(longitude)float3260.0 60.08 60.17 ... 79.92 80.0valid_min :-180.0valid_max :179.91667step :0.08332825units :degrees_eastunit_long :Degrees Eastlong_name :Longitudestandard_name :longitudeaxis :X_ChunkSizes :4320array([60.      , 60.083332, 60.166668, ..., 79.833336, 79.916664, 80.      ],\n      dtype=float32)latitude(latitude)float325.0 5.083 5.167 ... 24.92 25.0valid_min :-80.0valid_max :90.0step :0.08333588units :degrees_northunit_long :Degrees Northlong_name :Latitudestandard_name :latitudeaxis :Y_ChunkSizes :2041array([ 5.      ,  5.083333,  5.166667, ..., 24.833334, 24.916666, 25.      ],\n      dtype=float32)time(time)datetime64[ns]2003-01-01T12:00:00 ... 2003-02-...long_name :Time (hours since 1950-01-01)standard_name :timeaxis :T_ChunkSizes :1024array(['2003-01-01T12:00:00.000000000', '2003-01-02T12:00:00.000000000',\n       '2003-01-03T12:00:00.000000000', '2003-01-04T12:00:00.000000000',\n       '2003-01-05T12:00:00.000000000', '2003-01-06T12:00:00.000000000',\n       '2003-01-07T12:00:00.000000000', '2003-01-08T12:00:00.000000000',\n       '2003-01-09T12:00:00.000000000', '2003-01-10T12:00:00.000000000',\n       '2003-01-11T12:00:00.000000000', '2003-01-12T12:00:00.000000000',\n       '2003-01-13T12:00:00.000000000', '2003-01-14T12:00:00.000000000',\n       '2003-01-15T12:00:00.000000000', '2003-01-16T12:00:00.000000000',\n       '2003-01-17T12:00:00.000000000', '2003-01-18T12:00:00.000000000',\n       '2003-01-19T12:00:00.000000000', '2003-01-20T12:00:00.000000000',\n       '2003-01-21T12:00:00.000000000', '2003-01-22T12:00:00.000000000',\n       '2003-01-23T12:00:00.000000000', '2003-01-24T12:00:00.000000000',\n       '2003-01-25T12:00:00.000000000', '2003-01-26T12:00:00.000000000',\n       '2003-01-27T12:00:00.000000000', '2003-01-28T12:00:00.000000000',\n       '2003-01-29T12:00:00.000000000', '2003-01-30T12:00:00.000000000',\n       '2003-01-31T12:00:00.000000000', '2003-02-01T12:00:00.000000000',\n       '2003-02-02T12:00:00.000000000', '2003-02-03T12:00:00.000000000',\n       '2003-02-04T12:00:00.000000000', '2003-02-05T12:00:00.000000000',\n       '2003-02-06T12:00:00.000000000', '2003-02-07T12:00:00.000000000',\n       '2003-02-08T12:00:00.000000000', '2003-02-09T12:00:00.000000000',\n       '2003-02-10T12:00:00.000000000', '2003-02-11T12:00:00.000000000',\n       '2003-02-12T12:00:00.000000000', '2003-02-13T12:00:00.000000000',\n       '2003-02-14T12:00:00.000000000', '2003-02-15T12:00:00.000000000',\n       '2003-02-16T12:00:00.000000000', '2003-02-17T12:00:00.000000000',\n       '2003-02-18T12:00:00.000000000', '2003-02-19T12:00:00.000000000',\n       '2003-02-20T12:00:00.000000000', '2003-02-21T12:00:00.000000000',\n       '2003-02-22T12:00:00.000000000', '2003-02-23T12:00:00.000000000',\n       '2003-02-24T12:00:00.000000000', '2003-02-25T12:00:00.000000000',\n       '2003-02-26T12:00:00.000000000', '2003-02-27T12:00:00.000000000',\n       '2003-02-28T12:00:00.000000000'], dtype='datetime64[ns]')Indexes: (3)longitudePandasIndexPandasIndex(Index([             60.0, 60.08333206176758, 60.16666793823242,\n                   60.25, 60.33333206176758, 60.41666793823242,\n                    60.5, 60.58333206176758, 60.66666793823242,\n                   60.75,\n       ...\n                   79.25, 79.33333587646484, 79.41666412353516,\n                    79.5, 79.58333587646484, 79.66666412353516,\n                   79.75, 79.83333587646484, 79.91666412353516,\n                    80.0],\n      dtype='float32', name='longitude', length=241))latitudePandasIndexPandasIndex(Index([              5.0, 5.083333492279053, 5.166666507720947,\n                    5.25, 5.333333492279053, 5.416666507720947,\n                     5.5, 5.583333492279053, 5.666666507720947,\n                    5.75,\n       ...\n                   24.25, 24.33333396911621, 24.41666603088379,\n                    24.5, 24.58333396911621, 24.66666603088379,\n                   24.75, 24.83333396911621, 24.91666603088379,\n                    25.0],\n      dtype='float32', name='latitude', length=241))timePandasIndexPandasIndex(DatetimeIndex(['2003-01-01 12:00:00', '2003-01-02 12:00:00',\n               '2003-01-03 12:00:00', '2003-01-04 12:00:00',\n               '2003-01-05 12:00:00', '2003-01-06 12:00:00',\n               '2003-01-07 12:00:00', '2003-01-08 12:00:00',\n               '2003-01-09 12:00:00', '2003-01-10 12:00:00',\n               '2003-01-11 12:00:00', '2003-01-12 12:00:00',\n               '2003-01-13 12:00:00', '2003-01-14 12:00:00',\n               '2003-01-15 12:00:00', '2003-01-16 12:00:00',\n               '2003-01-17 12:00:00', '2003-01-18 12:00:00',\n               '2003-01-19 12:00:00', '2003-01-20 12:00:00',\n               '2003-01-21 12:00:00', '2003-01-22 12:00:00',\n               '2003-01-23 12:00:00', '2003-01-24 12:00:00',\n               '2003-01-25 12:00:00', '2003-01-26 12:00:00',\n               '2003-01-27 12:00:00', '2003-01-28 12:00:00',\n               '2003-01-29 12:00:00', '2003-01-30 12:00:00',\n               '2003-01-31 12:00:00', '2003-02-01 12:00:00',\n               '2003-02-02 12:00:00', '2003-02-03 12:00:00',\n               '2003-02-04 12:00:00', '2003-02-05 12:00:00',\n               '2003-02-06 12:00:00', '2003-02-07 12:00:00',\n               '2003-02-08 12:00:00', '2003-02-09 12:00:00',\n               '2003-02-10 12:00:00', '2003-02-11 12:00:00',\n               '2003-02-12 12:00:00', '2003-02-13 12:00:00',\n               '2003-02-14 12:00:00', '2003-02-15 12:00:00',\n               '2003-02-16 12:00:00', '2003-02-17 12:00:00',\n               '2003-02-18 12:00:00', '2003-02-19 12:00:00',\n               '2003-02-20 12:00:00', '2003-02-21 12:00:00',\n               '2003-02-22 12:00:00', '2003-02-23 12:00:00',\n               '2003-02-24 12:00:00', '2003-02-25 12:00:00',\n               '2003-02-26 12:00:00', '2003-02-27 12:00:00',\n               '2003-02-28 12:00:00'],\n              dtype='datetime64[ns]', name='time', freq=None))Attributes: (8)long_name :Salinitystandard_name :sea_water_salinityunits :1e-3unit_long :Practical Salinity Unitvalid_min :1valid_max :28336cell_methods :area: mean_ChunkSizes :[1, 7, 341, 720]\n\n\n\nDS.isel(time=0)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:    (longitude: 4320, latitude: 2041, depth: 50)\nCoordinates:\n  * longitude  (longitude) float32 -180.0 -179.9 -179.8 ... 179.8 179.8 179.9\n  * latitude   (latitude) float32 -80.0 -79.92 -79.83 ... 89.83 89.92 90.0\n  * depth      (depth) float32 0.494 1.541 2.646 ... 5.275e+03 5.728e+03\n    time       datetime64[ns] 1993-01-01T12:00:00\nData variables:\n    mlotst     (latitude, longitude) float32 ...\n    zos        (latitude, longitude) float32 ...\n    bottomT    (latitude, longitude) float32 ...\n    sithick    (latitude, longitude) float32 ...\n    siconc     (latitude, longitude) float32 ...\n    usi        (latitude, longitude) float32 ...\n    vsi        (latitude, longitude) float32 ...\n    thetao     (depth, latitude, longitude) float32 ...\n    so         (depth, latitude, longitude) float32 ...\n    uo         (depth, latitude, longitude) float32 ...\n    vo         (depth, latitude, longitude) float32 ...\nAttributes: (12/24)\n    title:              daily mean fields from Global Ocean Physics Analysis ...\n    easting:            longitude\n    northing:           latitude\n    history:            2022/05/25 21:54:07 MERCATOR OCEAN Netcdf creation\n    source:             MERCATOR GLORYS12V1\n    institution:        MERCATOR OCEAN\n    ...                 ...\n    longitude_min:      -180.0\n    longitude_max:      179.91667\n    latitude_min:       -80.0\n    latitude_max:       90.0\n    z_min:              0.494025\n    z_max:              5727.917xarray.DatasetDimensions:longitude: 4320latitude: 2041depth: 50Coordinates: (4)longitude(longitude)float32-180.0 -179.9 ... 179.8 179.9valid_min :-180.0valid_max :179.91667step :0.08332825units :degrees_eastunit_long :Degrees Eastlong_name :Longitudestandard_name :longitudeaxis :X_ChunkSizes :4320array([-180.     , -179.91667, -179.83333, ...,  179.75   ,  179.83333,\n        179.91667], dtype=float32)latitude(latitude)float32-80.0 -79.92 -79.83 ... 89.92 90.0valid_min :-80.0valid_max :90.0step :0.08333588units :degrees_northunit_long :Degrees Northlong_name :Latitudestandard_name :latitudeaxis :Y_ChunkSizes :2041array([-80.      , -79.916664, -79.833336, ...,  89.833336,  89.916664,\n        90.      ], dtype=float32)depth(depth)float320.494 1.541 ... 5.275e+03 5.728e+03valid_min :0.494025valid_max :5727.917units :mpositive :downunit_long :Meterslong_name :Depthstandard_name :depthaxis :Z_ChunkSizes :50array([4.940250e-01, 1.541375e+00, 2.645669e+00, 3.819495e+00, 5.078224e+00,\n       6.440614e+00, 7.929560e+00, 9.572997e+00, 1.140500e+01, 1.346714e+01,\n       1.581007e+01, 1.849556e+01, 2.159882e+01, 2.521141e+01, 2.944473e+01,\n       3.443415e+01, 4.034405e+01, 4.737369e+01, 5.576429e+01, 6.580727e+01,\n       7.785385e+01, 9.232607e+01, 1.097293e+02, 1.306660e+02, 1.558507e+02,\n       1.861256e+02, 2.224752e+02, 2.660403e+02, 3.181274e+02, 3.802130e+02,\n       4.539377e+02, 5.410889e+02, 6.435668e+02, 7.633331e+02, 9.023393e+02,\n       1.062440e+03, 1.245291e+03, 1.452251e+03, 1.684284e+03, 1.941893e+03,\n       2.225078e+03, 2.533336e+03, 2.865703e+03, 3.220820e+03, 3.597032e+03,\n       3.992484e+03, 4.405224e+03, 4.833291e+03, 5.274784e+03, 5.727917e+03],\n      dtype=float32)time()datetime64[ns]1993-01-01T12:00:00long_name :Time (hours since 1950-01-01)standard_name :timeaxis :T_ChunkSizes :1024array('1993-01-01T12:00:00.000000000', dtype='datetime64[ns]')Data variables: (11)mlotst(latitude, longitude)float32...long_name :Density ocean mixed layer thicknessstandard_name :ocean_mixed_layer_thickness_defined_by_sigma_thetaunits :munit_long :Metersvalid_min :1valid_max :6447cell_methods :area: mean_ChunkSizes :[1, 681, 1440][8817120 values with dtype=float32]zos(latitude, longitude)float32...long_name :Sea surface heightstandard_name :sea_surface_height_above_geoidunits :munit_long :Metersvalid_min :-6181valid_max :5039cell_methods :area: mean_ChunkSizes :[1, 681, 1440][8817120 values with dtype=float32]bottomT(latitude, longitude)float32...long_name :Sea floor potential temperaturestandard_name :sea_water_potential_temperature_at_sea_floorunits :degrees_Cunit_long :Degrees Celsiusvalid_min :-32757valid_max :21318cell_methods :area: mean_ChunkSizes :[1, 681, 1440][8817120 values with dtype=float32]sithick(latitude, longitude)float32...long_name :Sea ice thicknessstandard_name :sea_ice_thicknessunits :munit_long :Metersvalid_min :1valid_max :4618cell_methods :area: mean where sea_ice_ChunkSizes :[1, 681, 1440][8817120 values with dtype=float32]siconc(latitude, longitude)float32...long_name :Ice concentrationstandard_name :sea_ice_area_fractionunits :1unit_long :Fractionvalid_min :1valid_max :28060cell_methods :area: mean where sea_ice_ChunkSizes :[1, 681, 1440][8817120 values with dtype=float32]usi(latitude, longitude)float32...long_name :Sea ice eastward velocitystandard_name :eastward_sea_ice_velocityunits :m s-1unit_long :Meters per secondvalid_min :-32707valid_max :32144cell_methods :area: mean where sea_ice_ChunkSizes :[1, 681, 1440][8817120 values with dtype=float32]vsi(latitude, longitude)float32...long_name :Sea ice northward velocitystandard_name :northward_sea_ice_velocityunits :m s-1unit_long :Meters per secondvalid_min :-31907valid_max :31690cell_methods :area: mean where sea_ice_ChunkSizes :[1, 681, 1440][8817120 values with dtype=float32]thetao(depth, latitude, longitude)float32...long_name :Temperaturestandard_name :sea_water_potential_temperatureunits :degrees_Cunit_long :Degrees Celsiusvalid_min :-32764valid_max :21298cell_methods :area: mean_ChunkSizes :[1, 7, 341, 720][440856000 values with dtype=float32]so(depth, latitude, longitude)float32...long_name :Salinitystandard_name :sea_water_salinityunits :1e-3unit_long :Practical Salinity Unitvalid_min :1valid_max :28336cell_methods :area: mean_ChunkSizes :[1, 7, 341, 720][440856000 values with dtype=float32]uo(depth, latitude, longitude)float32...long_name :Eastward velocitystandard_name :eastward_sea_water_velocityunits :m s-1unit_long :Meters per secondvalid_min :-3573valid_max :3559cell_methods :area: mean_ChunkSizes :[1, 7, 341, 720][440856000 values with dtype=float32]vo(depth, latitude, longitude)float32...long_name :Northward velocitystandard_name :northward_sea_water_velocityunits :m s-1unit_long :Meters per secondvalid_min :-3300valid_max :3399cell_methods :area: mean_ChunkSizes :[1, 7, 341, 720][440856000 values with dtype=float32]Indexes: (3)longitudePandasIndexPandasIndex(Index([            -180.0, -179.9166717529297, -179.8333282470703,\n                  -179.75, -179.6666717529297, -179.5833282470703,\n                   -179.5, -179.4166717529297, -179.3333282470703,\n                  -179.25,\n       ...\n        179.1666717529297,             179.25,  179.3333282470703,\n        179.4166717529297,              179.5,  179.5833282470703,\n        179.6666717529297,             179.75,  179.8333282470703,\n        179.9166717529297],\n      dtype='float32', name='longitude', length=4320))latitudePandasIndexPandasIndex(Index([             -80.0, -79.91666412353516, -79.83333587646484,\n                   -79.75, -79.66666412353516, -79.58333587646484,\n                    -79.5, -79.41666412353516, -79.33333587646484,\n                   -79.25,\n       ...\n                    89.25,  89.33333587646484,  89.41666412353516,\n                     89.5,  89.58333587646484,  89.66666412353516,\n                    89.75,  89.83333587646484,  89.91666412353516,\n                     90.0],\n      dtype='float32', name='latitude', length=2041))depthPandasIndexPandasIndex(Index([0.49402499198913574,  1.5413750410079956,  2.6456689834594727,\n        3.8194949626922607,   5.078224182128906,   6.440614223480225,\n          7.92956018447876,   9.572997093200684,  11.404999732971191,\n        13.467140197753906,  15.810070037841797,  18.495559692382812,\n        21.598819732666016,  25.211410522460938,  29.444730758666992,\n         34.43415069580078,  40.344051361083984,   47.37369155883789,\n         55.76428985595703,   65.80726623535156,   77.85385131835938,\n          92.3260726928711,  109.72930145263672,  130.66600036621094,\n        155.85069274902344,  186.12559509277344,  222.47520446777344,\n         266.0403137207031,   318.1274108886719,   380.2130126953125,\n         453.9377136230469,   541.0889282226562,   643.5667724609375,\n         763.3331298828125,   902.3392944335938,    1062.43994140625,\n            1245.291015625,     1452.2509765625,  1684.2840576171875,\n        1941.8929443359375,   2225.077880859375,        2533.3359375,\n         2865.702880859375,   3220.820068359375,   3597.031982421875,\n          3992.48388671875,    4405.22412109375,      4833.291015625,\n           5274.7841796875,     5727.9169921875],\n      dtype='float32', name='depth'))Attributes: (24)title :daily mean fields from Global Ocean Physics Analysis and Forecast updated Dailyeasting :longitudenorthing :latitudehistory :2022/05/25 21:54:07 MERCATOR OCEAN Netcdf creationsource :MERCATOR GLORYS12V1institution :MERCATOR OCEANreferences :http://www.mercator-ocean.frcomment :CMEMS productConventions :CF-1.4domain_name :GL12field_type :meanfield_date :2020-12-31 00:00:00field_julian_date :25932.0julian_day_unit :days since 1950-01-01 00:00:00forecast_range :forecast_type :bulletin_date :2021-01-06 00:00:00bulletin_type :operationallongitude_min :-180.0longitude_max :179.91667latitude_min :-80.0latitude_max :90.0z_min :0.494025z_max :5727.917\n\n\nFinally, we can export this dataset to combine with the rest of the data later\n\nDS_sliced.to_netcdf('demonstrated data/salinity_at_0_49m.nc')"
  },
  {
    "objectID": "python-notebooks/01_earthaccess_Downloader.html#authorize-credentials",
    "href": "python-notebooks/01_earthaccess_Downloader.html#authorize-credentials",
    "title": "NASA Earth Data",
    "section": "Authorize credentials",
    "text": "Authorize credentials\nTo download data from NASA‚Äôs Earth Data database, it‚Äôs recommended that you set up a .netrc credential file so that you don‚Äôt have to manually log in every time you run a downloading script. To do this, consult 2021 Cloud Hackathon‚Äôs tutorial here. Make sure to register an account with Earth Data first before following the tutorial.\nWhen you finished implementing the .netrc file, continue with the tutorial below"
  },
  {
    "objectID": "python-notebooks/01_earthaccess_Downloader.html#import-necessary-libraries",
    "href": "python-notebooks/01_earthaccess_Downloader.html#import-necessary-libraries",
    "title": "NASA Earth Data",
    "section": "Import necessary libraries",
    "text": "Import necessary libraries\n\nimport xarray as xr\nimport earthaccess\nimport numpy as np\nimport pandas as pd\nimport os, glob"
  },
  {
    "objectID": "python-notebooks/01_earthaccess_Downloader.html#download-data-to-your-local-machine-using-earthaccess-library",
    "href": "python-notebooks/01_earthaccess_Downloader.html#download-data-to-your-local-machine-using-earthaccess-library",
    "title": "NASA Earth Data",
    "section": "Download data to your local machine using earthaccess library",
    "text": "Download data to your local machine using earthaccess library\nEarthaccess library streamlines your downloading, slicing, and searching for granules easier than ever. For cloud-hosted datasets (which is what this tutorial best works with), we choose to download granules to the local machine instead of streaming them to the working Python scripts as some users may not be physically available in the us-west region for streaming to be effective. Local downloading may result in heavy file sizes, but is consistent, and we am also providing some tweaks to save as much as you can, especially if your research interest area requires a long temporal range and does not cover globally.\n\n# Log in using .netrc file\nauth = earthaccess.login(strategy=\"netrc\")\n\nWe are already authenticated with NASA EDL"
  },
  {
    "objectID": "python-notebooks/01_earthaccess_Downloader.html#streaming-granules",
    "href": "python-notebooks/01_earthaccess_Downloader.html#streaming-granules",
    "title": "NASA Earth Data",
    "section": "Streaming granules",
    "text": "Streaming granules\n\n# EarthAccess's approach to collecting granules\nresults = earthaccess.search_data(\n    short_name='MUR-JPL-L4-GLOB-v4.1',\n    cloud_hosted=True,\n    bounding_box = (60, 5, 80, 25),\n    temporal=(\"2003-01-01\", \"2003-02-28\")\n)\n\nGranules found: 59\n\n\n\nfiles = earthaccess.open(results) # s3\n\n Opening 59 granules, approx size: 0.0 GB\n\n\n\n\n\n\n\n\n\n\n\n\nds = xr.open_mfdataset(files)\n\n\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:           (time: 59, lat: 17999, lon: 36000)\nCoordinates:\n  * time              (time) datetime64[ns] 2003-01-01T09:00:00 ... 2003-02-2...\n  * lat               (lat) float32 -89.99 -89.98 -89.97 ... 89.97 89.98 89.99\n  * lon               (lon) float32 -180.0 -180.0 -180.0 ... 180.0 180.0 180.0\nData variables:\n    analysed_sst      (time, lat, lon) float32 dask.array&lt;chunksize=(1, 17999, 36000), meta=np.ndarray&gt;\n    analysis_error    (time, lat, lon) float32 dask.array&lt;chunksize=(1, 17999, 36000), meta=np.ndarray&gt;\n    mask              (time, lat, lon) float32 dask.array&lt;chunksize=(1, 17999, 36000), meta=np.ndarray&gt;\n    sea_ice_fraction  (time, lat, lon) float32 dask.array&lt;chunksize=(1, 17999, 36000), meta=np.ndarray&gt;\nAttributes: (12/47)\n    Conventions:                CF-1.5\n    title:                      Daily MUR SST, Final product\n    summary:                    A merged, multi-sensor L4 Foundation SST anal...\n    references:                 http://podaac.jpl.nasa.gov/Multi-scale_Ultra-...\n    institution:                Jet Propulsion Laboratory\n    history:                    created at nominal 4-day latency; replaced nr...\n    ...                         ...\n    project:                    NASA Making Earth Science Data Records for Us...\n    publisher_name:             GHRSST Project Office\n    publisher_url:              http://www.ghrsst.org\n    publisher_email:            ghrsst-po@nceo.ac.uk\n    processing_level:           L4\n    cdm_data_type:              gridxarray.DatasetDimensions:time: 59lat: 17999lon: 36000Coordinates: (3)time(time)datetime64[ns]2003-01-01T09:00:00 ... 2003-02-...long_name :reference time of sst fieldstandard_name :timeaxis :Tcomment :Nominal time of analyzed fieldsarray(['2003-01-01T09:00:00.000000000', '2003-01-02T09:00:00.000000000',\n       '2003-01-03T09:00:00.000000000', '2003-01-04T09:00:00.000000000',\n       '2003-01-05T09:00:00.000000000', '2003-01-06T09:00:00.000000000',\n       '2003-01-07T09:00:00.000000000', '2003-01-08T09:00:00.000000000',\n       '2003-01-09T09:00:00.000000000', '2003-01-10T09:00:00.000000000',\n       '2003-01-11T09:00:00.000000000', '2003-01-12T09:00:00.000000000',\n       '2003-01-13T09:00:00.000000000', '2003-01-14T09:00:00.000000000',\n       '2003-01-15T09:00:00.000000000', '2003-01-16T09:00:00.000000000',\n       '2003-01-17T09:00:00.000000000', '2003-01-18T09:00:00.000000000',\n       '2003-01-19T09:00:00.000000000', '2003-01-20T09:00:00.000000000',\n       '2003-01-21T09:00:00.000000000', '2003-01-22T09:00:00.000000000',\n       '2003-01-23T09:00:00.000000000', '2003-01-24T09:00:00.000000000',\n       '2003-01-25T09:00:00.000000000', '2003-01-26T09:00:00.000000000',\n       '2003-01-27T09:00:00.000000000', '2003-01-28T09:00:00.000000000',\n       '2003-01-29T09:00:00.000000000', '2003-01-30T09:00:00.000000000',\n       '2003-01-31T09:00:00.000000000', '2003-02-01T09:00:00.000000000',\n       '2003-02-02T09:00:00.000000000', '2003-02-03T09:00:00.000000000',\n       '2003-02-04T09:00:00.000000000', '2003-02-05T09:00:00.000000000',\n       '2003-02-06T09:00:00.000000000', '2003-02-07T09:00:00.000000000',\n       '2003-02-08T09:00:00.000000000', '2003-02-09T09:00:00.000000000',\n       '2003-02-10T09:00:00.000000000', '2003-02-11T09:00:00.000000000',\n       '2003-02-12T09:00:00.000000000', '2003-02-13T09:00:00.000000000',\n       '2003-02-14T09:00:00.000000000', '2003-02-15T09:00:00.000000000',\n       '2003-02-16T09:00:00.000000000', '2003-02-17T09:00:00.000000000',\n       '2003-02-18T09:00:00.000000000', '2003-02-19T09:00:00.000000000',\n       '2003-02-20T09:00:00.000000000', '2003-02-21T09:00:00.000000000',\n       '2003-02-22T09:00:00.000000000', '2003-02-23T09:00:00.000000000',\n       '2003-02-24T09:00:00.000000000', '2003-02-25T09:00:00.000000000',\n       '2003-02-26T09:00:00.000000000', '2003-02-27T09:00:00.000000000',\n       '2003-02-28T09:00:00.000000000'], dtype='datetime64[ns]')lat(lat)float32-89.99 -89.98 ... 89.98 89.99long_name :latitudestandard_name :latitudeaxis :Yunits :degrees_northvalid_min :-90.0valid_max :90.0comment :nonearray([-89.99, -89.98, -89.97, ...,  89.97,  89.98,  89.99], dtype=float32)lon(lon)float32-180.0 -180.0 ... 180.0 180.0long_name :longitudestandard_name :longitudeaxis :Xunits :degrees_eastvalid_min :-180.0valid_max :180.0comment :nonearray([-179.99, -179.98, -179.97, ...,  179.98,  179.99,  180.  ],\n      dtype=float32)Data variables: (4)analysed_sst(time, lat, lon)float32dask.array&lt;chunksize=(1, 17999, 36000), meta=np.ndarray&gt;long_name :analysed sea surface temperaturestandard_name :sea_surface_foundation_temperatureunits :kelvinvalid_min :-32767valid_max :32767comment :\"Final\" version using Multi-Resolution Variational Analysis (MRVA) method for interpolationsource :AMSRE-REMSS, AVHRR_Pathfinder-PFV5.2-NODC_day, AVHRR_Pathfinder-PFV5.2-NODC_night, MODIS_A-JPL, MODIS_T-JPL, iQUAM-NOAA/NESDIS, Ice_Conc-OSISAF\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n142.42 GiB\n2.41 GiB\n\n\nShape\n(59, 17999, 36000)\n(1, 17999, 36000)\n\n\nDask graph\n59 chunks in 119 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nanalysis_error\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 17999, 36000), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nestimated error standard deviation of analysed_sst\n\nunits :\n\nkelvin\n\nvalid_min :\n\n0\n\nvalid_max :\n\n32767\n\ncomment :\n\nnone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n142.42 GiB\n2.41 GiB\n\n\nShape\n(59, 17999, 36000)\n(1, 17999, 36000)\n\n\nDask graph\n59 chunks in 119 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nmask\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 17999, 36000), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nsea/land field composite mask\n\nvalid_min :\n\n1\n\nvalid_max :\n\n31\n\nflag_masks :\n\n[ 1 2 4 8 16]\n\nflag_values :\n\n[ 1 2 5 9 13]\n\nflag_meanings :\n\n1=open-sea, 2=land, 5=open-lake, 9=open-sea with ice in the grid, 13=open-lake with ice in the grid\n\ncomment :\n\nmask can be used to further filter the data.\n\nsource :\n\nGMT \"grdlandmask\", ice flag from sea_ice_fraction data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n142.42 GiB\n2.41 GiB\n\n\nShape\n(59, 17999, 36000)\n(1, 17999, 36000)\n\n\nDask graph\n59 chunks in 119 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nsea_ice_fraction\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 17999, 36000), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nsea ice area fraction\n\nstandard_name :\n\nsea ice area fraction\n\nunits :\n\nfraction (between 0 and 1)\n\nvalid_min :\n\n0\n\nvalid_max :\n\n100\n\nsource :\n\nEUMETSAT OSI-SAF, copyright EUMETSAT\n\ncomment :\n\nice data interpolated by a nearest neighbor approach.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n142.42 GiB\n2.41 GiB\n\n\nShape\n(59, 17999, 36000)\n(1, 17999, 36000)\n\n\nDask graph\n59 chunks in 119 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nIndexes: (3)timePandasIndexPandasIndex(DatetimeIndex(['2003-01-01 09:00:00', '2003-01-02 09:00:00',\n               '2003-01-03 09:00:00', '2003-01-04 09:00:00',\n               '2003-01-05 09:00:00', '2003-01-06 09:00:00',\n               '2003-01-07 09:00:00', '2003-01-08 09:00:00',\n               '2003-01-09 09:00:00', '2003-01-10 09:00:00',\n               '2003-01-11 09:00:00', '2003-01-12 09:00:00',\n               '2003-01-13 09:00:00', '2003-01-14 09:00:00',\n               '2003-01-15 09:00:00', '2003-01-16 09:00:00',\n               '2003-01-17 09:00:00', '2003-01-18 09:00:00',\n               '2003-01-19 09:00:00', '2003-01-20 09:00:00',\n               '2003-01-21 09:00:00', '2003-01-22 09:00:00',\n               '2003-01-23 09:00:00', '2003-01-24 09:00:00',\n               '2003-01-25 09:00:00', '2003-01-26 09:00:00',\n               '2003-01-27 09:00:00', '2003-01-28 09:00:00',\n               '2003-01-29 09:00:00', '2003-01-30 09:00:00',\n               '2003-01-31 09:00:00', '2003-02-01 09:00:00',\n               '2003-02-02 09:00:00', '2003-02-03 09:00:00',\n               '2003-02-04 09:00:00', '2003-02-05 09:00:00',\n               '2003-02-06 09:00:00', '2003-02-07 09:00:00',\n               '2003-02-08 09:00:00', '2003-02-09 09:00:00',\n               '2003-02-10 09:00:00', '2003-02-11 09:00:00',\n               '2003-02-12 09:00:00', '2003-02-13 09:00:00',\n               '2003-02-14 09:00:00', '2003-02-15 09:00:00',\n               '2003-02-16 09:00:00', '2003-02-17 09:00:00',\n               '2003-02-18 09:00:00', '2003-02-19 09:00:00',\n               '2003-02-20 09:00:00', '2003-02-21 09:00:00',\n               '2003-02-22 09:00:00', '2003-02-23 09:00:00',\n               '2003-02-24 09:00:00', '2003-02-25 09:00:00',\n               '2003-02-26 09:00:00', '2003-02-27 09:00:00',\n               '2003-02-28 09:00:00'],\n              dtype='datetime64[ns]', name='time', freq=None))latPandasIndexPandasIndex(Index([-89.98999786376953,  -89.9800033569336, -89.97000122070312,\n       -89.95999908447266, -89.94999694824219, -89.94000244140625,\n       -89.93000030517578, -89.91999816894531, -89.91000366210938,\n        -89.9000015258789,\n       ...\n         89.9000015258789,  89.91000366210938,  89.91999816894531,\n        89.93000030517578,  89.94000244140625,  89.94999694824219,\n        89.95999908447266,  89.97000122070312,   89.9800033569336,\n        89.98999786376953],\n      dtype='float32', name='lat', length=17999))lonPandasIndexPandasIndex(Index([-179.99000549316406, -179.97999572753906, -179.97000122070312,\n        -179.9600067138672,  -179.9499969482422, -179.94000244140625,\n       -179.92999267578125,  -179.9199981689453, -179.91000366210938,\n       -179.89999389648438,\n       ...\n        179.91000366210938,   179.9199981689453,  179.92999267578125,\n        179.94000244140625,   179.9499969482422,   179.9600067138672,\n        179.97000122070312,  179.97999572753906,  179.99000549316406,\n                     180.0],\n      dtype='float32', name='lon', length=36000))Attributes: (47)Conventions :CF-1.5title :Daily MUR SST, Final productsummary :A merged, multi-sensor L4 Foundation SST analysis product from JPL.references :http://podaac.jpl.nasa.gov/Multi-scale_Ultra-high_Resolution_MUR-SSTinstitution :Jet Propulsion Laboratoryhistory :created at nominal 4-day latency; replaced nrt (1-day latency) version.comment :MUR = \"Multi-scale Ultra-high Reolution\"license :These data are available free of charge under data policy of JPL PO.DAAC.id :MUR-JPL-L4-GLOB-v04.1naming_authority :org.ghrsstproduct_version :04.1uuid :27665bc0-d5fc-11e1-9b23-0800200c9a66gds_version_id :2.0netcdf_version_id :4.1date_created :20150818T185930Zstart_time :20030101T090000Zstop_time :20030101T090000Ztime_coverage_start :20021231T210000Ztime_coverage_end :20030101T210000Zfile_quality_level :1source :AMSRE-REMSS, AVHRR_Pathfinder-PFV5.2-NODC_day, AVHRR_Pathfinder-PFV5.2-NODC_night, MODIS_A-JPL, MODIS_T-JPL, iQUAM-NOAA/NESDIS, Ice_Conc-OSISAFplatform :Aqua, DMSP, NOAA-POES, Suomi-NPP, Terrasensor :AMSR-E, AVHRR, MODIS, SSM/I, VIIRS, in-situMetadata_Conventions :Unidata Observation Dataset v1.0metadata_link :http://podaac.jpl.nasa.gov/ws/metadata/dataset/?format=iso&shortName=MUR-JPL-L4-GLOB-v04.1keywords :Oceans &gt; Ocean Temperature &gt; Sea Surface Temperaturekeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordsstandard_name_vocabulary :NetCDF Climate and Forecast (CF) Metadata Conventionsouthernmost_latitude :-90.0northernmost_latitude :90.0westernmost_longitude :-180.0easternmost_longitude :180.0spatial_resolution :0.01 degreesgeospatial_lat_units :degrees northgeospatial_lat_resolution :0.01 degreesgeospatial_lon_units :degrees eastgeospatial_lon_resolution :0.01 degreesacknowledgment :Please acknowledge the use of these data with the following statement:  These data were provided by JPL under support by NASA MEaSUREs program.creator_name :JPL MUR SST projectcreator_email :ghrsst@podaac.jpl.nasa.govcreator_url :http://mur.jpl.nasa.govproject :NASA Making Earth Science Data Records for Use in Research Environments (MEaSUREs) Programpublisher_name :GHRSST Project Officepublisher_url :http://www.ghrsst.orgpublisher_email :ghrsst-po@nceo.ac.ukprocessing_level :L4cdm_data_type :grid\n\n\n\nds.sel(lat=slice(5, 25), lon=slice(60, 80))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:           (time: 59, lat: 2001, lon: 2001)\nCoordinates:\n  * time              (time) datetime64[ns] 2003-01-01T09:00:00 ... 2003-02-2...\n  * lat               (lat) float32 5.0 5.01 5.02 5.03 ... 24.98 24.99 25.0\n  * lon               (lon) float32 60.0 60.01 60.02 60.03 ... 79.98 79.99 80.0\nData variables:\n    analysed_sst      (time, lat, lon) float32 dask.array&lt;chunksize=(1, 2001, 2001), meta=np.ndarray&gt;\n    analysis_error    (time, lat, lon) float32 dask.array&lt;chunksize=(1, 2001, 2001), meta=np.ndarray&gt;\n    mask              (time, lat, lon) float32 dask.array&lt;chunksize=(1, 2001, 2001), meta=np.ndarray&gt;\n    sea_ice_fraction  (time, lat, lon) float32 dask.array&lt;chunksize=(1, 2001, 2001), meta=np.ndarray&gt;\nAttributes: (12/47)\n    Conventions:                CF-1.5\n    title:                      Daily MUR SST, Final product\n    summary:                    A merged, multi-sensor L4 Foundation SST anal...\n    references:                 http://podaac.jpl.nasa.gov/Multi-scale_Ultra-...\n    institution:                Jet Propulsion Laboratory\n    history:                    created at nominal 4-day latency; replaced nr...\n    ...                         ...\n    project:                    NASA Making Earth Science Data Records for Us...\n    publisher_name:             GHRSST Project Office\n    publisher_url:              http://www.ghrsst.org\n    publisher_email:            ghrsst-po@nceo.ac.uk\n    processing_level:           L4\n    cdm_data_type:              gridxarray.DatasetDimensions:time: 59lat: 2001lon: 2001Coordinates: (3)time(time)datetime64[ns]2003-01-01T09:00:00 ... 2003-02-...long_name :reference time of sst fieldstandard_name :timeaxis :Tcomment :Nominal time of analyzed fieldsarray(['2003-01-01T09:00:00.000000000', '2003-01-02T09:00:00.000000000',\n       '2003-01-03T09:00:00.000000000', '2003-01-04T09:00:00.000000000',\n       '2003-01-05T09:00:00.000000000', '2003-01-06T09:00:00.000000000',\n       '2003-01-07T09:00:00.000000000', '2003-01-08T09:00:00.000000000',\n       '2003-01-09T09:00:00.000000000', '2003-01-10T09:00:00.000000000',\n       '2003-01-11T09:00:00.000000000', '2003-01-12T09:00:00.000000000',\n       '2003-01-13T09:00:00.000000000', '2003-01-14T09:00:00.000000000',\n       '2003-01-15T09:00:00.000000000', '2003-01-16T09:00:00.000000000',\n       '2003-01-17T09:00:00.000000000', '2003-01-18T09:00:00.000000000',\n       '2003-01-19T09:00:00.000000000', '2003-01-20T09:00:00.000000000',\n       '2003-01-21T09:00:00.000000000', '2003-01-22T09:00:00.000000000',\n       '2003-01-23T09:00:00.000000000', '2003-01-24T09:00:00.000000000',\n       '2003-01-25T09:00:00.000000000', '2003-01-26T09:00:00.000000000',\n       '2003-01-27T09:00:00.000000000', '2003-01-28T09:00:00.000000000',\n       '2003-01-29T09:00:00.000000000', '2003-01-30T09:00:00.000000000',\n       '2003-01-31T09:00:00.000000000', '2003-02-01T09:00:00.000000000',\n       '2003-02-02T09:00:00.000000000', '2003-02-03T09:00:00.000000000',\n       '2003-02-04T09:00:00.000000000', '2003-02-05T09:00:00.000000000',\n       '2003-02-06T09:00:00.000000000', '2003-02-07T09:00:00.000000000',\n       '2003-02-08T09:00:00.000000000', '2003-02-09T09:00:00.000000000',\n       '2003-02-10T09:00:00.000000000', '2003-02-11T09:00:00.000000000',\n       '2003-02-12T09:00:00.000000000', '2003-02-13T09:00:00.000000000',\n       '2003-02-14T09:00:00.000000000', '2003-02-15T09:00:00.000000000',\n       '2003-02-16T09:00:00.000000000', '2003-02-17T09:00:00.000000000',\n       '2003-02-18T09:00:00.000000000', '2003-02-19T09:00:00.000000000',\n       '2003-02-20T09:00:00.000000000', '2003-02-21T09:00:00.000000000',\n       '2003-02-22T09:00:00.000000000', '2003-02-23T09:00:00.000000000',\n       '2003-02-24T09:00:00.000000000', '2003-02-25T09:00:00.000000000',\n       '2003-02-26T09:00:00.000000000', '2003-02-27T09:00:00.000000000',\n       '2003-02-28T09:00:00.000000000'], dtype='datetime64[ns]')lat(lat)float325.0 5.01 5.02 ... 24.98 24.99 25.0long_name :latitudestandard_name :latitudeaxis :Yunits :degrees_northvalid_min :-90.0valid_max :90.0comment :nonearray([ 5.  ,  5.01,  5.02, ..., 24.98, 24.99, 25.  ], dtype=float32)lon(lon)float3260.0 60.01 60.02 ... 79.99 80.0long_name :longitudestandard_name :longitudeaxis :Xunits :degrees_eastvalid_min :-180.0valid_max :180.0comment :nonearray([60.  , 60.01, 60.02, ..., 79.98, 79.99, 80.  ], dtype=float32)Data variables: (4)analysed_sst(time, lat, lon)float32dask.array&lt;chunksize=(1, 2001, 2001), meta=np.ndarray&gt;long_name :analysed sea surface temperaturestandard_name :sea_surface_foundation_temperatureunits :kelvinvalid_min :-32767valid_max :32767comment :\"Final\" version using Multi-Resolution Variational Analysis (MRVA) method for interpolationsource :AMSRE-REMSS, AVHRR_Pathfinder-PFV5.2-NODC_day, AVHRR_Pathfinder-PFV5.2-NODC_night, MODIS_A-JPL, MODIS_T-JPL, iQUAM-NOAA/NESDIS, Ice_Conc-OSISAF\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n901.17 MiB\n15.27 MiB\n\n\nShape\n(59, 2001, 2001)\n(1, 2001, 2001)\n\n\nDask graph\n59 chunks in 120 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nanalysis_error\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 2001, 2001), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nestimated error standard deviation of analysed_sst\n\nunits :\n\nkelvin\n\nvalid_min :\n\n0\n\nvalid_max :\n\n32767\n\ncomment :\n\nnone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n901.17 MiB\n15.27 MiB\n\n\nShape\n(59, 2001, 2001)\n(1, 2001, 2001)\n\n\nDask graph\n59 chunks in 120 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nmask\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 2001, 2001), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nsea/land field composite mask\n\nvalid_min :\n\n1\n\nvalid_max :\n\n31\n\nflag_masks :\n\n[ 1 2 4 8 16]\n\nflag_values :\n\n[ 1 2 5 9 13]\n\nflag_meanings :\n\n1=open-sea, 2=land, 5=open-lake, 9=open-sea with ice in the grid, 13=open-lake with ice in the grid\n\ncomment :\n\nmask can be used to further filter the data.\n\nsource :\n\nGMT \"grdlandmask\", ice flag from sea_ice_fraction data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n901.17 MiB\n15.27 MiB\n\n\nShape\n(59, 2001, 2001)\n(1, 2001, 2001)\n\n\nDask graph\n59 chunks in 120 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nsea_ice_fraction\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 2001, 2001), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nsea ice area fraction\n\nstandard_name :\n\nsea ice area fraction\n\nunits :\n\nfraction (between 0 and 1)\n\nvalid_min :\n\n0\n\nvalid_max :\n\n100\n\nsource :\n\nEUMETSAT OSI-SAF, copyright EUMETSAT\n\ncomment :\n\nice data interpolated by a nearest neighbor approach.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n901.17 MiB\n15.27 MiB\n\n\nShape\n(59, 2001, 2001)\n(1, 2001, 2001)\n\n\nDask graph\n59 chunks in 120 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nIndexes: (3)timePandasIndexPandasIndex(DatetimeIndex(['2003-01-01 09:00:00', '2003-01-02 09:00:00',\n               '2003-01-03 09:00:00', '2003-01-04 09:00:00',\n               '2003-01-05 09:00:00', '2003-01-06 09:00:00',\n               '2003-01-07 09:00:00', '2003-01-08 09:00:00',\n               '2003-01-09 09:00:00', '2003-01-10 09:00:00',\n               '2003-01-11 09:00:00', '2003-01-12 09:00:00',\n               '2003-01-13 09:00:00', '2003-01-14 09:00:00',\n               '2003-01-15 09:00:00', '2003-01-16 09:00:00',\n               '2003-01-17 09:00:00', '2003-01-18 09:00:00',\n               '2003-01-19 09:00:00', '2003-01-20 09:00:00',\n               '2003-01-21 09:00:00', '2003-01-22 09:00:00',\n               '2003-01-23 09:00:00', '2003-01-24 09:00:00',\n               '2003-01-25 09:00:00', '2003-01-26 09:00:00',\n               '2003-01-27 09:00:00', '2003-01-28 09:00:00',\n               '2003-01-29 09:00:00', '2003-01-30 09:00:00',\n               '2003-01-31 09:00:00', '2003-02-01 09:00:00',\n               '2003-02-02 09:00:00', '2003-02-03 09:00:00',\n               '2003-02-04 09:00:00', '2003-02-05 09:00:00',\n               '2003-02-06 09:00:00', '2003-02-07 09:00:00',\n               '2003-02-08 09:00:00', '2003-02-09 09:00:00',\n               '2003-02-10 09:00:00', '2003-02-11 09:00:00',\n               '2003-02-12 09:00:00', '2003-02-13 09:00:00',\n               '2003-02-14 09:00:00', '2003-02-15 09:00:00',\n               '2003-02-16 09:00:00', '2003-02-17 09:00:00',\n               '2003-02-18 09:00:00', '2003-02-19 09:00:00',\n               '2003-02-20 09:00:00', '2003-02-21 09:00:00',\n               '2003-02-22 09:00:00', '2003-02-23 09:00:00',\n               '2003-02-24 09:00:00', '2003-02-25 09:00:00',\n               '2003-02-26 09:00:00', '2003-02-27 09:00:00',\n               '2003-02-28 09:00:00'],\n              dtype='datetime64[ns]', name='time', freq=None))latPandasIndexPandasIndex(Index([               5.0,  5.010000228881836,  5.019999980926514,\n         5.03000020980835,  5.039999961853027,  5.050000190734863,\n        5.059999942779541,  5.070000171661377,  5.079999923706055,\n        5.090000152587891,\n       ...\n        24.90999984741211, 24.920000076293945,  24.93000030517578,\n       24.940000534057617, 24.950000762939453, 24.959999084472656,\n       24.969999313354492, 24.979999542236328, 24.989999771118164,\n                     25.0],\n      dtype='float32', name='lat', length=2001))lonPandasIndexPandasIndex(Index([              60.0,   60.0099983215332,  60.02000045776367,\n       60.029998779296875, 60.040000915527344,  60.04999923706055,\n       60.060001373291016,  60.06999969482422,  60.08000183105469,\n        60.09000015258789,\n       ...\n        79.91000366210938,  79.91999816894531,  79.93000030517578,\n        79.94000244140625,  79.94999694824219,  79.95999908447266,\n        79.97000122070312,   79.9800033569336,  79.98999786376953,\n                     80.0],\n      dtype='float32', name='lon', length=2001))Attributes: (47)Conventions :CF-1.5title :Daily MUR SST, Final productsummary :A merged, multi-sensor L4 Foundation SST analysis product from JPL.references :http://podaac.jpl.nasa.gov/Multi-scale_Ultra-high_Resolution_MUR-SSTinstitution :Jet Propulsion Laboratoryhistory :created at nominal 4-day latency; replaced nrt (1-day latency) version.comment :MUR = \"Multi-scale Ultra-high Reolution\"license :These data are available free of charge under data policy of JPL PO.DAAC.id :MUR-JPL-L4-GLOB-v04.1naming_authority :org.ghrsstproduct_version :04.1uuid :27665bc0-d5fc-11e1-9b23-0800200c9a66gds_version_id :2.0netcdf_version_id :4.1date_created :20150818T185930Zstart_time :20030101T090000Zstop_time :20030101T090000Ztime_coverage_start :20021231T210000Ztime_coverage_end :20030101T210000Zfile_quality_level :1source :AMSRE-REMSS, AVHRR_Pathfinder-PFV5.2-NODC_day, AVHRR_Pathfinder-PFV5.2-NODC_night, MODIS_A-JPL, MODIS_T-JPL, iQUAM-NOAA/NESDIS, Ice_Conc-OSISAFplatform :Aqua, DMSP, NOAA-POES, Suomi-NPP, Terrasensor :AMSR-E, AVHRR, MODIS, SSM/I, VIIRS, in-situMetadata_Conventions :Unidata Observation Dataset v1.0metadata_link :http://podaac.jpl.nasa.gov/ws/metadata/dataset/?format=iso&shortName=MUR-JPL-L4-GLOB-v04.1keywords :Oceans &gt; Ocean Temperature &gt; Sea Surface Temperaturekeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordsstandard_name_vocabulary :NetCDF Climate and Forecast (CF) Metadata Conventionsouthernmost_latitude :-90.0northernmost_latitude :90.0westernmost_longitude :-180.0easternmost_longitude :180.0spatial_resolution :0.01 degreesgeospatial_lat_units :degrees northgeospatial_lat_resolution :0.01 degreesgeospatial_lon_units :degrees eastgeospatial_lon_resolution :0.01 degreesacknowledgment :Please acknowledge the use of these data with the following statement:  These data were provided by JPL under support by NASA MEaSUREs program.creator_name :JPL MUR SST projectcreator_email :ghrsst@podaac.jpl.nasa.govcreator_url :http://mur.jpl.nasa.govproject :NASA Making Earth Science Data Records for Use in Research Environments (MEaSUREs) Programpublisher_name :GHRSST Project Officepublisher_url :http://www.ghrsst.orgpublisher_email :ghrsst-po@nceo.ac.ukprocessing_level :L4cdm_data_type :grid"
  },
  {
    "objectID": "python-notebooks/01_earthaccess_Downloader.html#download-granules-for-an-extended-period-of-time",
    "href": "python-notebooks/01_earthaccess_Downloader.html#download-granules-for-an-extended-period-of-time",
    "title": "NASA Earth Data",
    "section": "Download granules for an extended period of time",
    "text": "Download granules for an extended period of time\nSince you cannot slice data spatially, downloading granules is going to take up a lot of disk space if you only need data within a small bounding box. For our task, we wrote a simple function to slice data and wrote it into a new file.\nYou can consult the earthacess library website or their notebooks for code snippets on how to browse and look up collections. For this notebook, we mainly focus on the downloading aspect. First, we need to get the list of granules to download.\nSince earthacess does not support spatial slicing, we developed a method to download, slice, combine, and export data yearly, then finally delete temporary downloaded files to save disk space. Assumed that you already knew the temporal, spatial range of the dataset of your chosen, we first download the data by year into a temporary folder, then slice the data and then export the combined data to another folder.\n\n# Our approach\n# Month end not included\n\ndef download_and_combine_granules(short_name, month_start, month_end, lat1=5, lat2=25, lon1=60, lon2=80):\n    for month in pd.date_range(month_start, month_end, freq='M').strftime('%Y-%m-%d'):      \n        print('Collecting granules')\n        granules = earthaccess.granule_query().short_name(short_name).temporal(f'{month[:7]}-01',month).get(366)\n        \n        MAIN_FOLDER = 'demonstrated data/earth_data'\n        TEMP_FOLDER = 'temp'\n        path_temp_folder = os.path.join(MAIN_FOLDER, TEMP_FOLDER)\n        path_processed_folder = os.path.join(MAIN_FOLDER, short_name)\n        # create folder to store data\n        if not os.path.exists(path_temp_folder):\n            os.makedirs(path_temp_folder)\n        if not os.path.exists(path_processed_folder):\n            os.makedirs(path_processed_folder)\n        files = earthaccess.download(granules, path_temp_folder)\n       \n        \n        ## if dataset coordinates are slice-able, use:\n        print('Slicing...')\n        data = xr.open_mfdataset(f'{path_temp_folder}/*.nc').sel(lat=slice(lat1, lat2), lon=slice(lon1, lon2))\n    \n        \n        # combine files together \n        ## for some collections, coordinate names are 'lat' and 'lon' while their underlying indices are 'latitude' and 'longitude', respectively\n        ## may or may not be applicable for other datasets on the site.\n        ## get bounding box if not sliceable\n        ### lat1_idx, lat2_idx, lon1_idx, lon2_idx = get_bounding_box(os.path.join(path_temp_folder, first_file), lat1, lat2, lon1, lon2)\n        ### data = xr.open_mfdataset(f'{path_temp_folder}/*.nc').isel(latitude=slice(lat1_idx, lat2_idx+1), longitude=slice(lon1_idx, lon2_idx+1))\n        \n        data.to_netcdf(f'{path_processed_folder}/{month}.nc')\n        \n        # delete files in the temporary folder\n        print('Deleting temporary files...')\n        files = glob.glob(f'{path_temp_folder}/*.*')\n        for f in files:\n            os.remove(f)\n\ndef get_bounding_box(file_path, lat1=0, lat2=30, lon1=60, lon2=80):\n    \"\"\"\n    The dataset we experimented did not have indexed coordinates, \n    so we resorted to slicing using index positions\n    \"\"\"\n    ds = xr.open_dataset(file_path)\n    \n    lat_vals = ds.lat.values\n    lon_vals = ds.lon.values\n    \n    lat1_idx = np.abs(lat_vals-lat1).argmin()\n    lat2_idx = np.abs(lat_vals-lat2).argmin()\n    lon1_idx = np.abs(lon_vals-lon1).argmin()\n    lon2_idx = np.abs(lon_vals-lon1).argmin()\n    \n    return lat1_idx, lat2_idx, lon1_idx, lon2_idx\n\n\n# download and combine every month's worth of data\ndownload_and_combine_granules(short_name='MUR25-JPL-L4-GLOB-v04.2',\n                          month_start='2003-01', month_end='2003-03', \n                          lat1=5, lat2=25, lon1=60, lon2=80)\n\nCollecting granules\n Getting 31 granules, approx download size: 0.0 GB\nSlicing...\nDeleting temporary files...\nCollecting granules\n Getting 28 granules, approx download size: 0.0 GB\nSlicing...\nDeleting temporary files...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlist_of_missing_dates = [\"2003-01-04\", \"2003-01-18\", \"2003-02-18\"]\n\nfor date in list_of_missing_dates:\n    result = earthaccess.search_data(\n        short_name='MUR-JPL-L4-GLOB-v4.1',\n        cloud_hosted=True,\n        bounding_box = (60, 5, 80, 25),\n        temporal=(date, date)\n    )\n    \n    MAIN_FOLDER = 'demonstrated data/earth_data'\n    TEMP_FOLDER = 'temp'\n    path_temp_folder = os.path.join(MAIN_FOLDER, TEMP_FOLDER)\n    file = earthaccess.download(result, path_temp_folder)\n\nGranules found: 1\n Getting 1 granules, approx download size: 0.0 GB\nError while downloading the file 20030218090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc\nTraceback (most recent call last):\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/earthaccess/store.py\", line 488, in _download_file\n    r.raise_for_status()\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/requests/models.py\", line 960, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 502 Server Error: Bad Gateway for url: https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20030218090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc"
  },
  {
    "objectID": "python-notebooks/01_earthaccess_Downloader.html#combine-files-together",
    "href": "python-notebooks/01_earthaccess_Downloader.html#combine-files-together",
    "title": "NASA Earth Data",
    "section": "Combine files together",
    "text": "Combine files together\nNow that we have all netcdf4 files in one place, all spatially sliced, combining the rest of the data is a piece of cake! Note that some of the data will be overlap in the process of combing data every year earlier, so it‚Äôs best practice to remove duplicates (if any)\n\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:           (time: 59, lat: 17999, lon: 36000)\nCoordinates:\n  * time              (time) datetime64[ns] 2003-01-01T09:00:00 ... 2003-02-2...\n  * lat               (lat) float32 -89.99 -89.98 -89.97 ... 89.97 89.98 89.99\n  * lon               (lon) float32 -180.0 -180.0 -180.0 ... 180.0 180.0 180.0\nData variables:\n    analysed_sst      (time, lat, lon) float32 dask.array&lt;chunksize=(1, 17999, 36000), meta=np.ndarray&gt;\n    analysis_error    (time, lat, lon) float32 dask.array&lt;chunksize=(1, 17999, 36000), meta=np.ndarray&gt;\n    mask              (time, lat, lon) float32 dask.array&lt;chunksize=(1, 17999, 36000), meta=np.ndarray&gt;\n    sea_ice_fraction  (time, lat, lon) float32 dask.array&lt;chunksize=(1, 17999, 36000), meta=np.ndarray&gt;\nAttributes: (12/47)\n    Conventions:                CF-1.5\n    title:                      Daily MUR SST, Final product\n    summary:                    A merged, multi-sensor L4 Foundation SST anal...\n    references:                 http://podaac.jpl.nasa.gov/Multi-scale_Ultra-...\n    institution:                Jet Propulsion Laboratory\n    history:                    created at nominal 4-day latency; replaced nr...\n    ...                         ...\n    project:                    NASA Making Earth Science Data Records for Us...\n    publisher_name:             GHRSST Project Office\n    publisher_url:              http://www.ghrsst.org\n    publisher_email:            ghrsst-po@nceo.ac.uk\n    processing_level:           L4\n    cdm_data_type:              gridxarray.DatasetDimensions:time: 59lat: 17999lon: 36000Coordinates: (3)time(time)datetime64[ns]2003-01-01T09:00:00 ... 2003-02-...long_name :reference time of sst fieldstandard_name :timeaxis :Tcomment :Nominal time of analyzed fieldsarray(['2003-01-01T09:00:00.000000000', '2003-01-02T09:00:00.000000000',\n       '2003-01-03T09:00:00.000000000', '2003-01-04T09:00:00.000000000',\n       '2003-01-05T09:00:00.000000000', '2003-01-06T09:00:00.000000000',\n       '2003-01-07T09:00:00.000000000', '2003-01-08T09:00:00.000000000',\n       '2003-01-09T09:00:00.000000000', '2003-01-10T09:00:00.000000000',\n       '2003-01-11T09:00:00.000000000', '2003-01-12T09:00:00.000000000',\n       '2003-01-13T09:00:00.000000000', '2003-01-14T09:00:00.000000000',\n       '2003-01-15T09:00:00.000000000', '2003-01-16T09:00:00.000000000',\n       '2003-01-17T09:00:00.000000000', '2003-01-18T09:00:00.000000000',\n       '2003-01-19T09:00:00.000000000', '2003-01-20T09:00:00.000000000',\n       '2003-01-21T09:00:00.000000000', '2003-01-22T09:00:00.000000000',\n       '2003-01-23T09:00:00.000000000', '2003-01-24T09:00:00.000000000',\n       '2003-01-25T09:00:00.000000000', '2003-01-26T09:00:00.000000000',\n       '2003-01-27T09:00:00.000000000', '2003-01-28T09:00:00.000000000',\n       '2003-01-29T09:00:00.000000000', '2003-01-30T09:00:00.000000000',\n       '2003-01-31T09:00:00.000000000', '2003-02-01T09:00:00.000000000',\n       '2003-02-02T09:00:00.000000000', '2003-02-03T09:00:00.000000000',\n       '2003-02-04T09:00:00.000000000', '2003-02-05T09:00:00.000000000',\n       '2003-02-06T09:00:00.000000000', '2003-02-07T09:00:00.000000000',\n       '2003-02-08T09:00:00.000000000', '2003-02-09T09:00:00.000000000',\n       '2003-02-10T09:00:00.000000000', '2003-02-11T09:00:00.000000000',\n       '2003-02-12T09:00:00.000000000', '2003-02-13T09:00:00.000000000',\n       '2003-02-14T09:00:00.000000000', '2003-02-15T09:00:00.000000000',\n       '2003-02-16T09:00:00.000000000', '2003-02-17T09:00:00.000000000',\n       '2003-02-18T09:00:00.000000000', '2003-02-19T09:00:00.000000000',\n       '2003-02-20T09:00:00.000000000', '2003-02-21T09:00:00.000000000',\n       '2003-02-22T09:00:00.000000000', '2003-02-23T09:00:00.000000000',\n       '2003-02-24T09:00:00.000000000', '2003-02-25T09:00:00.000000000',\n       '2003-02-26T09:00:00.000000000', '2003-02-27T09:00:00.000000000',\n       '2003-02-28T09:00:00.000000000'], dtype='datetime64[ns]')lat(lat)float32-89.99 -89.98 ... 89.98 89.99long_name :latitudestandard_name :latitudeaxis :Yunits :degrees_northvalid_min :-90.0valid_max :90.0comment :nonearray([-89.99, -89.98, -89.97, ...,  89.97,  89.98,  89.99], dtype=float32)lon(lon)float32-180.0 -180.0 ... 180.0 180.0long_name :longitudestandard_name :longitudeaxis :Xunits :degrees_eastvalid_min :-180.0valid_max :180.0comment :nonearray([-179.99, -179.98, -179.97, ...,  179.98,  179.99,  180.  ],\n      dtype=float32)Data variables: (4)analysed_sst(time, lat, lon)float32dask.array&lt;chunksize=(1, 17999, 36000), meta=np.ndarray&gt;long_name :analysed sea surface temperaturestandard_name :sea_surface_foundation_temperatureunits :kelvinvalid_min :-32767valid_max :32767comment :\"Final\" version using Multi-Resolution Variational Analysis (MRVA) method for interpolationsource :AMSRE-REMSS, AVHRR_Pathfinder-PFV5.2-NODC_day, AVHRR_Pathfinder-PFV5.2-NODC_night, MODIS_A-JPL, MODIS_T-JPL, iQUAM-NOAA/NESDIS, Ice_Conc-OSISAF\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n142.42 GiB\n2.41 GiB\n\n\nShape\n(59, 17999, 36000)\n(1, 17999, 36000)\n\n\nDask graph\n59 chunks in 119 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nanalysis_error\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 17999, 36000), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nestimated error standard deviation of analysed_sst\n\nunits :\n\nkelvin\n\nvalid_min :\n\n0\n\nvalid_max :\n\n32767\n\ncomment :\n\nnone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n142.42 GiB\n2.41 GiB\n\n\nShape\n(59, 17999, 36000)\n(1, 17999, 36000)\n\n\nDask graph\n59 chunks in 119 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nmask\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 17999, 36000), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nsea/land field composite mask\n\nvalid_min :\n\n1\n\nvalid_max :\n\n31\n\nflag_masks :\n\n[ 1 2 4 8 16]\n\nflag_values :\n\n[ 1 2 5 9 13]\n\nflag_meanings :\n\n1=open-sea, 2=land, 5=open-lake, 9=open-sea with ice in the grid, 13=open-lake with ice in the grid\n\ncomment :\n\nmask can be used to further filter the data.\n\nsource :\n\nGMT \"grdlandmask\", ice flag from sea_ice_fraction data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n142.42 GiB\n2.41 GiB\n\n\nShape\n(59, 17999, 36000)\n(1, 17999, 36000)\n\n\nDask graph\n59 chunks in 119 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nsea_ice_fraction\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 17999, 36000), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nsea ice area fraction\n\nstandard_name :\n\nsea ice area fraction\n\nunits :\n\nfraction (between 0 and 1)\n\nvalid_min :\n\n0\n\nvalid_max :\n\n100\n\nsource :\n\nEUMETSAT OSI-SAF, copyright EUMETSAT\n\ncomment :\n\nice data interpolated by a nearest neighbor approach.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n142.42 GiB\n2.41 GiB\n\n\nShape\n(59, 17999, 36000)\n(1, 17999, 36000)\n\n\nDask graph\n59 chunks in 119 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nIndexes: (3)timePandasIndexPandasIndex(DatetimeIndex(['2003-01-01 09:00:00', '2003-01-02 09:00:00',\n               '2003-01-03 09:00:00', '2003-01-04 09:00:00',\n               '2003-01-05 09:00:00', '2003-01-06 09:00:00',\n               '2003-01-07 09:00:00', '2003-01-08 09:00:00',\n               '2003-01-09 09:00:00', '2003-01-10 09:00:00',\n               '2003-01-11 09:00:00', '2003-01-12 09:00:00',\n               '2003-01-13 09:00:00', '2003-01-14 09:00:00',\n               '2003-01-15 09:00:00', '2003-01-16 09:00:00',\n               '2003-01-17 09:00:00', '2003-01-18 09:00:00',\n               '2003-01-19 09:00:00', '2003-01-20 09:00:00',\n               '2003-01-21 09:00:00', '2003-01-22 09:00:00',\n               '2003-01-23 09:00:00', '2003-01-24 09:00:00',\n               '2003-01-25 09:00:00', '2003-01-26 09:00:00',\n               '2003-01-27 09:00:00', '2003-01-28 09:00:00',\n               '2003-01-29 09:00:00', '2003-01-30 09:00:00',\n               '2003-01-31 09:00:00', '2003-02-01 09:00:00',\n               '2003-02-02 09:00:00', '2003-02-03 09:00:00',\n               '2003-02-04 09:00:00', '2003-02-05 09:00:00',\n               '2003-02-06 09:00:00', '2003-02-07 09:00:00',\n               '2003-02-08 09:00:00', '2003-02-09 09:00:00',\n               '2003-02-10 09:00:00', '2003-02-11 09:00:00',\n               '2003-02-12 09:00:00', '2003-02-13 09:00:00',\n               '2003-02-14 09:00:00', '2003-02-15 09:00:00',\n               '2003-02-16 09:00:00', '2003-02-17 09:00:00',\n               '2003-02-18 09:00:00', '2003-02-19 09:00:00',\n               '2003-02-20 09:00:00', '2003-02-21 09:00:00',\n               '2003-02-22 09:00:00', '2003-02-23 09:00:00',\n               '2003-02-24 09:00:00', '2003-02-25 09:00:00',\n               '2003-02-26 09:00:00', '2003-02-27 09:00:00',\n               '2003-02-28 09:00:00'],\n              dtype='datetime64[ns]', name='time', freq=None))latPandasIndexPandasIndex(Index([-89.98999786376953,  -89.9800033569336, -89.97000122070312,\n       -89.95999908447266, -89.94999694824219, -89.94000244140625,\n       -89.93000030517578, -89.91999816894531, -89.91000366210938,\n        -89.9000015258789,\n       ...\n         89.9000015258789,  89.91000366210938,  89.91999816894531,\n        89.93000030517578,  89.94000244140625,  89.94999694824219,\n        89.95999908447266,  89.97000122070312,   89.9800033569336,\n        89.98999786376953],\n      dtype='float32', name='lat', length=17999))lonPandasIndexPandasIndex(Index([-179.99000549316406, -179.97999572753906, -179.97000122070312,\n        -179.9600067138672,  -179.9499969482422, -179.94000244140625,\n       -179.92999267578125,  -179.9199981689453, -179.91000366210938,\n       -179.89999389648438,\n       ...\n        179.91000366210938,   179.9199981689453,  179.92999267578125,\n        179.94000244140625,   179.9499969482422,   179.9600067138672,\n        179.97000122070312,  179.97999572753906,  179.99000549316406,\n                     180.0],\n      dtype='float32', name='lon', length=36000))Attributes: (47)Conventions :CF-1.5title :Daily MUR SST, Final productsummary :A merged, multi-sensor L4 Foundation SST analysis product from JPL.references :http://podaac.jpl.nasa.gov/Multi-scale_Ultra-high_Resolution_MUR-SSTinstitution :Jet Propulsion Laboratoryhistory :created at nominal 4-day latency; replaced nrt (1-day latency) version.comment :MUR = \"Multi-scale Ultra-high Reolution\"license :These data are available free of charge under data policy of JPL PO.DAAC.id :MUR-JPL-L4-GLOB-v04.1naming_authority :org.ghrsstproduct_version :04.1uuid :27665bc0-d5fc-11e1-9b23-0800200c9a66gds_version_id :2.0netcdf_version_id :4.1date_created :20150818T185930Zstart_time :20030101T090000Zstop_time :20030101T090000Ztime_coverage_start :20021231T210000Ztime_coverage_end :20030101T210000Zfile_quality_level :1source :AMSRE-REMSS, AVHRR_Pathfinder-PFV5.2-NODC_day, AVHRR_Pathfinder-PFV5.2-NODC_night, MODIS_A-JPL, MODIS_T-JPL, iQUAM-NOAA/NESDIS, Ice_Conc-OSISAFplatform :Aqua, DMSP, NOAA-POES, Suomi-NPP, Terrasensor :AMSR-E, AVHRR, MODIS, SSM/I, VIIRS, in-situMetadata_Conventions :Unidata Observation Dataset v1.0metadata_link :http://podaac.jpl.nasa.gov/ws/metadata/dataset/?format=iso&shortName=MUR-JPL-L4-GLOB-v04.1keywords :Oceans &gt; Ocean Temperature &gt; Sea Surface Temperaturekeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordsstandard_name_vocabulary :NetCDF Climate and Forecast (CF) Metadata Conventionsouthernmost_latitude :-90.0northernmost_latitude :90.0westernmost_longitude :-180.0easternmost_longitude :180.0spatial_resolution :0.01 degreesgeospatial_lat_units :degrees northgeospatial_lat_resolution :0.01 degreesgeospatial_lon_units :degrees eastgeospatial_lon_resolution :0.01 degreesacknowledgment :Please acknowledge the use of these data with the following statement:  These data were provided by JPL under support by NASA MEaSUREs program.creator_name :JPL MUR SST projectcreator_email :ghrsst@podaac.jpl.nasa.govcreator_url :http://mur.jpl.nasa.govproject :NASA Making Earth Science Data Records for Use in Research Environments (MEaSUREs) Programpublisher_name :GHRSST Project Officepublisher_url :http://www.ghrsst.orgpublisher_email :ghrsst-po@nceo.ac.ukprocessing_level :L4cdm_data_type :grid\n\n\n\ncombined = xr.open_mfdataset('demonstrated data/earth_data/MUR25-JPL-L4-GLOB-v04.2/*.nc')\n\ncombined\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:           (time: 56, lat: 80, lon: 80)\nCoordinates:\n  * time              (time) datetime64[ns] 2003-01-01T09:00:00 ... 2003-02-2...\n  * lat               (lat) float32 5.125 5.375 5.625 ... 24.38 24.62 24.88\n  * lon               (lon) float32 60.12 60.38 60.62 ... 79.38 79.62 79.88\nData variables:\n    analysed_sst      (time, lat, lon) float32 dask.array&lt;chunksize=(29, 80, 80), meta=np.ndarray&gt;\n    analysis_error    (time, lat, lon) float32 dask.array&lt;chunksize=(29, 80, 80), meta=np.ndarray&gt;\n    mask              (time, lat, lon) float32 dask.array&lt;chunksize=(29, 80, 80), meta=np.ndarray&gt;\n    sea_ice_fraction  (time, lat, lon) float32 dask.array&lt;chunksize=(29, 80, 80), meta=np.ndarray&gt;\n    sst_anomaly       (time, lat, lon) float32 dask.array&lt;chunksize=(29, 80, 80), meta=np.ndarray&gt;\nAttributes: (12/54)\n    Conventions:                CF-1.7, ACDD-1.3\n    title:                      Daily 0.25-degree MUR SST, Final product\n    summary:                    A low-resolution version of the MUR SST analy...\n    keywords:                   Oceans &gt; Ocean Temperature &gt; Sea Surface Temp...\n    keywords_vocabulary:        NASA Global Change Master Directory (GCMD) Sc...\n    standard_name_vocabulary:   NetCDF Climate and Forecast (CF) Metadata Con...\n    ...                         ...\n    publisher_name:             GHRSST Project Office\n    publisher_url:              https://www.ghrsst.org\n    publisher_email:            gpc@ghrsst.org\n    file_quality_level:         3\n    metadata_link:              http://podaac.jpl.nasa.gov/ws/metadata/datase...\n    acknowledgment:             Please acknowledge the use of these data with...xarray.DatasetDimensions:time: 56lat: 80lon: 80Coordinates: (3)time(time)datetime64[ns]2003-01-01T09:00:00 ... 2003-02-...long_name :reference time of sst fieldstandard_name :timecoverage_content_type :coordinateaxis :Tcomment :Nominal time of analyzed fieldsarray(['2003-01-01T09:00:00.000000000', '2003-01-02T09:00:00.000000000',\n       '2003-01-03T09:00:00.000000000', '2003-01-05T09:00:00.000000000',\n       '2003-01-06T09:00:00.000000000', '2003-01-07T09:00:00.000000000',\n       '2003-01-08T09:00:00.000000000', '2003-01-09T09:00:00.000000000',\n       '2003-01-10T09:00:00.000000000', '2003-01-11T09:00:00.000000000',\n       '2003-01-12T09:00:00.000000000', '2003-01-13T09:00:00.000000000',\n       '2003-01-14T09:00:00.000000000', '2003-01-15T09:00:00.000000000',\n       '2003-01-16T09:00:00.000000000', '2003-01-17T09:00:00.000000000',\n       '2003-01-19T09:00:00.000000000', '2003-01-20T09:00:00.000000000',\n       '2003-01-21T09:00:00.000000000', '2003-01-22T09:00:00.000000000',\n       '2003-01-23T09:00:00.000000000', '2003-01-24T09:00:00.000000000',\n       '2003-01-25T09:00:00.000000000', '2003-01-26T09:00:00.000000000',\n       '2003-01-27T09:00:00.000000000', '2003-01-28T09:00:00.000000000',\n       '2003-01-29T09:00:00.000000000', '2003-01-30T09:00:00.000000000',\n       '2003-01-31T09:00:00.000000000', '2003-02-01T09:00:00.000000000',\n       '2003-02-02T09:00:00.000000000', '2003-02-03T09:00:00.000000000',\n       '2003-02-04T09:00:00.000000000', '2003-02-05T09:00:00.000000000',\n       '2003-02-06T09:00:00.000000000', '2003-02-07T09:00:00.000000000',\n       '2003-02-08T09:00:00.000000000', '2003-02-09T09:00:00.000000000',\n       '2003-02-10T09:00:00.000000000', '2003-02-11T09:00:00.000000000',\n       '2003-02-12T09:00:00.000000000', '2003-02-13T09:00:00.000000000',\n       '2003-02-14T09:00:00.000000000', '2003-02-15T09:00:00.000000000',\n       '2003-02-16T09:00:00.000000000', '2003-02-17T09:00:00.000000000',\n       '2003-02-19T09:00:00.000000000', '2003-02-20T09:00:00.000000000',\n       '2003-02-21T09:00:00.000000000', '2003-02-22T09:00:00.000000000',\n       '2003-02-23T09:00:00.000000000', '2003-02-24T09:00:00.000000000',\n       '2003-02-25T09:00:00.000000000', '2003-02-26T09:00:00.000000000',\n       '2003-02-27T09:00:00.000000000', '2003-02-28T09:00:00.000000000'],\n      dtype='datetime64[ns]')lat(lat)float325.125 5.375 5.625 ... 24.62 24.88long_name :latitudestandard_name :latitudecoverage_content_type :coordinateaxis :Yunits :degrees_northvalid_min :-90.0valid_max :90.0comment :geolocations inherited from the input data without correctionarray([ 5.125,  5.375,  5.625,  5.875,  6.125,  6.375,  6.625,  6.875,  7.125,\n        7.375,  7.625,  7.875,  8.125,  8.375,  8.625,  8.875,  9.125,  9.375,\n        9.625,  9.875, 10.125, 10.375, 10.625, 10.875, 11.125, 11.375, 11.625,\n       11.875, 12.125, 12.375, 12.625, 12.875, 13.125, 13.375, 13.625, 13.875,\n       14.125, 14.375, 14.625, 14.875, 15.125, 15.375, 15.625, 15.875, 16.125,\n       16.375, 16.625, 16.875, 17.125, 17.375, 17.625, 17.875, 18.125, 18.375,\n       18.625, 18.875, 19.125, 19.375, 19.625, 19.875, 20.125, 20.375, 20.625,\n       20.875, 21.125, 21.375, 21.625, 21.875, 22.125, 22.375, 22.625, 22.875,\n       23.125, 23.375, 23.625, 23.875, 24.125, 24.375, 24.625, 24.875],\n      dtype=float32)lon(lon)float3260.12 60.38 60.62 ... 79.62 79.88long_name :longitudestandard_name :longitudecoverage_content_type :coordinateaxis :Xunits :degrees_eastvalid_min :-180.0valid_max :180.0comment :geolocations inherited from the input data without correctionarray([60.125, 60.375, 60.625, 60.875, 61.125, 61.375, 61.625, 61.875, 62.125,\n       62.375, 62.625, 62.875, 63.125, 63.375, 63.625, 63.875, 64.125, 64.375,\n       64.625, 64.875, 65.125, 65.375, 65.625, 65.875, 66.125, 66.375, 66.625,\n       66.875, 67.125, 67.375, 67.625, 67.875, 68.125, 68.375, 68.625, 68.875,\n       69.125, 69.375, 69.625, 69.875, 70.125, 70.375, 70.625, 70.875, 71.125,\n       71.375, 71.625, 71.875, 72.125, 72.375, 72.625, 72.875, 73.125, 73.375,\n       73.625, 73.875, 74.125, 74.375, 74.625, 74.875, 75.125, 75.375, 75.625,\n       75.875, 76.125, 76.375, 76.625, 76.875, 77.125, 77.375, 77.625, 77.875,\n       78.125, 78.375, 78.625, 78.875, 79.125, 79.375, 79.625, 79.875],\n      dtype=float32)Data variables: (5)analysed_sst(time, lat, lon)float32dask.array&lt;chunksize=(29, 80, 80), meta=np.ndarray&gt;long_name :analysed sea surface temperaturestandard_name :sea_surface_foundation_temperaturecoverage_content_type :physicalMeasurementunits :kelvinvalid_min :-32767valid_max :32767comment :\"Final\" version using Multi-Resolution Variational Analysis (MRVA) method for interpolationsource :MODIS_T-JPL, MODIS_A-JPL, AMSRE-REMSS, AVHRR17_G-ACSPO, iQUAM-NOAA/NESDIS, Ice_Conc-OSISAF\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.37 MiB\n725.00 kiB\n\n\nShape\n(56, 80, 80)\n(29, 80, 80)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nanalysis_error\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 80, 80), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nestimated error standard deviation of analysed_sst\n\ncoverage_content_type :\n\nqualityInformation\n\nunits :\n\nkelvin\n\nvalid_min :\n\n0\n\nvalid_max :\n\n32767\n\ncomment :\n\nuncertainty in \"analysed_sst\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.37 MiB\n725.00 kiB\n\n\nShape\n(56, 80, 80)\n(29, 80, 80)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nmask\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 80, 80), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nsea/land field composite mask\n\ncoverage_content_type :\n\nreferenceInformation\n\nvalid_min :\n\n1\n\nvalid_max :\n\n31\n\nflag_masks :\n\n[ 1 2 4 8 16]\n\nflag_meanings :\n\nwater land optional_lake_surface sea_ice optional_river_surface\n\ncomment :\n\nflag interpretation as integer values: 1=water, 2=land, 5=lake, 9=water with ice in the grid, 13=lake with ice in the grid, 17=river\n\nsource :\n\nGMT \"grdlandmask\", ice flag from sea_ice_fraction data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.37 MiB\n725.00 kiB\n\n\nShape\n(56, 80, 80)\n(29, 80, 80)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nsea_ice_fraction\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 80, 80), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nsea ice area fraction\n\nstandard_name :\n\nsea_ice_area_fraction\n\ncoverage_content_type :\n\nauxiliaryInformation\n\nvalid_min :\n\n0\n\nvalid_max :\n\n100\n\nsource :\n\nEUMETSAT OSI-SAF, copyright EUMETSAT\n\ncomment :\n\nice fraction is a dimensionless quantity between 0 and 1; it has been interpolated by a nearest neighbor approach.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.37 MiB\n725.00 kiB\n\n\nShape\n(56, 80, 80)\n(29, 80, 80)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nsst_anomaly\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 80, 80), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nSST anomaly from a seasonal SST climatology based on the MUR data over 2003-2014 period\n\ncoverage_content_type :\n\nauxiliaryInformation\n\nunits :\n\nkelvin\n\nvalid_min :\n\n-32767\n\nvalid_max :\n\n32767\n\ncomment :\n\nanomaly reference to the day-of-year average between 2003 and 2014\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.37 MiB\n725.00 kiB\n\n\nShape\n(56, 80, 80)\n(29, 80, 80)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nIndexes: (3)timePandasIndexPandasIndex(DatetimeIndex(['2003-01-01 09:00:00', '2003-01-02 09:00:00',\n               '2003-01-03 09:00:00', '2003-01-05 09:00:00',\n               '2003-01-06 09:00:00', '2003-01-07 09:00:00',\n               '2003-01-08 09:00:00', '2003-01-09 09:00:00',\n               '2003-01-10 09:00:00', '2003-01-11 09:00:00',\n               '2003-01-12 09:00:00', '2003-01-13 09:00:00',\n               '2003-01-14 09:00:00', '2003-01-15 09:00:00',\n               '2003-01-16 09:00:00', '2003-01-17 09:00:00',\n               '2003-01-19 09:00:00', '2003-01-20 09:00:00',\n               '2003-01-21 09:00:00', '2003-01-22 09:00:00',\n               '2003-01-23 09:00:00', '2003-01-24 09:00:00',\n               '2003-01-25 09:00:00', '2003-01-26 09:00:00',\n               '2003-01-27 09:00:00', '2003-01-28 09:00:00',\n               '2003-01-29 09:00:00', '2003-01-30 09:00:00',\n               '2003-01-31 09:00:00', '2003-02-01 09:00:00',\n               '2003-02-02 09:00:00', '2003-02-03 09:00:00',\n               '2003-02-04 09:00:00', '2003-02-05 09:00:00',\n               '2003-02-06 09:00:00', '2003-02-07 09:00:00',\n               '2003-02-08 09:00:00', '2003-02-09 09:00:00',\n               '2003-02-10 09:00:00', '2003-02-11 09:00:00',\n               '2003-02-12 09:00:00', '2003-02-13 09:00:00',\n               '2003-02-14 09:00:00', '2003-02-15 09:00:00',\n               '2003-02-16 09:00:00', '2003-02-17 09:00:00',\n               '2003-02-19 09:00:00', '2003-02-20 09:00:00',\n               '2003-02-21 09:00:00', '2003-02-22 09:00:00',\n               '2003-02-23 09:00:00', '2003-02-24 09:00:00',\n               '2003-02-25 09:00:00', '2003-02-26 09:00:00',\n               '2003-02-27 09:00:00', '2003-02-28 09:00:00'],\n              dtype='datetime64[ns]', name='time', freq=None))latPandasIndexPandasIndex(Index([ 5.125,  5.375,  5.625,  5.875,  6.125,  6.375,  6.625,  6.875,  7.125,\n        7.375,  7.625,  7.875,  8.125,  8.375,  8.625,  8.875,  9.125,  9.375,\n        9.625,  9.875, 10.125, 10.375, 10.625, 10.875, 11.125, 11.375, 11.625,\n       11.875, 12.125, 12.375, 12.625, 12.875, 13.125, 13.375, 13.625, 13.875,\n       14.125, 14.375, 14.625, 14.875, 15.125, 15.375, 15.625, 15.875, 16.125,\n       16.375, 16.625, 16.875, 17.125, 17.375, 17.625, 17.875, 18.125, 18.375,\n       18.625, 18.875, 19.125, 19.375, 19.625, 19.875, 20.125, 20.375, 20.625,\n       20.875, 21.125, 21.375, 21.625, 21.875, 22.125, 22.375, 22.625, 22.875,\n       23.125, 23.375, 23.625, 23.875, 24.125, 24.375, 24.625, 24.875],\n      dtype='float32', name='lat'))lonPandasIndexPandasIndex(Index([60.125, 60.375, 60.625, 60.875, 61.125, 61.375, 61.625, 61.875, 62.125,\n       62.375, 62.625, 62.875, 63.125, 63.375, 63.625, 63.875, 64.125, 64.375,\n       64.625, 64.875, 65.125, 65.375, 65.625, 65.875, 66.125, 66.375, 66.625,\n       66.875, 67.125, 67.375, 67.625, 67.875, 68.125, 68.375, 68.625, 68.875,\n       69.125, 69.375, 69.625, 69.875, 70.125, 70.375, 70.625, 70.875, 71.125,\n       71.375, 71.625, 71.875, 72.125, 72.375, 72.625, 72.875, 73.125, 73.375,\n       73.625, 73.875, 74.125, 74.375, 74.625, 74.875, 75.125, 75.375, 75.625,\n       75.875, 76.125, 76.375, 76.625, 76.875, 77.125, 77.375, 77.625, 77.875,\n       78.125, 78.375, 78.625, 78.875, 79.125, 79.375, 79.625, 79.875],\n      dtype='float32', name='lon'))Attributes: (54)Conventions :CF-1.7, ACDD-1.3title :Daily 0.25-degree MUR SST, Final productsummary :A low-resolution version of the MUR SST analysis, a merged, multi-sensor L4 Foundation SST analysis product from JPL.keywords :Oceans &gt; Ocean Temperature &gt; Sea Surface Temperaturekeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordsstandard_name_vocabulary :NetCDF Climate and Forecast (CF) Metadata Conventionhistory :created at nominal 4-day latency; replaced nrt (1-day latency) version.source :MODIS_T-JPL, MODIS_A-JPL, AMSRE-REMSS, AVHRR17_G-ACSPO, iQUAM-NOAA/NESDIS, Ice_Conc-OSISAFplatform :Terra, Aqua, NOAA-17, Buoys/Shipsinstrument :MODIS, AMSR-E, AVHRR, in-situsensor :MODIS, AMSR-E, AVHRR, in-situprocessing_level :L4cdm_data_type :gridproduct_version :04.2references :Chin et al. (2017) \"Remote Sensing of Environment\", volulme 200, pages 154-169.  http://dx.doi.org/10.1016/j.rse.2017.07.029creator_name :JPL MUR SST projectcreator_email :ghrsst@podaac.jpl.nasa.govcreator_url :http://mur.jpl.nasa.govcreator_institution :Jet Propulsion Laboratoryinstitution :Jet Propulsion Laboratoryproject :NASA MEaSUREs and COVERAGEprogram :NASA Earth Science Data and Information System (ESDIS)southernmost_latitude :-90.0northernmost_latitude :90.0westernmost_longitude :-180.0easternmost_longitude :180.0geospatial_lat_min :-90.0geospatial_lat_max :90.0geospatial_lon_min :-180.0geospatial_lon_max :180.0geospatial_lat_units :degrees northgeospatial_lat_resolution :0.25geospatial_lon_units :degrees eastgeospatial_lon_resolution :0.25date_created :2019-08-28start_time :20030101T090000Zstop_time :20030101T090000Ztime_coverage_start :20021231T210000Ztime_coverage_end :20030101T210000Ztime_coverage_resolution :P1Dlicense :These data are available free of charge under data policy of JPL PO.DAAC.id :MUR25-JPL-L4-GLOB-v04.2uuid :27665bc0-d5fc-11e1-9b23-0800200c9a66comment :MUR = \"Multi-scale Ultra-high Resolution\"naming_authority :org.ghrsstgds_version_id :2.0netcdf_version_id :04.2spatial_resolution :0.25 degreespublisher_name :GHRSST Project Officepublisher_url :https://www.ghrsst.orgpublisher_email :gpc@ghrsst.orgfile_quality_level :3metadata_link :http://podaac.jpl.nasa.gov/ws/metadata/dataset/?format=iso&shortName=MUR25-JPL-L4-GLOB-v04.2acknowledgment :Please acknowledge the use of these data with the following statement:  These data were provided by JPL under support by NASA MEaSUREs and COVERAGE programs.\n\n\nYou can see that we do not need all variables: let‚Äôs take a look into each and see what we need.\nThe data is ‚Äúchunked‚Äù (broke down into smaller, more mangaeable pieces to eliminate duplicated copies of repeating data on storage). Therefore you can‚Äôt see the actual data if you clicking on the interactive panel above, as it is lazy-loaded to only show the general information of the whole object.\nSimply load the dataset to see the actual underlying data values\n\n# load sst_anomaly to see underlying data\ncombined.sst_anomaly.compute()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'sst_anomaly' (time: 56, lat: 80, lon: 80)&gt;\narray([[[-0.086     , -0.02      , -0.026     , ..., -0.252     ,\n         -0.49400002, -0.499     ],\n        [-0.03      , -0.24300002, -0.163     , ..., -0.344     ,\n         -0.416     , -0.38200003],\n        [-0.41700003, -0.37100002, -0.082     , ...,  0.1       ,\n         -0.066     , -0.1       ],\n        ...,\n        [-0.628     , -1.077     , -1.2720001 , ...,         nan,\n                 nan,         nan],\n        [-0.39000002, -0.564     , -0.80500007, ...,         nan,\n                 nan,         nan],\n        [-0.75600004, -0.675     , -0.53900003, ...,         nan,\n                 nan,         nan]],\n\n       [[-0.046     , -0.06900001, -0.089     , ..., -0.333     ,\n         -0.51000005, -0.60400003],\n        [-0.177     , -0.379     , -0.277     , ..., -0.34300002,\n         -0.418     , -0.43100002],\n        [-0.358     , -0.38700002, -0.16600001, ..., -0.026     ,\n         -0.158     , -0.12900001],\n...\n        [ 0.43300003,  0.48000002,  0.633     , ...,         nan,\n                 nan,         nan],\n        [ 0.439     ,  0.54200006,  0.51900005, ...,         nan,\n                 nan,         nan],\n        [ 0.642     ,  0.637     ,  0.609     , ...,         nan,\n                 nan,         nan]],\n\n       [[ 0.194     ,  0.432     ,  0.31      , ..., -0.081     ,\n          0.162     ,  0.14      ],\n        [ 0.147     ,  0.20400001,  0.164     , ...,  0.035     ,\n          0.32500002,  0.094     ],\n        [ 0.194     ,  0.216     ,  0.11300001, ...,  0.24700001,\n          0.33900002,  0.231     ],\n        ...,\n        [-0.085     ,  0.098     ,  0.23200001, ...,         nan,\n                 nan,         nan],\n        [-0.156     , -0.20700002, -0.014     , ...,         nan,\n                 nan,         nan],\n        [-0.037     , -0.07      , -0.012     , ...,         nan,\n                 nan,         nan]]], dtype=float32)\nCoordinates:\n  * time     (time) datetime64[ns] 2003-01-01T09:00:00 ... 2003-02-28T09:00:00\n  * lat      (lat) float32 5.125 5.375 5.625 5.875 ... 24.12 24.38 24.62 24.88\n  * lon      (lon) float32 60.12 60.38 60.62 60.88 ... 79.12 79.38 79.62 79.88\nAttributes:\n    long_name:              SST anomaly from a seasonal SST climatology based...\n    coverage_content_type:  auxiliaryInformation\n    units:                  kelvin\n    valid_min:              -32767\n    valid_max:              32767\n    comment:                anomaly reference to the day-of-year average betw...xarray.DataArray'sst_anomaly'time: 56lat: 80lon: 80-0.086 -0.02 -0.026 -0.038 0.026 0.099 ... nan nan nan nan nan nanarray([[[-0.086     , -0.02      , -0.026     , ..., -0.252     ,\n         -0.49400002, -0.499     ],\n        [-0.03      , -0.24300002, -0.163     , ..., -0.344     ,\n         -0.416     , -0.38200003],\n        [-0.41700003, -0.37100002, -0.082     , ...,  0.1       ,\n         -0.066     , -0.1       ],\n        ...,\n        [-0.628     , -1.077     , -1.2720001 , ...,         nan,\n                 nan,         nan],\n        [-0.39000002, -0.564     , -0.80500007, ...,         nan,\n                 nan,         nan],\n        [-0.75600004, -0.675     , -0.53900003, ...,         nan,\n                 nan,         nan]],\n\n       [[-0.046     , -0.06900001, -0.089     , ..., -0.333     ,\n         -0.51000005, -0.60400003],\n        [-0.177     , -0.379     , -0.277     , ..., -0.34300002,\n         -0.418     , -0.43100002],\n        [-0.358     , -0.38700002, -0.16600001, ..., -0.026     ,\n         -0.158     , -0.12900001],\n...\n        [ 0.43300003,  0.48000002,  0.633     , ...,         nan,\n                 nan,         nan],\n        [ 0.439     ,  0.54200006,  0.51900005, ...,         nan,\n                 nan,         nan],\n        [ 0.642     ,  0.637     ,  0.609     , ...,         nan,\n                 nan,         nan]],\n\n       [[ 0.194     ,  0.432     ,  0.31      , ..., -0.081     ,\n          0.162     ,  0.14      ],\n        [ 0.147     ,  0.20400001,  0.164     , ...,  0.035     ,\n          0.32500002,  0.094     ],\n        [ 0.194     ,  0.216     ,  0.11300001, ...,  0.24700001,\n          0.33900002,  0.231     ],\n        ...,\n        [-0.085     ,  0.098     ,  0.23200001, ...,         nan,\n                 nan,         nan],\n        [-0.156     , -0.20700002, -0.014     , ...,         nan,\n                 nan,         nan],\n        [-0.037     , -0.07      , -0.012     , ...,         nan,\n                 nan,         nan]]], dtype=float32)Coordinates: (3)time(time)datetime64[ns]2003-01-01T09:00:00 ... 2003-02-...long_name :reference time of sst fieldstandard_name :timecoverage_content_type :coordinateaxis :Tcomment :Nominal time of analyzed fieldsarray(['2003-01-01T09:00:00.000000000', '2003-01-02T09:00:00.000000000',\n       '2003-01-03T09:00:00.000000000', '2003-01-05T09:00:00.000000000',\n       '2003-01-06T09:00:00.000000000', '2003-01-07T09:00:00.000000000',\n       '2003-01-08T09:00:00.000000000', '2003-01-09T09:00:00.000000000',\n       '2003-01-10T09:00:00.000000000', '2003-01-11T09:00:00.000000000',\n       '2003-01-12T09:00:00.000000000', '2003-01-13T09:00:00.000000000',\n       '2003-01-14T09:00:00.000000000', '2003-01-15T09:00:00.000000000',\n       '2003-01-16T09:00:00.000000000', '2003-01-17T09:00:00.000000000',\n       '2003-01-19T09:00:00.000000000', '2003-01-20T09:00:00.000000000',\n       '2003-01-21T09:00:00.000000000', '2003-01-22T09:00:00.000000000',\n       '2003-01-23T09:00:00.000000000', '2003-01-24T09:00:00.000000000',\n       '2003-01-25T09:00:00.000000000', '2003-01-26T09:00:00.000000000',\n       '2003-01-27T09:00:00.000000000', '2003-01-28T09:00:00.000000000',\n       '2003-01-29T09:00:00.000000000', '2003-01-30T09:00:00.000000000',\n       '2003-01-31T09:00:00.000000000', '2003-02-01T09:00:00.000000000',\n       '2003-02-02T09:00:00.000000000', '2003-02-03T09:00:00.000000000',\n       '2003-02-04T09:00:00.000000000', '2003-02-05T09:00:00.000000000',\n       '2003-02-06T09:00:00.000000000', '2003-02-07T09:00:00.000000000',\n       '2003-02-08T09:00:00.000000000', '2003-02-09T09:00:00.000000000',\n       '2003-02-10T09:00:00.000000000', '2003-02-11T09:00:00.000000000',\n       '2003-02-12T09:00:00.000000000', '2003-02-13T09:00:00.000000000',\n       '2003-02-14T09:00:00.000000000', '2003-02-15T09:00:00.000000000',\n       '2003-02-16T09:00:00.000000000', '2003-02-17T09:00:00.000000000',\n       '2003-02-19T09:00:00.000000000', '2003-02-20T09:00:00.000000000',\n       '2003-02-21T09:00:00.000000000', '2003-02-22T09:00:00.000000000',\n       '2003-02-23T09:00:00.000000000', '2003-02-24T09:00:00.000000000',\n       '2003-02-25T09:00:00.000000000', '2003-02-26T09:00:00.000000000',\n       '2003-02-27T09:00:00.000000000', '2003-02-28T09:00:00.000000000'],\n      dtype='datetime64[ns]')lat(lat)float325.125 5.375 5.625 ... 24.62 24.88long_name :latitudestandard_name :latitudecoverage_content_type :coordinateaxis :Yunits :degrees_northvalid_min :-90.0valid_max :90.0comment :geolocations inherited from the input data without correctionarray([ 5.125,  5.375,  5.625,  5.875,  6.125,  6.375,  6.625,  6.875,  7.125,\n        7.375,  7.625,  7.875,  8.125,  8.375,  8.625,  8.875,  9.125,  9.375,\n        9.625,  9.875, 10.125, 10.375, 10.625, 10.875, 11.125, 11.375, 11.625,\n       11.875, 12.125, 12.375, 12.625, 12.875, 13.125, 13.375, 13.625, 13.875,\n       14.125, 14.375, 14.625, 14.875, 15.125, 15.375, 15.625, 15.875, 16.125,\n       16.375, 16.625, 16.875, 17.125, 17.375, 17.625, 17.875, 18.125, 18.375,\n       18.625, 18.875, 19.125, 19.375, 19.625, 19.875, 20.125, 20.375, 20.625,\n       20.875, 21.125, 21.375, 21.625, 21.875, 22.125, 22.375, 22.625, 22.875,\n       23.125, 23.375, 23.625, 23.875, 24.125, 24.375, 24.625, 24.875],\n      dtype=float32)lon(lon)float3260.12 60.38 60.62 ... 79.62 79.88long_name :longitudestandard_name :longitudecoverage_content_type :coordinateaxis :Xunits :degrees_eastvalid_min :-180.0valid_max :180.0comment :geolocations inherited from the input data without correctionarray([60.125, 60.375, 60.625, 60.875, 61.125, 61.375, 61.625, 61.875, 62.125,\n       62.375, 62.625, 62.875, 63.125, 63.375, 63.625, 63.875, 64.125, 64.375,\n       64.625, 64.875, 65.125, 65.375, 65.625, 65.875, 66.125, 66.375, 66.625,\n       66.875, 67.125, 67.375, 67.625, 67.875, 68.125, 68.375, 68.625, 68.875,\n       69.125, 69.375, 69.625, 69.875, 70.125, 70.375, 70.625, 70.875, 71.125,\n       71.375, 71.625, 71.875, 72.125, 72.375, 72.625, 72.875, 73.125, 73.375,\n       73.625, 73.875, 74.125, 74.375, 74.625, 74.875, 75.125, 75.375, 75.625,\n       75.875, 76.125, 76.375, 76.625, 76.875, 77.125, 77.375, 77.625, 77.875,\n       78.125, 78.375, 78.625, 78.875, 79.125, 79.375, 79.625, 79.875],\n      dtype=float32)Indexes: (3)timePandasIndexPandasIndex(DatetimeIndex(['2003-01-01 09:00:00', '2003-01-02 09:00:00',\n               '2003-01-03 09:00:00', '2003-01-05 09:00:00',\n               '2003-01-06 09:00:00', '2003-01-07 09:00:00',\n               '2003-01-08 09:00:00', '2003-01-09 09:00:00',\n               '2003-01-10 09:00:00', '2003-01-11 09:00:00',\n               '2003-01-12 09:00:00', '2003-01-13 09:00:00',\n               '2003-01-14 09:00:00', '2003-01-15 09:00:00',\n               '2003-01-16 09:00:00', '2003-01-17 09:00:00',\n               '2003-01-19 09:00:00', '2003-01-20 09:00:00',\n               '2003-01-21 09:00:00', '2003-01-22 09:00:00',\n               '2003-01-23 09:00:00', '2003-01-24 09:00:00',\n               '2003-01-25 09:00:00', '2003-01-26 09:00:00',\n               '2003-01-27 09:00:00', '2003-01-28 09:00:00',\n               '2003-01-29 09:00:00', '2003-01-30 09:00:00',\n               '2003-01-31 09:00:00', '2003-02-01 09:00:00',\n               '2003-02-02 09:00:00', '2003-02-03 09:00:00',\n               '2003-02-04 09:00:00', '2003-02-05 09:00:00',\n               '2003-02-06 09:00:00', '2003-02-07 09:00:00',\n               '2003-02-08 09:00:00', '2003-02-09 09:00:00',\n               '2003-02-10 09:00:00', '2003-02-11 09:00:00',\n               '2003-02-12 09:00:00', '2003-02-13 09:00:00',\n               '2003-02-14 09:00:00', '2003-02-15 09:00:00',\n               '2003-02-16 09:00:00', '2003-02-17 09:00:00',\n               '2003-02-19 09:00:00', '2003-02-20 09:00:00',\n               '2003-02-21 09:00:00', '2003-02-22 09:00:00',\n               '2003-02-23 09:00:00', '2003-02-24 09:00:00',\n               '2003-02-25 09:00:00', '2003-02-26 09:00:00',\n               '2003-02-27 09:00:00', '2003-02-28 09:00:00'],\n              dtype='datetime64[ns]', name='time', freq=None))latPandasIndexPandasIndex(Index([ 5.125,  5.375,  5.625,  5.875,  6.125,  6.375,  6.625,  6.875,  7.125,\n        7.375,  7.625,  7.875,  8.125,  8.375,  8.625,  8.875,  9.125,  9.375,\n        9.625,  9.875, 10.125, 10.375, 10.625, 10.875, 11.125, 11.375, 11.625,\n       11.875, 12.125, 12.375, 12.625, 12.875, 13.125, 13.375, 13.625, 13.875,\n       14.125, 14.375, 14.625, 14.875, 15.125, 15.375, 15.625, 15.875, 16.125,\n       16.375, 16.625, 16.875, 17.125, 17.375, 17.625, 17.875, 18.125, 18.375,\n       18.625, 18.875, 19.125, 19.375, 19.625, 19.875, 20.125, 20.375, 20.625,\n       20.875, 21.125, 21.375, 21.625, 21.875, 22.125, 22.375, 22.625, 22.875,\n       23.125, 23.375, 23.625, 23.875, 24.125, 24.375, 24.625, 24.875],\n      dtype='float32', name='lat'))lonPandasIndexPandasIndex(Index([60.125, 60.375, 60.625, 60.875, 61.125, 61.375, 61.625, 61.875, 62.125,\n       62.375, 62.625, 62.875, 63.125, 63.375, 63.625, 63.875, 64.125, 64.375,\n       64.625, 64.875, 65.125, 65.375, 65.625, 65.875, 66.125, 66.375, 66.625,\n       66.875, 67.125, 67.375, 67.625, 67.875, 68.125, 68.375, 68.625, 68.875,\n       69.125, 69.375, 69.625, 69.875, 70.125, 70.375, 70.625, 70.875, 71.125,\n       71.375, 71.625, 71.875, 72.125, 72.375, 72.625, 72.875, 73.125, 73.375,\n       73.625, 73.875, 74.125, 74.375, 74.625, 74.875, 75.125, 75.375, 75.625,\n       75.875, 76.125, 76.375, 76.625, 76.875, 77.125, 77.375, 77.625, 77.875,\n       78.125, 78.375, 78.625, 78.875, 79.125, 79.375, 79.625, 79.875],\n      dtype='float32', name='lon'))Attributes: (6)long_name :SST anomaly from a seasonal SST climatology based on the MUR data over 2003-2014 periodcoverage_content_type :auxiliaryInformationunits :kelvinvalid_min :-32767valid_max :32767comment :anomaly reference to the day-of-year average between 2003 and 2014"
  },
  {
    "objectID": "python-notebooks/02_Get_Points.html",
    "href": "python-notebooks/02_Get_Points.html",
    "title": "Getting Environmental Variables‚Äô Data at a Time-Space instance",
    "section": "",
    "text": "You have a pandas dataframe with time, lat, lon that you got from say reading a csv file.\nYou have a xarray dataset from reading in the zarr file\nGoal is a pandas data from with time, lat, lon, values for all the environmental varables at that time, lat, lon\n\n\nimport xarray as xr\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport holoviews as hv\nimport hvplot.xarray\n\n\n\n\n\n\n\n\n\n\n\n\n# Change the file path accordingly\nda = xr.open_zarr('/home/jovyan/shared/data/INDIAN_OCEAN_025GRID_DAILY.zarr/')\n\n\ndf = pd.read_csv('/home/jovyan/shared/data/species_occ/io-sea-turtles.csv')\n\n\ndf.head()\n\n\n\n\n\n\n\n\noccurrenceID\nscientificName\ndateIdentified\neventDate\ndecimalLatitude\ndecimalLongitude\ncoordinateUncertaintyInMeters\nindividualCount\nlifeStage\nsex\nbathymetry\nshoredistance\nsst\nsss\ndate\n\n\n\n\n0\n1014_8853\nCaretta caretta\n2012-03-30T08:55:10\n2012-03-30T08:55:10\n9.14804\n50.69448\n0.11\n1.0\nJuvenile\nNaN\n35\n4680\n26.21\n35.63\n2012-03-30\n\n\n1\n1014_9766\nCaretta caretta\n2011-04-12T19:12:41\n2011-04-12T19:12:41\n6.40193\n59.87883\n0.11\n1.0\nJuvenile\nNaN\n3051\n899682\n28.67\n35.74\n2011-04-12\n\n\n2\n2069_333\nChelonia mydas\n2016-07-24T15:18:00\n2016-07-24T15:18:00\n25.69900\n55.78900\n111.32\n1.0\nNaN\nNaN\n8\n-67\n28.19\n38.72\n2016-07-24\n\n\n3\n1014_8861\nCaretta caretta\n2012-03-31T02:43:31\n2012-03-31T02:43:31\n9.21307\n50.82211\n0.11\n1.0\nJuvenile\nNaN\n136\n12208\n26.21\n35.65\n2012-03-31\n\n\n4\n1014_15706\nCaretta caretta\n2011-05-14T17:53:55\n2011-05-14T17:53:55\n15.39172\n55.71303\n0.11\n1.0\nJuvenile\nNaN\n2784\n193567\n27.00\n36.09\n2011-05-14\n\n\n\n\n\n\n\n\ndf = df[:20]\n\n\ndf['occurrenceID']\n\n0      1014_8853\n1      1014_9766\n2       2069_333\n3      1014_8861\n4     1014_15706\n5      2069_4254\n6      2069_3631\n7      2069_4910\n8     1014_16785\n9      2069_3419\n10     2069_5265\n11      2069_442\n12     2069_4668\n13    1014_15473\n14    1014_16862\n15    1014_16860\n16     1014_2087\n17    1014_16484\n18    1014_16899\n19    1014_16669\nName: occurrenceID, dtype: object\n\n\n\nptime = xr.DataArray(df['date'], dims=['point']) # data is from 1979-01-01 to 2022-12-31\nplat = xr.DataArray(df['decimalLatitude'], dims=['point'])  # Points to select need common dimensions\nplon = xr.DataArray(df['decimalLongitude'], dims=['point'])\n\n\nda.sel(time=ptime, lat=plat, lon=plon, method='nearest').squeeze().to_pandas()\n\n\n\n\n\n\n\n\nCHL\nCHL_uncertainty\nadt\nair_temp\ncurr_dir\ncurr_speed\nlat\nlon\nmlotst\nsla\n...\ntime\ntopo\nu_curr\nu_wind\nug_curr\nv_curr\nv_wind\nvg_curr\nwind_dir\nwind_speed\n\n\npoint\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n0.077778\n53.305443\nNaN\n299.421875\nNaN\nNaN\n9.25\n50.75\n13.046663\nNaN\n...\n2012-03-30\n-32.0\nNaN\n-2.179688\nNaN\nNaN\n1.361979\nNaN\n148.000763\n2.570219\n\n\n1\n0.087336\n38.892536\n0.842800\n301.322906\n164.178543\n0.183589\n6.50\n60.00\n21.820736\n0.097500\n...\n2011-04-12\n-2685.0\n-0.176634\n-4.575521\n-0.114507\n0.050054\n-2.950521\n0.011828\n-147.184082\n5.444352\n\n\n2\n1.637924\n53.872570\nNaN\n306.830719\nNaN\nNaN\n25.75\n55.75\n11.291849\nNaN\n...\n2016-07-24\n-9.0\nNaN\n-6.393229\nNaN\nNaN\n2.523438\nNaN\n158.460617\n6.873217\n\n\n3\n0.201034\n43.664085\nNaN\n299.718750\nNaN\nNaN\n9.25\n50.75\n12.817775\nNaN\n...\n2012-03-31\n-32.0\nNaN\n-2.481771\nNaN\nNaN\n1.101562\nNaN\n156.065399\n2.715258\n\n\n4\n0.099818\n38.205021\n0.754650\n301.520844\n49.900036\n0.530751\n15.50\n55.75\n11.520738\n0.164175\n...\n2011-05-14\n-3001.0\n0.341869\n-1.447917\n0.343384\n0.405983\n2.841146\n0.416438\n117.004478\n3.188820\n\n\n5\n0.689319\n28.379957\n0.462725\n298.252594\n70.057045\n0.139277\n24.50\n53.00\n7.171850\n-0.046750\n...\n2018-03-31\n-8.0\n0.047505\n3.419271\n0.053919\n0.130925\n-4.880208\n0.138531\n-54.983360\n5.958846\n\n\n6\n0.738169\n35.419315\n0.472050\n298.463531\n82.680542\n0.163747\n24.50\n53.25\n7.782220\n-0.038450\n...\n2018-03-31\n-10.0\n0.020862\n3.609375\n0.032643\n0.162413\n-4.356771\n0.158398\n-50.359882\n5.657653\n\n\n7\n2.898179\n48.014732\n0.678450\n306.979156\n83.276283\n0.146953\n24.75\n53.00\n11.978516\n0.166025\n...\n2019-08-16\n-14.0\n0.017206\n0.359375\n0.028725\n0.145943\n-2.687500\n0.140182\n-82.383530\n2.711421\n\n\n8\n0.139877\n31.482536\n0.752725\n298.158844\n132.076324\n0.093470\n15.50\n53.75\n11.291849\n0.220450\n...\n2011-04-05\n-2440.0\n-0.062636\n-0.677083\n-0.059925\n0.069378\n2.070312\n0.064436\n108.110016\n2.178218\n\n\n9\n0.922149\n21.962460\n0.450300\n300.229156\n-94.786385\n0.117982\n24.50\n53.00\n7.171850\n-0.059225\n...\n2017-04-27\n-8.0\n-0.009845\n2.500000\n0.023389\n-0.117570\n-2.994792\n-0.083020\n-50.145458\n3.901125\n\n\n10\nNaN\nNaN\nNaN\n305.799469\nNaN\nNaN\n24.25\n51.75\nNaN\n0.209875\n...\n2019-10-07\n20.0\nNaN\n-5.515625\nNaN\nNaN\n-0.151042\nNaN\n-178.431381\n5.517693\n\n\n11\nNaN\nNaN\nNaN\n304.533844\nNaN\nNaN\n25.75\n56.00\nNaN\nNaN\n...\n2016-09-24\n15.0\nNaN\n1.070312\nNaN\nNaN\n0.791667\nNaN\n36.488827\n1.331280\n\n\n12\n2.525344\n40.379536\n0.554250\n304.059906\n-166.844086\n0.089691\n24.50\n53.00\n7.171850\n0.044725\n...\n2019-05-28\n-8.0\n-0.087337\n-1.195312\n-0.069366\n-0.020414\n-1.648438\n-0.018889\n-125.946548\n2.036202\n\n\n13\n0.131018\n31.812517\n0.766250\n299.195312\n-24.765293\n0.264651\n16.00\n54.25\n19.074070\n0.230625\n...\n2011-04-11\n-2732.0\n0.240311\n1.335938\n0.209158\n-0.110863\n4.286458\n-0.100357\n72.689590\n4.489817\n\n\n14\n0.153894\n35.990002\n0.736325\n299.567719\n-132.773819\n0.464942\n14.50\n53.50\n15.182960\n0.152625\n...\n2011-04-13\n-2407.0\n-0.315745\n-1.312500\n-0.326198\n-0.341286\n2.192708\n-0.356120\n120.903671\n2.555509\n\n\n15\n0.153894\n35.990002\n0.736325\n299.567719\n-132.773819\n0.464942\n14.50\n53.50\n15.182960\n0.152625\n...\n2011-04-13\n-2407.0\n-0.315745\n-1.312500\n-0.326198\n-0.341286\n2.192708\n-0.356120\n120.903671\n2.555509\n\n\n16\n0.164712\n40.660110\n0.642800\n301.687500\n-17.193645\n0.117679\n16.75\n54.75\n12.588886\n0.092950\n...\n2012-05-13\n-2268.0\n0.112420\n6.015625\n0.092263\n-0.034786\n2.247396\n-0.001633\n20.485338\n6.421723\n\n\n17\n0.164154\n30.657511\n0.639750\n298.781250\n91.528801\n0.122125\n5.25\n54.00\n22.049625\n-0.079750\n...\n2011-02-10\n-4884.0\n-0.003258\n-4.299479\n0.094443\n0.122081\n-1.971354\n0.049081\n-155.368073\n4.729879\n\n\n18\n0.140080\n31.470005\n0.657150\n300.768219\n-103.054863\n0.217463\n13.50\n52.75\n13.809626\n0.037550\n...\n2011-04-16\n-2609.0\n-0.049121\n-2.177083\n-0.053068\n-0.211842\n0.380208\n-0.216044\n170.093689\n2.210034\n\n\n19\n0.182036\n30.130001\n0.736275\n298.018219\n151.549240\n0.114068\n16.50\n53.75\n16.174812\n0.193825\n...\n2011-03-24\n-2329.0\n-0.100291\n2.276042\n-0.100962\n0.054342\n2.364583\n0.063244\n46.093052\n3.282015\n\n\n\n\n20 rows √ó 22 columns\n\n\n\n\n# side check\n# add occurence ID for points (database)\n\npoints = pd.read_csv('/home/jovyan/shared-readwrite/data/species_occ/result.csv')\n\n\npoints\n\n\n\n\n\n\n\n\nCHL\nCHL_uncertainty\nadt\nair_temp\ncurr_dir\ncurr_speed\nlat\nlon\nmlotst\nsla\n...\ntopo\nu_curr\nu_wind\nug_curr\nv_curr\nv_wind\nvg_curr\nwind_dir\nwind_speed\noccurrenceID\n\n\n\n\n0\n0.077778\n53.305443\nNaN\n299.42188\nNaN\nNaN\n9.25\n50.75\n13.046663\nNaN\n...\n-32.0\nNaN\n-2.179688\nNaN\nNaN\n1.361979\nNaN\n148.000760\n2.570219\n1014_8853\n\n\n1\n0.087336\n38.892536\n0.842800\n301.32290\n164.178540\n0.183589\n6.50\n60.00\n21.820736\n0.097500\n...\n-2685.0\n-0.176634\n-4.575521\n-0.114507\n0.050054\n-2.950521\n0.011828\n-147.184080\n5.444352\n1014_9766\n\n\n2\n1.637924\n53.872570\nNaN\n306.83072\nNaN\nNaN\n25.75\n55.75\n11.291849\nNaN\n...\n-9.0\nNaN\n-6.393229\nNaN\nNaN\n2.523438\nNaN\n158.460620\n6.873217\n2069_333\n\n\n3\n0.201034\n43.664085\nNaN\n299.71875\nNaN\nNaN\n9.25\n50.75\n12.817775\nNaN\n...\n-32.0\nNaN\n-2.481771\nNaN\nNaN\n1.101562\nNaN\n156.065400\n2.715258\n1014_8861\n\n\n4\n0.099818\n38.205020\n0.754650\n301.52084\n49.900036\n0.530751\n15.50\n55.75\n11.520738\n0.164175\n...\n-3001.0\n0.341869\n-1.447917\n0.343384\n0.405983\n2.841146\n0.416438\n117.004480\n3.188820\n1014_15706\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n13205\n0.134301\n74.170000\n0.717500\n301.84116\n-91.168960\n0.037919\n19.75\n61.50\n36.469620\n0.063825\n...\n-3553.0\n-0.000774\n9.335938\n-0.130193\n-0.037911\n9.286458\n0.006756\n44.847763\n13.168069\n1014_2555\n\n\n13206\n0.705127\n28.212389\n0.620425\n300.71094\n108.626130\n0.072745\n24.75\n53.00\n11.139257\n0.108000\n...\n-14.0\n-0.023234\n2.289062\n-0.026993\n0.068935\n-2.375000\n0.052721\n-46.055580\n3.298550\n2069_7427\n\n\n13207\n0.974497\n32.285313\nNaN\n305.82553\nNaN\nNaN\n25.75\n55.75\n11.673330\nNaN\n...\n-9.0\nNaN\n1.078125\nNaN\nNaN\n-0.213542\nNaN\n-11.203434\n1.099069\n2069_587\n\n\n13208\n0.411613\n36.068120\n0.597650\n304.20572\n-107.023180\n0.112533\n24.75\n53.00\n12.741478\n0.085250\n...\n-14.0\n-0.032945\n2.906250\n-0.009166\n-0.107603\n-1.510417\n-0.094187\n-27.461494\n3.275309\n2069_272\n\n\n13209\n0.254070\n43.014880\n0.697400\n299.09897\n-94.746110\n0.627941\n15.50\n55.50\n12.207404\n0.109350\n...\n-2929.0\n-0.051956\n-1.369792\n-0.054724\n-0.625787\n0.901042\n-0.642169\n146.663280\n1.639575\n1014_1635\n\n\n\n\n13210 rows √ó 23 columns\n\n\n\n\npoints['occurrenceID'] = df['occurrenceID']\n\n\npoints.to_csv('/home/jovyan/shared-readwrite/data/species_occ/result2.csv', index=False)\n\n\npoints = points.drop(columns=['point'])"
  },
  {
    "objectID": "python-notebooks/02_Graphing_Map_Data.html#install-and-import-necessary-libraries",
    "href": "python-notebooks/02_Graphing_Map_Data.html#install-and-import-necessary-libraries",
    "title": "Graphing Data from our final product",
    "section": "(Install and) Import necessary libraries",
    "text": "(Install and) Import necessary libraries\n\n! pip install cmocean\n\nRequirement already satisfied: cmocean in /srv/conda/envs/notebook/lib/python3.9/site-packages (3.0.3)\nRequirement already satisfied: matplotlib in /srv/conda/envs/notebook/lib/python3.9/site-packages (from cmocean) (3.7.1)\nRequirement already satisfied: numpy in /srv/conda/envs/notebook/lib/python3.9/site-packages (from cmocean) (1.23.5)\nRequirement already satisfied: packaging in /srv/conda/envs/notebook/lib/python3.9/site-packages (from cmocean) (23.1)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from matplotlib-&gt;cmocean) (1.0.7)\nRequirement already satisfied: cycler&gt;=0.10 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from matplotlib-&gt;cmocean) (0.11.0)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from matplotlib-&gt;cmocean) (4.39.4)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from matplotlib-&gt;cmocean) (1.4.4)\nRequirement already satisfied: pillow&gt;=6.2.0 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from matplotlib-&gt;cmocean) (9.5.0)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from matplotlib-&gt;cmocean) (3.0.9)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from matplotlib-&gt;cmocean) (2.8.2)\nRequirement already satisfied: importlib-resources&gt;=3.2.0 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from matplotlib-&gt;cmocean) (5.12.0)\nRequirement already satisfied: zipp&gt;=3.1.0 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from importlib-resources&gt;=3.2.0-&gt;matplotlib-&gt;cmocean) (3.15.0)\nRequirement already satisfied: six&gt;=1.5 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib-&gt;cmocean) (1.16.0)\n\n\n\n! conda install --channel conda-forge pygmt -y\n\nCollecting package metadata (current_repodata.json): done\nSolving environment: done\n\n\n==&gt; WARNING: A newer version of conda exists. &lt;==\n  current version: 22.11.1\n  latest version: 23.7.4\n\nPlease update conda by running\n\n    $ conda update -n base -c conda-forge conda\n\nOr to minimize the number of packages updated during conda update use\n\n     conda install conda=23.7.4\n\n\n\n# All requested packages already installed.\n\n\n\n\n# Run in terminal\n# conda create --name pygmt --channel conda-forge pygmt\n# then run\n# conda activate pygmt\n\n\nCommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.\nTo initialize your shell, run\n\n    $ conda init &lt;SHELL_NAME&gt;\n\nCurrently supported shells are:\n  - bash\n  - fish\n  - tcsh\n  - xonsh\n  - zsh\n  - powershell\n\nSee 'conda init --help' for more information and options.\n\nIMPORTANT: You may need to close and restart your shell after running 'conda init'.\n\n\n\n\n\nimport xarray as xr\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport holoviews as hv\nimport hvplot.xarray\nimport cmocean\nimport cartopy\n\n\n\n\n\n\n\n\n\n\n\n\nimport pygmt"
  },
  {
    "objectID": "python-notebooks/02_Graphing_Map_Data.html#read-data",
    "href": "python-notebooks/02_Graphing_Map_Data.html#read-data",
    "title": "Graphing Data from our final product",
    "section": "Read data",
    "text": "Read data\n\n# Change the file path accordingly\nds = xr.open_zarr('/home/jovyan/shared-readwrite/data/INDIAN_OCEAN_025GRID_DAILY.zarr/')"
  },
  {
    "objectID": "python-notebooks/02_Graphing_Map_Data.html#slicing-data",
    "href": "python-notebooks/02_Graphing_Map_Data.html#slicing-data",
    "title": "Graphing Data from our final product",
    "section": "Slicing data",
    "text": "Slicing data\n\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:          (time: 8523, lat: 177, lon: 241)\nCoordinates:\n  * lat              (lat) float32 32.0 31.75 31.5 31.25 ... -11.5 -11.75 -12.0\n  * lon              (lon) float32 42.0 42.25 42.5 42.75 ... 101.5 101.8 102.0\n  * time             (time) datetime64[ns] 1997-09-01 1997-09-02 ... 2020-12-31\nData variables: (12/14)\n    CHL              (time, lat, lon) float32 dask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;\n    CHL_uncertainty  (time, lat, lon) float32 dask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;\n    adt              (time, lat, lon) float32 dask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;\n    air_temp         (time, lat, lon) float32 dask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;\n    direction        (time, lat, lon) float32 dask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;\n    sla              (time, lat, lon) float32 dask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;\n    ...               ...\n    u_curr           (time, lat, lon) float32 dask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;\n    u_wind           (time, lat, lon) float32 dask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;\n    ug_curr          (time, lat, lon) float32 dask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;\n    v_curr           (time, lat, lon) float32 dask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;\n    v_wind           (time, lat, lon) float32 dask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;\n    vg_curr          (time, lat, lon) float32 dask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;\nAttributes: (12/17)\n    creator_email:              minhphan@uw.edu\n    creator_name:               Minh Phan\n    creator_type:               person\n    date_created:               2023-07-07\n    geospatial_lat_max:         32.0\n    geospatial_lat_min:         -12.0\n    ...                         ...\n    geospatial_lon_units:       degrees_east\n    source:                     OSCAR, ERA5 Reanalysis, Copernicus Climate Ch...\n    summary:                    Daily mean of 0.25 x 0.25 degrees gridded dat...\n    time_coverage_end:          2020-12-31T23:59:59\n    time_coverage_start:        2000-01-01T00:00:00\n    title:                      Climate Data for Coastal Upwelling Machine Le...xarray.DatasetDimensions:time: 8523lat: 177lon: 241Coordinates: (3)lat(lat)float3232.0 31.75 31.5 ... -11.75 -12.0long_name :latitudestandard_name :latitudeunits :degrees_northarray([ 32.  ,  31.75,  31.5 ,  31.25,  31.  ,  30.75,  30.5 ,  30.25,  30.  ,\n        29.75,  29.5 ,  29.25,  29.  ,  28.75,  28.5 ,  28.25,  28.  ,  27.75,\n        27.5 ,  27.25,  27.  ,  26.75,  26.5 ,  26.25,  26.  ,  25.75,  25.5 ,\n        25.25,  25.  ,  24.75,  24.5 ,  24.25,  24.  ,  23.75,  23.5 ,  23.25,\n        23.  ,  22.75,  22.5 ,  22.25,  22.  ,  21.75,  21.5 ,  21.25,  21.  ,\n        20.75,  20.5 ,  20.25,  20.  ,  19.75,  19.5 ,  19.25,  19.  ,  18.75,\n        18.5 ,  18.25,  18.  ,  17.75,  17.5 ,  17.25,  17.  ,  16.75,  16.5 ,\n        16.25,  16.  ,  15.75,  15.5 ,  15.25,  15.  ,  14.75,  14.5 ,  14.25,\n        14.  ,  13.75,  13.5 ,  13.25,  13.  ,  12.75,  12.5 ,  12.25,  12.  ,\n        11.75,  11.5 ,  11.25,  11.  ,  10.75,  10.5 ,  10.25,  10.  ,   9.75,\n         9.5 ,   9.25,   9.  ,   8.75,   8.5 ,   8.25,   8.  ,   7.75,   7.5 ,\n         7.25,   7.  ,   6.75,   6.5 ,   6.25,   6.  ,   5.75,   5.5 ,   5.25,\n         5.  ,   4.75,   4.5 ,   4.25,   4.  ,   3.75,   3.5 ,   3.25,   3.  ,\n         2.75,   2.5 ,   2.25,   2.  ,   1.75,   1.5 ,   1.25,   1.  ,   0.75,\n         0.5 ,   0.25,   0.  ,  -0.25,  -0.5 ,  -0.75,  -1.  ,  -1.25,  -1.5 ,\n        -1.75,  -2.  ,  -2.25,  -2.5 ,  -2.75,  -3.  ,  -3.25,  -3.5 ,  -3.75,\n        -4.  ,  -4.25,  -4.5 ,  -4.75,  -5.  ,  -5.25,  -5.5 ,  -5.75,  -6.  ,\n        -6.25,  -6.5 ,  -6.75,  -7.  ,  -7.25,  -7.5 ,  -7.75,  -8.  ,  -8.25,\n        -8.5 ,  -8.75,  -9.  ,  -9.25,  -9.5 ,  -9.75, -10.  , -10.25, -10.5 ,\n       -10.75, -11.  , -11.25, -11.5 , -11.75, -12.  ], dtype=float32)lon(lon)float3242.0 42.25 42.5 ... 101.8 102.0long_name :longitudestandard_name :longitudeunits :degrees_eastarray([ 42.  ,  42.25,  42.5 , ..., 101.5 , 101.75, 102.  ], dtype=float32)time(time)datetime64[ns]1997-09-01 ... 2020-12-31axis :Tcomment :Data is averaged over the daylong_name :time centered on the daystandard_name :timetime_bounds :2000-01-01 00:00:00 to 2000-01-01 23:59:59array(['1997-09-01T00:00:00.000000000', '1997-09-02T00:00:00.000000000',\n       '1997-09-03T00:00:00.000000000', ..., '2020-12-29T00:00:00.000000000',\n       '2020-12-30T00:00:00.000000000', '2020-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (14)CHL(time, lat, lon)float32dask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;_ChunkSizes :[1, 256, 256]ancillary_variables :flags CHL_uncertaintycoverage_content_type :modelResultinput_files_reprocessings :Processors versions: MODIS R2022.0NRT/VIIRSN R2022.0NRT/OLCIA 07.02/VIIRSJ1 R2022.0NRT/OLCIB 07.02long_name :Chlorophyll-a concentration - Mean of the binned pixelsstandard_name :mass_concentration_of_chlorophyll_a_in_sea_watertype :surfaceunits :milligram m-3valid_max :1000.0valid_min :0.0\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.35 GiB\n16.27 MiB\n\n\nShape\n(8523, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n87 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nCHL_uncertainty\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;\n\n\n\n\n_ChunkSizes :\n\n[1, 256, 256]\n\ncoverage_content_type :\n\nqualityInformation\n\nlong_name :\n\nChlorophyll-a concentration - Uncertainty estimation\n\nunits :\n\n%\n\nvalid_max :\n\n32767\n\nvalid_min :\n\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.35 GiB\n16.27 MiB\n\n\nShape\n(8523, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n87 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nadt\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nThe absolute dynamic topography is the sea surface height above geoid; the adt is obtained as follows: adt=sla+mdt where mdt is the mean dynamic topography; see the product user manual for details\n\ngrid_mapping :\n\ncrs\n\nlong_name :\n\nAbsolute dynamic topography\n\nstandard_name :\n\nsea_surface_height_above_geoid\n\nunits :\n\nm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.35 GiB\n16.27 MiB\n\n\nShape\n(8523, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n87 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nair_temp\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n2 metre temperature\n\nnameCDM :\n\n2_metre_temperature_surface\n\nnameECMWF :\n\n2 metre temperature\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n2t\n\nstandard_name :\n\nair_temperature\n\nunits :\n\nK\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.35 GiB\n16.27 MiB\n\n\nShape\n(8523, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n87 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\ndirection\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre wind direction\n\nunits :\n\n¬∞C\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.35 GiB\n16.27 MiB\n\n\nShape\n(8523, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n87 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nsla\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nancillary_variables :\n\nerr_sla\n\ncomment :\n\nThe sea level anomaly is the sea surface height above mean sea surface; it is referenced to the [1993, 2012] period; see the product user manual for details\n\ngrid_mapping :\n\ncrs\n\nlong_name :\n\nSea level anomaly\n\nstandard_name :\n\nsea_surface_height_above_sea_level\n\nunits :\n\nm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.35 GiB\n16.27 MiB\n\n\nShape\n(8523, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n87 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nspeed\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre absolute speed\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.35 GiB\n16.27 MiB\n\n\nShape\n(8523, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n87 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nsst\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nSea surface temperature\n\nnameCDM :\n\nSea_surface_temperature_surface\n\nnameECMWF :\n\nSea surface temperature\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\nsst\n\nstandard_name :\n\nsea_surface_temperature\n\nunits :\n\nK\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.35 GiB\n16.27 MiB\n\n\nShape\n(8523, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n87 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nu_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nVelocities are an average over the top 30m of the mixed layer\n\ncoverage_content_type :\n\nmodelResult\n\ndepth :\n\n15m\n\nlong_name :\n\nzonal total surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148 ; WIND source: ECMWF ERA5 10m wind DOI: 10.24381/cds.adbb2d47 ; SST source: CMC 0.2 deg SST V2.0 DOI: 10.5067/GHCMC-4FM02\n\nstandard_name :\n\neastward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.35 GiB\n16.27 MiB\n\n\nShape\n(8523, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n87 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nu_wind\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre U wind component\n\nnameCDM :\n\n10_metre_U_wind_component_surface\n\nnameECMWF :\n\n10 metre U wind component\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n10u\n\nstandard_name :\n\neastward_wind\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.35 GiB\n16.27 MiB\n\n\nShape\n(8523, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n87 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nug_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nGeostrophic velocities calculated from absolute dynamic topography\n\ndepth :\n\n15m\n\nlong_name :\n\nzonal geostrophic surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148\n\nstandard_name :\n\ngeostrophic_eastward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.35 GiB\n16.27 MiB\n\n\nShape\n(8523, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n87 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nv_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nVelocities are an average over the top 30m of the mixed layer\n\ncoverage_content_type :\n\nmodelResult\n\ndepth :\n\n15m\n\nlong_name :\n\nmeridional total surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148 ; WIND source: ECMWF ERA5 10m wind DOI: 10.24381/cds.adbb2d47 ; SST source: CMC 0.2 deg SST V2.0 DOI: 10.5067/GHCMC-4FM02\n\nstandard_name :\n\nnorthward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.35 GiB\n16.27 MiB\n\n\nShape\n(8523, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n87 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nv_wind\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre V wind component\n\nnameCDM :\n\n10_metre_V_wind_component_surface\n\nnameECMWF :\n\n10 metre V wind component\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n10v\n\nstandard_name :\n\nnorthward_wind\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.35 GiB\n16.27 MiB\n\n\nShape\n(8523, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n87 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nvg_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nGeostrophic velocities calculated from absolute dynamic topography\n\ndepth :\n\n15m\n\nlong_name :\n\nmeridional geostrophic surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148\n\nstandard_name :\n\ngeostrophic_northward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.35 GiB\n16.27 MiB\n\n\nShape\n(8523, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n87 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nAttributes: (17)creator_email :minhphan@uw.educreator_name :Minh Phancreator_type :persondate_created :2023-07-07geospatial_lat_max :32.0geospatial_lat_min :-12.0geospatial_lat_resolution :0.25geospatial_lat_units :degrees_northgeospatial_lon_max :102.0geospatial_lon_min :42.0geospatial_lon_resolution :0.25geospatial_lon_units :degrees_eastsource :OSCAR, ERA5 Reanalysis, Copernicus Climate Change Service (C3S), Copernicus Marine Environment Monitoring Service (CMEMS)summary :Daily mean of 0.25 x 0.25 degrees gridded data from multiple climate variables that may influence the patterns of coastal upwelling in the focused areatime_coverage_end :2020-12-31T23:59:59time_coverage_start :2000-01-01T00:00:00title :Climate Data for Coastal Upwelling Machine Learning Project in Indian Ocean\n\n\nWe can slice data by the dimensions (latitude, longitude, time) and data variables.\n\n# slice by latitude\n# notice how we specify the range in reverse\nds.sel(lat=slice(0, -12))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:          (time: 8523, lat: 49, lon: 241)\nCoordinates:\n  * lat              (lat) float32 0.0 -0.25 -0.5 -0.75 ... -11.5 -11.75 -12.0\n  * lon              (lon) float32 42.0 42.25 42.5 42.75 ... 101.5 101.8 102.0\n  * time             (time) datetime64[ns] 1997-09-01 1997-09-02 ... 2020-12-31\nData variables: (12/14)\n    CHL              (time, lat, lon) float32 dask.array&lt;chunksize=(29, 49, 241), meta=np.ndarray&gt;\n    CHL_uncertainty  (time, lat, lon) float32 dask.array&lt;chunksize=(29, 49, 241), meta=np.ndarray&gt;\n    adt              (time, lat, lon) float32 dask.array&lt;chunksize=(29, 49, 241), meta=np.ndarray&gt;\n    air_temp         (time, lat, lon) float32 dask.array&lt;chunksize=(29, 49, 241), meta=np.ndarray&gt;\n    direction        (time, lat, lon) float32 dask.array&lt;chunksize=(29, 49, 241), meta=np.ndarray&gt;\n    sla              (time, lat, lon) float32 dask.array&lt;chunksize=(29, 49, 241), meta=np.ndarray&gt;\n    ...               ...\n    u_curr           (time, lat, lon) float32 dask.array&lt;chunksize=(29, 49, 241), meta=np.ndarray&gt;\n    u_wind           (time, lat, lon) float32 dask.array&lt;chunksize=(29, 49, 241), meta=np.ndarray&gt;\n    ug_curr          (time, lat, lon) float32 dask.array&lt;chunksize=(29, 49, 241), meta=np.ndarray&gt;\n    v_curr           (time, lat, lon) float32 dask.array&lt;chunksize=(29, 49, 241), meta=np.ndarray&gt;\n    v_wind           (time, lat, lon) float32 dask.array&lt;chunksize=(29, 49, 241), meta=np.ndarray&gt;\n    vg_curr          (time, lat, lon) float32 dask.array&lt;chunksize=(29, 49, 241), meta=np.ndarray&gt;\nAttributes: (12/17)\n    creator_email:              minhphan@uw.edu\n    creator_name:               Minh Phan\n    creator_type:               person\n    date_created:               2023-07-07\n    geospatial_lat_max:         32.0\n    geospatial_lat_min:         -12.0\n    ...                         ...\n    geospatial_lon_units:       degrees_east\n    source:                     OSCAR, ERA5 Reanalysis, Copernicus Climate Ch...\n    summary:                    Daily mean of 0.25 x 0.25 degrees gridded dat...\n    time_coverage_end:          2020-12-31T23:59:59\n    time_coverage_start:        2000-01-01T00:00:00\n    title:                      Climate Data for Coastal Upwelling Machine Le...xarray.DatasetDimensions:time: 8523lat: 49lon: 241Coordinates: (3)lat(lat)float320.0 -0.25 -0.5 ... -11.75 -12.0long_name :latitudestandard_name :latitudeunits :degrees_northarray([  0.  ,  -0.25,  -0.5 ,  -0.75,  -1.  ,  -1.25,  -1.5 ,  -1.75,  -2.  ,\n        -2.25,  -2.5 ,  -2.75,  -3.  ,  -3.25,  -3.5 ,  -3.75,  -4.  ,  -4.25,\n        -4.5 ,  -4.75,  -5.  ,  -5.25,  -5.5 ,  -5.75,  -6.  ,  -6.25,  -6.5 ,\n        -6.75,  -7.  ,  -7.25,  -7.5 ,  -7.75,  -8.  ,  -8.25,  -8.5 ,  -8.75,\n        -9.  ,  -9.25,  -9.5 ,  -9.75, -10.  , -10.25, -10.5 , -10.75, -11.  ,\n       -11.25, -11.5 , -11.75, -12.  ], dtype=float32)lon(lon)float3242.0 42.25 42.5 ... 101.8 102.0long_name :longitudestandard_name :longitudeunits :degrees_eastarray([ 42.  ,  42.25,  42.5 , ..., 101.5 , 101.75, 102.  ], dtype=float32)time(time)datetime64[ns]1997-09-01 ... 2020-12-31axis :Tcomment :Data is averaged over the daylong_name :time centered on the daystandard_name :timetime_bounds :2000-01-01 00:00:00 to 2000-01-01 23:59:59array(['1997-09-01T00:00:00.000000000', '1997-09-02T00:00:00.000000000',\n       '1997-09-03T00:00:00.000000000', ..., '2020-12-29T00:00:00.000000000',\n       '2020-12-30T00:00:00.000000000', '2020-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (14)CHL(time, lat, lon)float32dask.array&lt;chunksize=(29, 49, 241), meta=np.ndarray&gt;_ChunkSizes :[1, 256, 256]ancillary_variables :flags CHL_uncertaintycoverage_content_type :modelResultinput_files_reprocessings :Processors versions: MODIS R2022.0NRT/VIIRSN R2022.0NRT/OLCIA 07.02/VIIRSJ1 R2022.0NRT/OLCIB 07.02long_name :Chlorophyll-a concentration - Mean of the binned pixelsstandard_name :mass_concentration_of_chlorophyll_a_in_sea_watertype :surfaceunits :milligram m-3valid_max :1000.0valid_min :0.0\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n383.94 MiB\n4.50 MiB\n\n\nShape\n(8523, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n87 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nCHL_uncertainty\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 49, 241), meta=np.ndarray&gt;\n\n\n\n\n_ChunkSizes :\n\n[1, 256, 256]\n\ncoverage_content_type :\n\nqualityInformation\n\nlong_name :\n\nChlorophyll-a concentration - Uncertainty estimation\n\nunits :\n\n%\n\nvalid_max :\n\n32767\n\nvalid_min :\n\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n383.94 MiB\n4.50 MiB\n\n\nShape\n(8523, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n87 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nadt\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 49, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nThe absolute dynamic topography is the sea surface height above geoid; the adt is obtained as follows: adt=sla+mdt where mdt is the mean dynamic topography; see the product user manual for details\n\ngrid_mapping :\n\ncrs\n\nlong_name :\n\nAbsolute dynamic topography\n\nstandard_name :\n\nsea_surface_height_above_geoid\n\nunits :\n\nm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n383.94 MiB\n4.50 MiB\n\n\nShape\n(8523, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n87 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nair_temp\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 49, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n2 metre temperature\n\nnameCDM :\n\n2_metre_temperature_surface\n\nnameECMWF :\n\n2 metre temperature\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n2t\n\nstandard_name :\n\nair_temperature\n\nunits :\n\nK\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n383.94 MiB\n4.50 MiB\n\n\nShape\n(8523, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n87 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\ndirection\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 49, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre wind direction\n\nunits :\n\n¬∞C\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n383.94 MiB\n4.50 MiB\n\n\nShape\n(8523, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n87 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nsla\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 49, 241), meta=np.ndarray&gt;\n\n\n\n\nancillary_variables :\n\nerr_sla\n\ncomment :\n\nThe sea level anomaly is the sea surface height above mean sea surface; it is referenced to the [1993, 2012] period; see the product user manual for details\n\ngrid_mapping :\n\ncrs\n\nlong_name :\n\nSea level anomaly\n\nstandard_name :\n\nsea_surface_height_above_sea_level\n\nunits :\n\nm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n383.94 MiB\n4.50 MiB\n\n\nShape\n(8523, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n87 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nspeed\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 49, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre absolute speed\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n383.94 MiB\n4.50 MiB\n\n\nShape\n(8523, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n87 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nsst\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 49, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nSea surface temperature\n\nnameCDM :\n\nSea_surface_temperature_surface\n\nnameECMWF :\n\nSea surface temperature\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\nsst\n\nstandard_name :\n\nsea_surface_temperature\n\nunits :\n\nK\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n383.94 MiB\n4.50 MiB\n\n\nShape\n(8523, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n87 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nu_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 49, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nVelocities are an average over the top 30m of the mixed layer\n\ncoverage_content_type :\n\nmodelResult\n\ndepth :\n\n15m\n\nlong_name :\n\nzonal total surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148 ; WIND source: ECMWF ERA5 10m wind DOI: 10.24381/cds.adbb2d47 ; SST source: CMC 0.2 deg SST V2.0 DOI: 10.5067/GHCMC-4FM02\n\nstandard_name :\n\neastward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n383.94 MiB\n4.50 MiB\n\n\nShape\n(8523, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n87 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nu_wind\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 49, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre U wind component\n\nnameCDM :\n\n10_metre_U_wind_component_surface\n\nnameECMWF :\n\n10 metre U wind component\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n10u\n\nstandard_name :\n\neastward_wind\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n383.94 MiB\n4.50 MiB\n\n\nShape\n(8523, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n87 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nug_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 49, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nGeostrophic velocities calculated from absolute dynamic topography\n\ndepth :\n\n15m\n\nlong_name :\n\nzonal geostrophic surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148\n\nstandard_name :\n\ngeostrophic_eastward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n383.94 MiB\n4.50 MiB\n\n\nShape\n(8523, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n87 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nv_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 49, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nVelocities are an average over the top 30m of the mixed layer\n\ncoverage_content_type :\n\nmodelResult\n\ndepth :\n\n15m\n\nlong_name :\n\nmeridional total surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148 ; WIND source: ECMWF ERA5 10m wind DOI: 10.24381/cds.adbb2d47 ; SST source: CMC 0.2 deg SST V2.0 DOI: 10.5067/GHCMC-4FM02\n\nstandard_name :\n\nnorthward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n383.94 MiB\n4.50 MiB\n\n\nShape\n(8523, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n87 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nv_wind\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 49, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre V wind component\n\nnameCDM :\n\n10_metre_V_wind_component_surface\n\nnameECMWF :\n\n10 metre V wind component\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n10v\n\nstandard_name :\n\nnorthward_wind\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n383.94 MiB\n4.50 MiB\n\n\nShape\n(8523, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n87 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nvg_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 49, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nGeostrophic velocities calculated from absolute dynamic topography\n\ndepth :\n\n15m\n\nlong_name :\n\nmeridional geostrophic surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148\n\nstandard_name :\n\ngeostrophic_northward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n383.94 MiB\n4.50 MiB\n\n\nShape\n(8523, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n87 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nAttributes: (17)creator_email :minhphan@uw.educreator_name :Minh Phancreator_type :persondate_created :2023-07-07geospatial_lat_max :32.0geospatial_lat_min :-12.0geospatial_lat_resolution :0.25geospatial_lat_units :degrees_northgeospatial_lon_max :102.0geospatial_lon_min :42.0geospatial_lon_resolution :0.25geospatial_lon_units :degrees_eastsource :OSCAR, ERA5 Reanalysis, Copernicus Climate Change Service (C3S), Copernicus Marine Environment Monitoring Service (CMEMS)summary :Daily mean of 0.25 x 0.25 degrees gridded data from multiple climate variables that may influence the patterns of coastal upwelling in the focused areatime_coverage_end :2020-12-31T23:59:59time_coverage_start :2000-01-01T00:00:00title :Climate Data for Coastal Upwelling Machine Learning Project in Indian Ocean\n\n\n\n# slice by longitude\nds.sel(lon=slice(42, 45))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:          (time: 8523, lat: 177, lon: 13)\nCoordinates:\n  * lat              (lat) float32 32.0 31.75 31.5 31.25 ... -11.5 -11.75 -12.0\n  * lon              (lon) float32 42.0 42.25 42.5 42.75 ... 44.5 44.75 45.0\n  * time             (time) datetime64[ns] 1997-09-01 1997-09-02 ... 2020-12-31\nData variables: (12/14)\n    CHL              (time, lat, lon) float32 dask.array&lt;chunksize=(29, 177, 13), meta=np.ndarray&gt;\n    CHL_uncertainty  (time, lat, lon) float32 dask.array&lt;chunksize=(29, 177, 13), meta=np.ndarray&gt;\n    adt              (time, lat, lon) float32 dask.array&lt;chunksize=(29, 177, 13), meta=np.ndarray&gt;\n    air_temp         (time, lat, lon) float32 dask.array&lt;chunksize=(29, 177, 13), meta=np.ndarray&gt;\n    direction        (time, lat, lon) float32 dask.array&lt;chunksize=(29, 177, 13), meta=np.ndarray&gt;\n    sla              (time, lat, lon) float32 dask.array&lt;chunksize=(29, 177, 13), meta=np.ndarray&gt;\n    ...               ...\n    u_curr           (time, lat, lon) float32 dask.array&lt;chunksize=(29, 177, 13), meta=np.ndarray&gt;\n    u_wind           (time, lat, lon) float32 dask.array&lt;chunksize=(29, 177, 13), meta=np.ndarray&gt;\n    ug_curr          (time, lat, lon) float32 dask.array&lt;chunksize=(29, 177, 13), meta=np.ndarray&gt;\n    v_curr           (time, lat, lon) float32 dask.array&lt;chunksize=(29, 177, 13), meta=np.ndarray&gt;\n    v_wind           (time, lat, lon) float32 dask.array&lt;chunksize=(29, 177, 13), meta=np.ndarray&gt;\n    vg_curr          (time, lat, lon) float32 dask.array&lt;chunksize=(29, 177, 13), meta=np.ndarray&gt;\nAttributes: (12/17)\n    creator_email:              minhphan@uw.edu\n    creator_name:               Minh Phan\n    creator_type:               person\n    date_created:               2023-07-07\n    geospatial_lat_max:         32.0\n    geospatial_lat_min:         -12.0\n    ...                         ...\n    geospatial_lon_units:       degrees_east\n    source:                     OSCAR, ERA5 Reanalysis, Copernicus Climate Ch...\n    summary:                    Daily mean of 0.25 x 0.25 degrees gridded dat...\n    time_coverage_end:          2020-12-31T23:59:59\n    time_coverage_start:        2000-01-01T00:00:00\n    title:                      Climate Data for Coastal Upwelling Machine Le...xarray.DatasetDimensions:time: 8523lat: 177lon: 13Coordinates: (3)lat(lat)float3232.0 31.75 31.5 ... -11.75 -12.0long_name :latitudestandard_name :latitudeunits :degrees_northarray([ 32.  ,  31.75,  31.5 ,  31.25,  31.  ,  30.75,  30.5 ,  30.25,  30.  ,\n        29.75,  29.5 ,  29.25,  29.  ,  28.75,  28.5 ,  28.25,  28.  ,  27.75,\n        27.5 ,  27.25,  27.  ,  26.75,  26.5 ,  26.25,  26.  ,  25.75,  25.5 ,\n        25.25,  25.  ,  24.75,  24.5 ,  24.25,  24.  ,  23.75,  23.5 ,  23.25,\n        23.  ,  22.75,  22.5 ,  22.25,  22.  ,  21.75,  21.5 ,  21.25,  21.  ,\n        20.75,  20.5 ,  20.25,  20.  ,  19.75,  19.5 ,  19.25,  19.  ,  18.75,\n        18.5 ,  18.25,  18.  ,  17.75,  17.5 ,  17.25,  17.  ,  16.75,  16.5 ,\n        16.25,  16.  ,  15.75,  15.5 ,  15.25,  15.  ,  14.75,  14.5 ,  14.25,\n        14.  ,  13.75,  13.5 ,  13.25,  13.  ,  12.75,  12.5 ,  12.25,  12.  ,\n        11.75,  11.5 ,  11.25,  11.  ,  10.75,  10.5 ,  10.25,  10.  ,   9.75,\n         9.5 ,   9.25,   9.  ,   8.75,   8.5 ,   8.25,   8.  ,   7.75,   7.5 ,\n         7.25,   7.  ,   6.75,   6.5 ,   6.25,   6.  ,   5.75,   5.5 ,   5.25,\n         5.  ,   4.75,   4.5 ,   4.25,   4.  ,   3.75,   3.5 ,   3.25,   3.  ,\n         2.75,   2.5 ,   2.25,   2.  ,   1.75,   1.5 ,   1.25,   1.  ,   0.75,\n         0.5 ,   0.25,   0.  ,  -0.25,  -0.5 ,  -0.75,  -1.  ,  -1.25,  -1.5 ,\n        -1.75,  -2.  ,  -2.25,  -2.5 ,  -2.75,  -3.  ,  -3.25,  -3.5 ,  -3.75,\n        -4.  ,  -4.25,  -4.5 ,  -4.75,  -5.  ,  -5.25,  -5.5 ,  -5.75,  -6.  ,\n        -6.25,  -6.5 ,  -6.75,  -7.  ,  -7.25,  -7.5 ,  -7.75,  -8.  ,  -8.25,\n        -8.5 ,  -8.75,  -9.  ,  -9.25,  -9.5 ,  -9.75, -10.  , -10.25, -10.5 ,\n       -10.75, -11.  , -11.25, -11.5 , -11.75, -12.  ], dtype=float32)lon(lon)float3242.0 42.25 42.5 ... 44.5 44.75 45.0long_name :longitudestandard_name :longitudeunits :degrees_eastarray([42.  , 42.25, 42.5 , 42.75, 43.  , 43.25, 43.5 , 43.75, 44.  , 44.25,\n       44.5 , 44.75, 45.  ], dtype=float32)time(time)datetime64[ns]1997-09-01 ... 2020-12-31axis :Tcomment :Data is averaged over the daylong_name :time centered on the daystandard_name :timetime_bounds :2000-01-01 00:00:00 to 2000-01-01 23:59:59array(['1997-09-01T00:00:00.000000000', '1997-09-02T00:00:00.000000000',\n       '1997-09-03T00:00:00.000000000', ..., '2020-12-29T00:00:00.000000000',\n       '2020-12-30T00:00:00.000000000', '2020-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (14)CHL(time, lat, lon)float32dask.array&lt;chunksize=(29, 177, 13), meta=np.ndarray&gt;_ChunkSizes :[1, 256, 256]ancillary_variables :flags CHL_uncertaintycoverage_content_type :modelResultinput_files_reprocessings :Processors versions: MODIS R2022.0NRT/VIIRSN R2022.0NRT/OLCIA 07.02/VIIRSJ1 R2022.0NRT/OLCIB 07.02long_name :Chlorophyll-a concentration - Mean of the binned pixelsstandard_name :mass_concentration_of_chlorophyll_a_in_sea_watertype :surfaceunits :milligram m-3valid_max :1000.0valid_min :0.0\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n74.81 MiB\n898.83 kiB\n\n\nShape\n(8523, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n87 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nCHL_uncertainty\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 177, 13), meta=np.ndarray&gt;\n\n\n\n\n_ChunkSizes :\n\n[1, 256, 256]\n\ncoverage_content_type :\n\nqualityInformation\n\nlong_name :\n\nChlorophyll-a concentration - Uncertainty estimation\n\nunits :\n\n%\n\nvalid_max :\n\n32767\n\nvalid_min :\n\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n74.81 MiB\n898.83 kiB\n\n\nShape\n(8523, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n87 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nadt\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 177, 13), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nThe absolute dynamic topography is the sea surface height above geoid; the adt is obtained as follows: adt=sla+mdt where mdt is the mean dynamic topography; see the product user manual for details\n\ngrid_mapping :\n\ncrs\n\nlong_name :\n\nAbsolute dynamic topography\n\nstandard_name :\n\nsea_surface_height_above_geoid\n\nunits :\n\nm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n74.81 MiB\n898.83 kiB\n\n\nShape\n(8523, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n87 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nair_temp\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 177, 13), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n2 metre temperature\n\nnameCDM :\n\n2_metre_temperature_surface\n\nnameECMWF :\n\n2 metre temperature\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n2t\n\nstandard_name :\n\nair_temperature\n\nunits :\n\nK\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n74.81 MiB\n898.83 kiB\n\n\nShape\n(8523, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n87 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\ndirection\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 177, 13), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre wind direction\n\nunits :\n\n¬∞C\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n74.81 MiB\n898.83 kiB\n\n\nShape\n(8523, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n87 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nsla\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 177, 13), meta=np.ndarray&gt;\n\n\n\n\nancillary_variables :\n\nerr_sla\n\ncomment :\n\nThe sea level anomaly is the sea surface height above mean sea surface; it is referenced to the [1993, 2012] period; see the product user manual for details\n\ngrid_mapping :\n\ncrs\n\nlong_name :\n\nSea level anomaly\n\nstandard_name :\n\nsea_surface_height_above_sea_level\n\nunits :\n\nm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n74.81 MiB\n898.83 kiB\n\n\nShape\n(8523, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n87 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nspeed\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 177, 13), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre absolute speed\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n74.81 MiB\n898.83 kiB\n\n\nShape\n(8523, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n87 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nsst\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 177, 13), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nSea surface temperature\n\nnameCDM :\n\nSea_surface_temperature_surface\n\nnameECMWF :\n\nSea surface temperature\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\nsst\n\nstandard_name :\n\nsea_surface_temperature\n\nunits :\n\nK\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n74.81 MiB\n898.83 kiB\n\n\nShape\n(8523, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n87 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nu_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 177, 13), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nVelocities are an average over the top 30m of the mixed layer\n\ncoverage_content_type :\n\nmodelResult\n\ndepth :\n\n15m\n\nlong_name :\n\nzonal total surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148 ; WIND source: ECMWF ERA5 10m wind DOI: 10.24381/cds.adbb2d47 ; SST source: CMC 0.2 deg SST V2.0 DOI: 10.5067/GHCMC-4FM02\n\nstandard_name :\n\neastward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n74.81 MiB\n898.83 kiB\n\n\nShape\n(8523, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n87 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nu_wind\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 177, 13), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre U wind component\n\nnameCDM :\n\n10_metre_U_wind_component_surface\n\nnameECMWF :\n\n10 metre U wind component\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n10u\n\nstandard_name :\n\neastward_wind\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n74.81 MiB\n898.83 kiB\n\n\nShape\n(8523, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n87 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nug_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 177, 13), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nGeostrophic velocities calculated from absolute dynamic topography\n\ndepth :\n\n15m\n\nlong_name :\n\nzonal geostrophic surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148\n\nstandard_name :\n\ngeostrophic_eastward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n74.81 MiB\n898.83 kiB\n\n\nShape\n(8523, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n87 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nv_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 177, 13), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nVelocities are an average over the top 30m of the mixed layer\n\ncoverage_content_type :\n\nmodelResult\n\ndepth :\n\n15m\n\nlong_name :\n\nmeridional total surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148 ; WIND source: ECMWF ERA5 10m wind DOI: 10.24381/cds.adbb2d47 ; SST source: CMC 0.2 deg SST V2.0 DOI: 10.5067/GHCMC-4FM02\n\nstandard_name :\n\nnorthward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n74.81 MiB\n898.83 kiB\n\n\nShape\n(8523, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n87 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nv_wind\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 177, 13), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre V wind component\n\nnameCDM :\n\n10_metre_V_wind_component_surface\n\nnameECMWF :\n\n10 metre V wind component\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n10v\n\nstandard_name :\n\nnorthward_wind\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n74.81 MiB\n898.83 kiB\n\n\nShape\n(8523, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n87 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nvg_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 177, 13), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nGeostrophic velocities calculated from absolute dynamic topography\n\ndepth :\n\n15m\n\nlong_name :\n\nmeridional geostrophic surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148\n\nstandard_name :\n\ngeostrophic_northward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n74.81 MiB\n898.83 kiB\n\n\nShape\n(8523, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n87 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nAttributes: (17)creator_email :minhphan@uw.educreator_name :Minh Phancreator_type :persondate_created :2023-07-07geospatial_lat_max :32.0geospatial_lat_min :-12.0geospatial_lat_resolution :0.25geospatial_lat_units :degrees_northgeospatial_lon_max :102.0geospatial_lon_min :42.0geospatial_lon_resolution :0.25geospatial_lon_units :degrees_eastsource :OSCAR, ERA5 Reanalysis, Copernicus Climate Change Service (C3S), Copernicus Marine Environment Monitoring Service (CMEMS)summary :Daily mean of 0.25 x 0.25 degrees gridded data from multiple climate variables that may influence the patterns of coastal upwelling in the focused areatime_coverage_end :2020-12-31T23:59:59time_coverage_start :2000-01-01T00:00:00title :Climate Data for Coastal Upwelling Machine Learning Project in Indian Ocean\n\n\n\n# slice by time\nds.sel(time=slice('1998', '1999'))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:          (time: 730, lat: 177, lon: 241)\nCoordinates:\n  * lat              (lat) float32 32.0 31.75 31.5 31.25 ... -11.5 -11.75 -12.0\n  * lon              (lon) float32 42.0 42.25 42.5 42.75 ... 101.5 101.8 102.0\n  * time             (time) datetime64[ns] 1998-01-01 1998-01-02 ... 1999-12-31\nData variables: (12/14)\n    CHL              (time, lat, lon) float32 dask.array&lt;chunksize=(7, 177, 241), meta=np.ndarray&gt;\n    CHL_uncertainty  (time, lat, lon) float32 dask.array&lt;chunksize=(7, 177, 241), meta=np.ndarray&gt;\n    adt              (time, lat, lon) float32 dask.array&lt;chunksize=(7, 177, 241), meta=np.ndarray&gt;\n    air_temp         (time, lat, lon) float32 dask.array&lt;chunksize=(7, 177, 241), meta=np.ndarray&gt;\n    direction        (time, lat, lon) float32 dask.array&lt;chunksize=(7, 177, 241), meta=np.ndarray&gt;\n    sla              (time, lat, lon) float32 dask.array&lt;chunksize=(7, 177, 241), meta=np.ndarray&gt;\n    ...               ...\n    u_curr           (time, lat, lon) float32 dask.array&lt;chunksize=(7, 177, 241), meta=np.ndarray&gt;\n    u_wind           (time, lat, lon) float32 dask.array&lt;chunksize=(7, 177, 241), meta=np.ndarray&gt;\n    ug_curr          (time, lat, lon) float32 dask.array&lt;chunksize=(7, 177, 241), meta=np.ndarray&gt;\n    v_curr           (time, lat, lon) float32 dask.array&lt;chunksize=(7, 177, 241), meta=np.ndarray&gt;\n    v_wind           (time, lat, lon) float32 dask.array&lt;chunksize=(7, 177, 241), meta=np.ndarray&gt;\n    vg_curr          (time, lat, lon) float32 dask.array&lt;chunksize=(7, 177, 241), meta=np.ndarray&gt;\nAttributes: (12/17)\n    creator_email:              minhphan@uw.edu\n    creator_name:               Minh Phan\n    creator_type:               person\n    date_created:               2023-07-07\n    geospatial_lat_max:         32.0\n    geospatial_lat_min:         -12.0\n    ...                         ...\n    geospatial_lon_units:       degrees_east\n    source:                     OSCAR, ERA5 Reanalysis, Copernicus Climate Ch...\n    summary:                    Daily mean of 0.25 x 0.25 degrees gridded dat...\n    time_coverage_end:          2020-12-31T23:59:59\n    time_coverage_start:        2000-01-01T00:00:00\n    title:                      Climate Data for Coastal Upwelling Machine Le...xarray.DatasetDimensions:time: 730lat: 177lon: 241Coordinates: (3)lat(lat)float3232.0 31.75 31.5 ... -11.75 -12.0long_name :latitudestandard_name :latitudeunits :degrees_northarray([ 32.  ,  31.75,  31.5 ,  31.25,  31.  ,  30.75,  30.5 ,  30.25,  30.  ,\n        29.75,  29.5 ,  29.25,  29.  ,  28.75,  28.5 ,  28.25,  28.  ,  27.75,\n        27.5 ,  27.25,  27.  ,  26.75,  26.5 ,  26.25,  26.  ,  25.75,  25.5 ,\n        25.25,  25.  ,  24.75,  24.5 ,  24.25,  24.  ,  23.75,  23.5 ,  23.25,\n        23.  ,  22.75,  22.5 ,  22.25,  22.  ,  21.75,  21.5 ,  21.25,  21.  ,\n        20.75,  20.5 ,  20.25,  20.  ,  19.75,  19.5 ,  19.25,  19.  ,  18.75,\n        18.5 ,  18.25,  18.  ,  17.75,  17.5 ,  17.25,  17.  ,  16.75,  16.5 ,\n        16.25,  16.  ,  15.75,  15.5 ,  15.25,  15.  ,  14.75,  14.5 ,  14.25,\n        14.  ,  13.75,  13.5 ,  13.25,  13.  ,  12.75,  12.5 ,  12.25,  12.  ,\n        11.75,  11.5 ,  11.25,  11.  ,  10.75,  10.5 ,  10.25,  10.  ,   9.75,\n         9.5 ,   9.25,   9.  ,   8.75,   8.5 ,   8.25,   8.  ,   7.75,   7.5 ,\n         7.25,   7.  ,   6.75,   6.5 ,   6.25,   6.  ,   5.75,   5.5 ,   5.25,\n         5.  ,   4.75,   4.5 ,   4.25,   4.  ,   3.75,   3.5 ,   3.25,   3.  ,\n         2.75,   2.5 ,   2.25,   2.  ,   1.75,   1.5 ,   1.25,   1.  ,   0.75,\n         0.5 ,   0.25,   0.  ,  -0.25,  -0.5 ,  -0.75,  -1.  ,  -1.25,  -1.5 ,\n        -1.75,  -2.  ,  -2.25,  -2.5 ,  -2.75,  -3.  ,  -3.25,  -3.5 ,  -3.75,\n        -4.  ,  -4.25,  -4.5 ,  -4.75,  -5.  ,  -5.25,  -5.5 ,  -5.75,  -6.  ,\n        -6.25,  -6.5 ,  -6.75,  -7.  ,  -7.25,  -7.5 ,  -7.75,  -8.  ,  -8.25,\n        -8.5 ,  -8.75,  -9.  ,  -9.25,  -9.5 ,  -9.75, -10.  , -10.25, -10.5 ,\n       -10.75, -11.  , -11.25, -11.5 , -11.75, -12.  ], dtype=float32)lon(lon)float3242.0 42.25 42.5 ... 101.8 102.0long_name :longitudestandard_name :longitudeunits :degrees_eastarray([ 42.  ,  42.25,  42.5 , ..., 101.5 , 101.75, 102.  ], dtype=float32)time(time)datetime64[ns]1998-01-01 ... 1999-12-31axis :Tcomment :Data is averaged over the daylong_name :time centered on the daystandard_name :timetime_bounds :2000-01-01 00:00:00 to 2000-01-01 23:59:59array(['1998-01-01T00:00:00.000000000', '1998-01-02T00:00:00.000000000',\n       '1998-01-03T00:00:00.000000000', ..., '1999-12-29T00:00:00.000000000',\n       '1999-12-30T00:00:00.000000000', '1999-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (14)CHL(time, lat, lon)float32dask.array&lt;chunksize=(7, 177, 241), meta=np.ndarray&gt;_ChunkSizes :[1, 256, 256]ancillary_variables :flags CHL_uncertaintycoverage_content_type :modelResultinput_files_reprocessings :Processors versions: MODIS R2022.0NRT/VIIRSN R2022.0NRT/OLCIA 07.02/VIIRSJ1 R2022.0NRT/OLCIB 07.02long_name :Chlorophyll-a concentration - Mean of the binned pixelsstandard_name :mass_concentration_of_chlorophyll_a_in_sea_watertype :surfaceunits :milligram m-3valid_max :1000.0valid_min :0.0\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n9 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nCHL_uncertainty\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(7, 177, 241), meta=np.ndarray&gt;\n\n\n\n\n_ChunkSizes :\n\n[1, 256, 256]\n\ncoverage_content_type :\n\nqualityInformation\n\nlong_name :\n\nChlorophyll-a concentration - Uncertainty estimation\n\nunits :\n\n%\n\nvalid_max :\n\n32767\n\nvalid_min :\n\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n9 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nadt\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(7, 177, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nThe absolute dynamic topography is the sea surface height above geoid; the adt is obtained as follows: adt=sla+mdt where mdt is the mean dynamic topography; see the product user manual for details\n\ngrid_mapping :\n\ncrs\n\nlong_name :\n\nAbsolute dynamic topography\n\nstandard_name :\n\nsea_surface_height_above_geoid\n\nunits :\n\nm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n9 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nair_temp\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(7, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n2 metre temperature\n\nnameCDM :\n\n2_metre_temperature_surface\n\nnameECMWF :\n\n2 metre temperature\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n2t\n\nstandard_name :\n\nair_temperature\n\nunits :\n\nK\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n9 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\ndirection\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(7, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre wind direction\n\nunits :\n\n¬∞C\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n9 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nsla\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(7, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nancillary_variables :\n\nerr_sla\n\ncomment :\n\nThe sea level anomaly is the sea surface height above mean sea surface; it is referenced to the [1993, 2012] period; see the product user manual for details\n\ngrid_mapping :\n\ncrs\n\nlong_name :\n\nSea level anomaly\n\nstandard_name :\n\nsea_surface_height_above_sea_level\n\nunits :\n\nm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n9 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nspeed\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(7, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre absolute speed\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n9 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nsst\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(7, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nSea surface temperature\n\nnameCDM :\n\nSea_surface_temperature_surface\n\nnameECMWF :\n\nSea surface temperature\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\nsst\n\nstandard_name :\n\nsea_surface_temperature\n\nunits :\n\nK\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n9 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nu_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(7, 177, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nVelocities are an average over the top 30m of the mixed layer\n\ncoverage_content_type :\n\nmodelResult\n\ndepth :\n\n15m\n\nlong_name :\n\nzonal total surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148 ; WIND source: ECMWF ERA5 10m wind DOI: 10.24381/cds.adbb2d47 ; SST source: CMC 0.2 deg SST V2.0 DOI: 10.5067/GHCMC-4FM02\n\nstandard_name :\n\neastward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n9 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nu_wind\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(7, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre U wind component\n\nnameCDM :\n\n10_metre_U_wind_component_surface\n\nnameECMWF :\n\n10 metre U wind component\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n10u\n\nstandard_name :\n\neastward_wind\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n9 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nug_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(7, 177, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nGeostrophic velocities calculated from absolute dynamic topography\n\ndepth :\n\n15m\n\nlong_name :\n\nzonal geostrophic surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148\n\nstandard_name :\n\ngeostrophic_eastward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n9 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nv_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(7, 177, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nVelocities are an average over the top 30m of the mixed layer\n\ncoverage_content_type :\n\nmodelResult\n\ndepth :\n\n15m\n\nlong_name :\n\nmeridional total surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148 ; WIND source: ECMWF ERA5 10m wind DOI: 10.24381/cds.adbb2d47 ; SST source: CMC 0.2 deg SST V2.0 DOI: 10.5067/GHCMC-4FM02\n\nstandard_name :\n\nnorthward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n9 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nv_wind\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(7, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre V wind component\n\nnameCDM :\n\n10_metre_V_wind_component_surface\n\nnameECMWF :\n\n10 metre V wind component\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n10v\n\nstandard_name :\n\nnorthward_wind\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n9 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nvg_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(7, 177, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nGeostrophic velocities calculated from absolute dynamic topography\n\ndepth :\n\n15m\n\nlong_name :\n\nmeridional geostrophic surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148\n\nstandard_name :\n\ngeostrophic_northward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n9 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nAttributes: (17)creator_email :minhphan@uw.educreator_name :Minh Phancreator_type :persondate_created :2023-07-07geospatial_lat_max :32.0geospatial_lat_min :-12.0geospatial_lat_resolution :0.25geospatial_lat_units :degrees_northgeospatial_lon_max :102.0geospatial_lon_min :42.0geospatial_lon_resolution :0.25geospatial_lon_units :degrees_eastsource :OSCAR, ERA5 Reanalysis, Copernicus Climate Change Service (C3S), Copernicus Marine Environment Monitoring Service (CMEMS)summary :Daily mean of 0.25 x 0.25 degrees gridded data from multiple climate variables that may influence the patterns of coastal upwelling in the focused areatime_coverage_end :2020-12-31T23:59:59time_coverage_start :2000-01-01T00:00:00title :Climate Data for Coastal Upwelling Machine Learning Project in Indian Ocean\n\n\n\n# slice by variable\nds[['u_curr', 'u_wind']]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (time: 8523, lat: 177, lon: 241)\nCoordinates:\n  * lat      (lat) float32 32.0 31.75 31.5 31.25 ... -11.25 -11.5 -11.75 -12.0\n  * lon      (lon) float32 42.0 42.25 42.5 42.75 ... 101.2 101.5 101.8 102.0\n  * time     (time) datetime64[ns] 1997-09-01 1997-09-02 ... 2020-12-31\nData variables:\n    u_curr   (time, lat, lon) float32 dask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;\n    u_wind   (time, lat, lon) float32 dask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;\nAttributes: (12/17)\n    creator_email:              minhphan@uw.edu\n    creator_name:               Minh Phan\n    creator_type:               person\n    date_created:               2023-07-07\n    geospatial_lat_max:         32.0\n    geospatial_lat_min:         -12.0\n    ...                         ...\n    geospatial_lon_units:       degrees_east\n    source:                     OSCAR, ERA5 Reanalysis, Copernicus Climate Ch...\n    summary:                    Daily mean of 0.25 x 0.25 degrees gridded dat...\n    time_coverage_end:          2020-12-31T23:59:59\n    time_coverage_start:        2000-01-01T00:00:00\n    title:                      Climate Data for Coastal Upwelling Machine Le...xarray.DatasetDimensions:time: 8523lat: 177lon: 241Coordinates: (3)lat(lat)float3232.0 31.75 31.5 ... -11.75 -12.0long_name :latitudestandard_name :latitudeunits :degrees_northarray([ 32.  ,  31.75,  31.5 ,  31.25,  31.  ,  30.75,  30.5 ,  30.25,  30.  ,\n        29.75,  29.5 ,  29.25,  29.  ,  28.75,  28.5 ,  28.25,  28.  ,  27.75,\n        27.5 ,  27.25,  27.  ,  26.75,  26.5 ,  26.25,  26.  ,  25.75,  25.5 ,\n        25.25,  25.  ,  24.75,  24.5 ,  24.25,  24.  ,  23.75,  23.5 ,  23.25,\n        23.  ,  22.75,  22.5 ,  22.25,  22.  ,  21.75,  21.5 ,  21.25,  21.  ,\n        20.75,  20.5 ,  20.25,  20.  ,  19.75,  19.5 ,  19.25,  19.  ,  18.75,\n        18.5 ,  18.25,  18.  ,  17.75,  17.5 ,  17.25,  17.  ,  16.75,  16.5 ,\n        16.25,  16.  ,  15.75,  15.5 ,  15.25,  15.  ,  14.75,  14.5 ,  14.25,\n        14.  ,  13.75,  13.5 ,  13.25,  13.  ,  12.75,  12.5 ,  12.25,  12.  ,\n        11.75,  11.5 ,  11.25,  11.  ,  10.75,  10.5 ,  10.25,  10.  ,   9.75,\n         9.5 ,   9.25,   9.  ,   8.75,   8.5 ,   8.25,   8.  ,   7.75,   7.5 ,\n         7.25,   7.  ,   6.75,   6.5 ,   6.25,   6.  ,   5.75,   5.5 ,   5.25,\n         5.  ,   4.75,   4.5 ,   4.25,   4.  ,   3.75,   3.5 ,   3.25,   3.  ,\n         2.75,   2.5 ,   2.25,   2.  ,   1.75,   1.5 ,   1.25,   1.  ,   0.75,\n         0.5 ,   0.25,   0.  ,  -0.25,  -0.5 ,  -0.75,  -1.  ,  -1.25,  -1.5 ,\n        -1.75,  -2.  ,  -2.25,  -2.5 ,  -2.75,  -3.  ,  -3.25,  -3.5 ,  -3.75,\n        -4.  ,  -4.25,  -4.5 ,  -4.75,  -5.  ,  -5.25,  -5.5 ,  -5.75,  -6.  ,\n        -6.25,  -6.5 ,  -6.75,  -7.  ,  -7.25,  -7.5 ,  -7.75,  -8.  ,  -8.25,\n        -8.5 ,  -8.75,  -9.  ,  -9.25,  -9.5 ,  -9.75, -10.  , -10.25, -10.5 ,\n       -10.75, -11.  , -11.25, -11.5 , -11.75, -12.  ], dtype=float32)lon(lon)float3242.0 42.25 42.5 ... 101.8 102.0long_name :longitudestandard_name :longitudeunits :degrees_eastarray([ 42.  ,  42.25,  42.5 , ..., 101.5 , 101.75, 102.  ], dtype=float32)time(time)datetime64[ns]1997-09-01 ... 2020-12-31axis :Tcomment :Data is averaged over the daylong_name :time centered on the daystandard_name :timetime_bounds :2000-01-01 00:00:00 to 2000-01-01 23:59:59array(['1997-09-01T00:00:00.000000000', '1997-09-02T00:00:00.000000000',\n       '1997-09-03T00:00:00.000000000', ..., '2020-12-29T00:00:00.000000000',\n       '2020-12-30T00:00:00.000000000', '2020-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (2)u_curr(time, lat, lon)float32dask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;comment :Velocities are an average over the top 30m of the mixed layercoverage_content_type :modelResultdepth :15mlong_name :zonal total surface currentsource :SSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148 ; WIND source: ECMWF ERA5 10m wind DOI: 10.24381/cds.adbb2d47 ; SST source: CMC 0.2 deg SST V2.0 DOI: 10.5067/GHCMC-4FM02standard_name :eastward_sea_water_velocityunits :m s-1valid_max :3.0valid_min :-3.0\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.35 GiB\n16.27 MiB\n\n\nShape\n(8523, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n87 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nu_wind\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(29, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre U wind component\n\nnameCDM :\n\n10_metre_U_wind_component_surface\n\nnameECMWF :\n\n10 metre U wind component\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n10u\n\nstandard_name :\n\neastward_wind\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.35 GiB\n16.27 MiB\n\n\nShape\n(8523, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n87 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nAttributes: (17)creator_email :minhphan@uw.educreator_name :Minh Phancreator_type :persondate_created :2023-07-07geospatial_lat_max :32.0geospatial_lat_min :-12.0geospatial_lat_resolution :0.25geospatial_lat_units :degrees_northgeospatial_lon_max :102.0geospatial_lon_min :42.0geospatial_lon_resolution :0.25geospatial_lon_units :degrees_eastsource :OSCAR, ERA5 Reanalysis, Copernicus Climate Change Service (C3S), Copernicus Marine Environment Monitoring Service (CMEMS)summary :Daily mean of 0.25 x 0.25 degrees gridded data from multiple climate variables that may influence the patterns of coastal upwelling in the focused areatime_coverage_end :2020-12-31T23:59:59time_coverage_start :2000-01-01T00:00:00title :Climate Data for Coastal Upwelling Machine Learning Project in Indian Ocean\n\n\n\n# combine multiple slicing options all at once\nds[['u_curr', 'u_wind']].sel(time=slice('1998', '1999'), \n                             lat=slice(0, -12), \n                             lon=slice(42, 45))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (time: 730, lat: 49, lon: 13)\nCoordinates:\n  * lat      (lat) float32 0.0 -0.25 -0.5 -0.75 ... -11.25 -11.5 -11.75 -12.0\n  * lon      (lon) float32 42.0 42.25 42.5 42.75 43.0 ... 44.25 44.5 44.75 45.0\n  * time     (time) datetime64[ns] 1998-01-01 1998-01-02 ... 1999-12-31\nData variables:\n    u_curr   (time, lat, lon) float32 dask.array&lt;chunksize=(7, 49, 13), meta=np.ndarray&gt;\n    u_wind   (time, lat, lon) float32 dask.array&lt;chunksize=(7, 49, 13), meta=np.ndarray&gt;\nAttributes: (12/17)\n    creator_email:              minhphan@uw.edu\n    creator_name:               Minh Phan\n    creator_type:               person\n    date_created:               2023-07-07\n    geospatial_lat_max:         32.0\n    geospatial_lat_min:         -12.0\n    ...                         ...\n    geospatial_lon_units:       degrees_east\n    source:                     OSCAR, ERA5 Reanalysis, Copernicus Climate Ch...\n    summary:                    Daily mean of 0.25 x 0.25 degrees gridded dat...\n    time_coverage_end:          2020-12-31T23:59:59\n    time_coverage_start:        2000-01-01T00:00:00\n    title:                      Climate Data for Coastal Upwelling Machine Le...xarray.DatasetDimensions:time: 730lat: 49lon: 13Coordinates: (3)lat(lat)float320.0 -0.25 -0.5 ... -11.75 -12.0long_name :latitudestandard_name :latitudeunits :degrees_northarray([  0.  ,  -0.25,  -0.5 ,  -0.75,  -1.  ,  -1.25,  -1.5 ,  -1.75,  -2.  ,\n        -2.25,  -2.5 ,  -2.75,  -3.  ,  -3.25,  -3.5 ,  -3.75,  -4.  ,  -4.25,\n        -4.5 ,  -4.75,  -5.  ,  -5.25,  -5.5 ,  -5.75,  -6.  ,  -6.25,  -6.5 ,\n        -6.75,  -7.  ,  -7.25,  -7.5 ,  -7.75,  -8.  ,  -8.25,  -8.5 ,  -8.75,\n        -9.  ,  -9.25,  -9.5 ,  -9.75, -10.  , -10.25, -10.5 , -10.75, -11.  ,\n       -11.25, -11.5 , -11.75, -12.  ], dtype=float32)lon(lon)float3242.0 42.25 42.5 ... 44.5 44.75 45.0long_name :longitudestandard_name :longitudeunits :degrees_eastarray([42.  , 42.25, 42.5 , 42.75, 43.  , 43.25, 43.5 , 43.75, 44.  , 44.25,\n       44.5 , 44.75, 45.  ], dtype=float32)time(time)datetime64[ns]1998-01-01 ... 1999-12-31axis :Tcomment :Data is averaged over the daylong_name :time centered on the daystandard_name :timetime_bounds :2000-01-01 00:00:00 to 2000-01-01 23:59:59array(['1998-01-01T00:00:00.000000000', '1998-01-02T00:00:00.000000000',\n       '1998-01-03T00:00:00.000000000', ..., '1999-12-29T00:00:00.000000000',\n       '1999-12-30T00:00:00.000000000', '1999-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (2)u_curr(time, lat, lon)float32dask.array&lt;chunksize=(7, 49, 13), meta=np.ndarray&gt;comment :Velocities are an average over the top 30m of the mixed layercoverage_content_type :modelResultdepth :15mlong_name :zonal total surface currentsource :SSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148 ; WIND source: ECMWF ERA5 10m wind DOI: 10.24381/cds.adbb2d47 ; SST source: CMC 0.2 deg SST V2.0 DOI: 10.5067/GHCMC-4FM02standard_name :eastward_sea_water_velocityunits :m s-1valid_max :3.0valid_min :-3.0\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.77 MiB\n248.83 kiB\n\n\nShape\n(730, 49, 13)\n(100, 49, 13)\n\n\nDask graph\n9 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nu_wind\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(7, 49, 13), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre U wind component\n\nnameCDM :\n\n10_metre_U_wind_component_surface\n\nnameECMWF :\n\n10 metre U wind component\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n10u\n\nstandard_name :\n\neastward_wind\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.77 MiB\n248.83 kiB\n\n\nShape\n(730, 49, 13)\n(100, 49, 13)\n\n\nDask graph\n9 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nAttributes: (17)creator_email :minhphan@uw.educreator_name :Minh Phancreator_type :persondate_created :2023-07-07geospatial_lat_max :32.0geospatial_lat_min :-12.0geospatial_lat_resolution :0.25geospatial_lat_units :degrees_northgeospatial_lon_max :102.0geospatial_lon_min :42.0geospatial_lon_resolution :0.25geospatial_lon_units :degrees_eastsource :OSCAR, ERA5 Reanalysis, Copernicus Climate Change Service (C3S), Copernicus Marine Environment Monitoring Service (CMEMS)summary :Daily mean of 0.25 x 0.25 degrees gridded data from multiple climate variables that may influence the patterns of coastal upwelling in the focused areatime_coverage_end :2020-12-31T23:59:59time_coverage_start :2000-01-01T00:00:00title :Climate Data for Coastal Upwelling Machine Learning Project in Indian Ocean"
  },
  {
    "objectID": "python-notebooks/02_Graphing_Map_Data.html#basic-data-graphing",
    "href": "python-notebooks/02_Graphing_Map_Data.html#basic-data-graphing",
    "title": "Graphing Data from our final product",
    "section": "Basic data graphing",
    "text": "Basic data graphing\nWe can also graph the data right from slicing, especially heatmaps from 2D arrays, or line charts. This is especially useful when we want to inspect elements on the go.\n\n# make sure that the array you slice for a heatmap visualization is a 2D array\nheatmap_arr = ds['wind_speed'].sel(time='2000-01-02')\nheatmap_arr\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'wind_speed' (lat: 177, lon: 241)&gt;\ndask.array&lt;getitem, shape=(177, 241), dtype=float32, chunksize=(177, 241), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * lat      (lat) float32 32.0 31.75 31.5 31.25 ... -11.25 -11.5 -11.75 -12.0\n  * lon      (lon) float32 42.0 42.25 42.5 42.75 ... 101.2 101.5 101.8 102.0\n    time     datetime64[ns] 2000-01-02\nAttributes:\n    long_name:  10 metre absolute speed\n    units:      m s**-1xarray.DataArray'wind_speed'lat: 177lon: 241dask.array&lt;chunksize=(177, 241), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n166.63 kiB\n166.63 kiB\n\n\nShape\n(177, 241)\n(177, 241)\n\n\nDask graph\n1 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\nCoordinates: (3)lat(lat)float3232.0 31.75 31.5 ... -11.75 -12.0long_name :latitudestandard_name :latitudeunits :degrees_northarray([ 32.  ,  31.75,  31.5 ,  31.25,  31.  ,  30.75,  30.5 ,  30.25,  30.  ,\n        29.75,  29.5 ,  29.25,  29.  ,  28.75,  28.5 ,  28.25,  28.  ,  27.75,\n        27.5 ,  27.25,  27.  ,  26.75,  26.5 ,  26.25,  26.  ,  25.75,  25.5 ,\n        25.25,  25.  ,  24.75,  24.5 ,  24.25,  24.  ,  23.75,  23.5 ,  23.25,\n        23.  ,  22.75,  22.5 ,  22.25,  22.  ,  21.75,  21.5 ,  21.25,  21.  ,\n        20.75,  20.5 ,  20.25,  20.  ,  19.75,  19.5 ,  19.25,  19.  ,  18.75,\n        18.5 ,  18.25,  18.  ,  17.75,  17.5 ,  17.25,  17.  ,  16.75,  16.5 ,\n        16.25,  16.  ,  15.75,  15.5 ,  15.25,  15.  ,  14.75,  14.5 ,  14.25,\n        14.  ,  13.75,  13.5 ,  13.25,  13.  ,  12.75,  12.5 ,  12.25,  12.  ,\n        11.75,  11.5 ,  11.25,  11.  ,  10.75,  10.5 ,  10.25,  10.  ,   9.75,\n         9.5 ,   9.25,   9.  ,   8.75,   8.5 ,   8.25,   8.  ,   7.75,   7.5 ,\n         7.25,   7.  ,   6.75,   6.5 ,   6.25,   6.  ,   5.75,   5.5 ,   5.25,\n         5.  ,   4.75,   4.5 ,   4.25,   4.  ,   3.75,   3.5 ,   3.25,   3.  ,\n         2.75,   2.5 ,   2.25,   2.  ,   1.75,   1.5 ,   1.25,   1.  ,   0.75,\n         0.5 ,   0.25,   0.  ,  -0.25,  -0.5 ,  -0.75,  -1.  ,  -1.25,  -1.5 ,\n        -1.75,  -2.  ,  -2.25,  -2.5 ,  -2.75,  -3.  ,  -3.25,  -3.5 ,  -3.75,\n        -4.  ,  -4.25,  -4.5 ,  -4.75,  -5.  ,  -5.25,  -5.5 ,  -5.75,  -6.  ,\n        -6.25,  -6.5 ,  -6.75,  -7.  ,  -7.25,  -7.5 ,  -7.75,  -8.  ,  -8.25,\n        -8.5 ,  -8.75,  -9.  ,  -9.25,  -9.5 ,  -9.75, -10.  , -10.25, -10.5 ,\n       -10.75, -11.  , -11.25, -11.5 , -11.75, -12.  ], dtype=float32)lon(lon)float3242.0 42.25 42.5 ... 101.8 102.0long_name :longitudestandard_name :longitudeunits :degrees_eastarray([ 42.  ,  42.25,  42.5 , ..., 101.5 , 101.75, 102.  ], dtype=float32)time()datetime64[ns]2000-01-02axis :Tcomment :Data is averaged over the daylong_name :time centered on the daystandard_name :timetime_bounds :2000-01-01 00:00:00 to 2000-01-01 23:59:59array('2000-01-02T00:00:00.000000000', dtype='datetime64[ns]')Indexes: (2)latPandasIndexPandasIndex(Index([  32.0,  31.75,   31.5,  31.25,   31.0,  30.75,   30.5,  30.25,   30.0,\n        29.75,\n       ...\n        -9.75,  -10.0, -10.25,  -10.5, -10.75,  -11.0, -11.25,  -11.5, -11.75,\n        -12.0],\n      dtype='float32', name='lat', length=177))lonPandasIndexPandasIndex(Index([  42.0,  42.25,   42.5,  42.75,   43.0,  43.25,   43.5,  43.75,   44.0,\n        44.25,\n       ...\n        99.75,  100.0, 100.25,  100.5, 100.75,  101.0, 101.25,  101.5, 101.75,\n        102.0],\n      dtype='float32', name='lon', length=241))Attributes: (2)long_name :10 metre absolute speedunits :m s**-1\n\n\n\n2D array graphing with matplotlib\nBy default, graphing arrays with Xarray is done using matplotlib.\nFor 2D arrays we have multiple options to choose for our graphs\n\nHeatmaps\n\nheatmap_arr.plot.imshow()\n\n&lt;matplotlib.image.AxesImage at 0x7fba15c77940&gt;\n\n\n\n\n\n\n\nContour maps\n\n# contour map with no filling\nheatmap_arr.plot.contour()\n\n&lt;matplotlib.contour.QuadContourSet at 0x7fba146d44c0&gt;\n\n\n\n\n\n\n# contour map with color filling\nheatmap_arr.plot.contourf()\n\n&lt;matplotlib.contour.QuadContourSet at 0x7fba145a13a0&gt;\n\n\n\n\n\n\n\nSurface plot\n\nheatmap_arr.plot.surface()\n\n&lt;mpl_toolkits.mplot3d.art3d.Poly3DCollection at 0x7fba1450bf40&gt;\n\n\n\n\n\n\n\n\n1D array graphing with Matplotlib\n\nLine plots\nWe can also plot line graphs if our data is presented in 1D arrays. For the example below, we average our wind speed over the whole area and plot it over time. Matplotlib automatically infers what graph to create if you don‚Äôt explicitly call which one to. This piece of code below is a demonstration:\n\nds['wind_speed'].mean(dim=['lat', 'lon']).plot(figsize=(10, 5))\n\n\n\n\nWe can add in parameters to customize our graphs, as additional arguments are passed to the underlying matplotlib plot() function.\n\nds['air_temp'].mean(dim=['lat', 'lon']).sel(time=slice('2007', '2009')).plot.line('r-o', figsize=(10,5), markersize=1)\n\n\n\n\n\n\nHistogram\n\n# creating a new Axe object if there is no currently\n# available one\nax = plt.gca() \nds['wind_dir'].plot.hist(ax = ax)\nax.set_xlabel('10 metre wind direction (degrees east)')\nax.set_ylabel('frequency')\nax.set_title('Daily average wind direction distribution over covered area (1979-2022)')\n\nText(0.5, 1.0, 'Daily average wind direction distribution over covered area (1979-2022)')\n\n\n\n\n\n\n\n\nGraphing with holoviews\nXarray also support graphing using holoviews if you want interactive visualizations.\n\n# scroll along the axes to stretch the graph\nheatmap_arr.hvplot().options(cmap='bgy', width=600, height=500)"
  },
  {
    "objectID": "python-notebooks/02_Graphing_Map_Data.html#maps",
    "href": "python-notebooks/02_Graphing_Map_Data.html#maps",
    "title": "Graphing Data from our final product",
    "section": "Maps",
    "text": "Maps\nSometimes we want to identify areas in which our data may behave differently. One of the best ways to do this is to create maps.\n\nCoastline mapping\nCoastlines can help us separate land and ocean area. We can plot the coastline for our region of interest without using any data.\n\ngeo_axes = plt.axes(projection=cartopy.crs.PlateCarree())\n# add stock image\n# geo_axes.stock_img()\ngeo_axes.set_extent([42, 102, -12, 32])\ngeo_axes.coastlines()\n\n&lt;cartopy.mpl.feature_artist.FeatureArtist at 0x7fba16af6040&gt;\n\n\n\n\n\nWe can also apply the coastline onto our data. The example below shows a heatmap with the coastline drawn on top of it\n\ntopo = xr.open_dataset('/home/jovyan/shared-readwrite/data/INDIAN_OCEAN_HQ_TOPO.nc')\n\n\nfig, ax = plt.subplots(subplot_kw=dict(projection=cartopy.crs.PlateCarree()))\ntopo.topo.plot.imshow(ax=ax)\nax.add_feature(cartopy.feature.COASTLINE)\n\n&lt;cartopy.mpl.feature_artist.FeatureArtist at 0x7fb9e4224be0&gt;\n\n\n\n\n\n\n\nTopography mapping\nWe also have a topography dataset that spans the same grid as our final dataset. At the moment of writing this moment, this dataset has not been incoporated into the latter one yet, as you have seen earlier in this notebook that we have to import it separately.\nWe can use this dataset to visualize the topography through contour maps.\n\ntopo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:    (latitude: 5281, longitude: 7201)\nCoordinates:\n  * latitude   (latitude) float64 32.0 31.99 31.98 31.98 ... -11.98 -11.99 -12.0\n  * longitude  (longitude) float64 42.0 42.01 42.02 42.03 ... 102.0 102.0 102.0\nData variables:\n    topo       (latitude, longitude) float32 349.0 352.0 ... -3.727e+03xarray.DatasetDimensions:latitude: 5281longitude: 7201Coordinates: (2)latitude(latitude)float6432.0 31.99 31.98 ... -11.99 -12.0_CoordinateAxisType :Latactual_range :[-89.99166667  90.        ]axis :Yioos_category :Locationlong_name :Latitudestandard_name :latitudeunits :degrees_northarray([ 32.      ,  31.991667,  31.983333, ..., -11.983333, -11.991667,\n       -12.      ])longitude(longitude)float6442.0 42.01 42.02 ... 102.0 102.0_CoordinateAxisType :Lonactual_range :[-180.          179.99166667]axis :Xioos_category :Locationlong_name :Longitudestandard_name :longitudeunits :degrees_eastarray([ 42.      ,  42.008333,  42.016667, ..., 101.983333, 101.991667,\n       102.      ])Data variables: (1)topo(latitude, longitude)float32349.0 352.0 ... -3.727e+03colorBarMaximum :8000.0colorBarMinimum :-8000.0colorBarPalette :Topographygrid_mapping :GDAL_Geographicsioos_category :Locationlong_name :Topographystandard_name :altitudeunits :metersarray([[  349.,   352.,   351., ...,  3313.,  2918.,  2786.],\n       [  349.,   351.,   351., ...,  3538.,  3142.,  3249.],\n       [  351.,   353.,   351., ...,  3497.,  3352.,  3515.],\n       ...,\n       [-2651., -2653., -2657., ..., -3197., -3361., -3517.],\n       [-2654., -2657., -2661., ..., -3318., -3472., -3616.],\n       [-2658., -2661., -2666., ..., -3450., -3594., -3727.]], dtype=float32)Indexes: (2)latitudePandasIndexPandasIndex(Index([               32.0,  31.991666666666667,  31.983333333333334,\n                    31.975,   31.96666666666667,  31.958333333333336,\n        31.950000000000003,   31.94166666666667,  31.933333333333337,\n        31.925000000000004,\n       ...\n       -11.924999999999997, -11.933333333333337, -11.941666666666663,\n       -11.950000000000003, -11.958333333333329, -11.966666666666669,\n       -11.974999999999994, -11.983333333333334,  -11.99166666666666,\n                     -12.0],\n      dtype='float64', name='latitude', length=5281))longitudePandasIndexPandasIndex(Index([              42.0, 42.008333333333326,  42.01666666666665,\n       42.025000000000006,  42.03333333333333,  42.04166666666666,\n        42.04999999999998,  42.05833333333334,  42.06666666666666,\n        42.07499999999999,\n       ...\n       101.92500000000001, 101.93333333333334, 101.94166666666666,\n       101.94999999999999, 101.95833333333331, 101.96666666666664,\n       101.97500000000002, 101.98333333333335, 101.99166666666667,\n                    102.0],\n      dtype='float64', name='longitude', length=7201))Attributes: (0)\n\n\n\nfig, (ax1, ax2) = plt.subplots(2,1, figsize=(8, 12))\ntopo_contour = topo.topo.plot.contour(levels=12, ax=ax1, cmap='coolwarm')\ntopo_contourf = topo.topo.plot.contourf(levels=12, ax=ax2, cmap='coolwarm', label='')\n# add label in line\n# ax.clabel(topo_contour, inline=1, fontsize=10)\n\nhue1, color1 = topo_contour.legend_elements()\nhue2, color2 = topo_contourf.legend_elements()\nax1.legend(hue1, color1, loc='center left', bbox_to_anchor=(1, 0.5))\nax1.set_aspect('equal')\nax2.set_aspect('equal')\n# ax2.get_legend().set_visible(False)\n\n/srv/conda/envs/notebook/lib/python3.9/site-packages/xarray/plot/dataarray_plot.py:2141: UserWarning: The following kwargs were not used by contour: 'label'\n  primitive = ax.contourf(x, y, z, **kwargs)\n\n\n\n\n\nExporting the contour lines to graph on other datasets:\n\n# extract lines from contour paths\nlines = []\nfor contour_path in topo_contour.collections:\n    for path in contour_path.get_paths():\n        lines.extend(path.to_polygons())\n\n\nlen(lines)\n\n12327\n\n\nWe want to graph the topography lines on top of this heatmap. Can we do this with matplotlib?\nLet‚Äôs take a look of the contour lines generated from matplotlib.pyplot.contour() function: it returned a list of PathCollection objects containing elevent different contour types based on elevation (each with a different color). Each elevation has multiple ‚Äúlines‚Äù as each of the line denotes an elevation mark.\n\ntopo_contour.collections\n\n&lt;a list of 11 PathCollection objects&gt;\n\n\n\nlen(topo_contour.collections[0].get_paths())\n\n0\n\n\n\nlen(topo_contour.collections[2].get_paths())\n\n30\n\n\nIn the example below, we graph the contour lines back onto the original contour maps.\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 7))\ntopo.topo.plot.imshow(ax=ax1)\nfor contour in topo_contour.collections[2:9]:\n    for path in contour.get_paths():\n        for line in path.to_polygons():\n            ax1.scatter(line[:,0], line[:, 1], color='black', s=0.1) # graph some of the contours (as dots)\n\ntopo.topo.plot.imshow(ax=ax2)\nfor line in lines:\n    ax2.scatter(line[:,0], line[:, 1], color='black', s=0.5) # graph all contours (as dots)\n    \nax1.set_aspect('equal')\nax2.set_aspect('equal')\n\n\n\n\n\n# very weird lines\nax = plt.gca()\ntopo.topo.plot.imshow(ax=ax)\nfor line in lines[:1200]: # there is an order of the lines sorted by topography height\n    ax.plot(line[:,0], line[:, 1], color='black', markersize=0.1)\n\n\n\n\n\n\nCreating topography contour maps with pygmt\nBecause of the weird lines caused by the connection of the open-ended paths, we want to experiment with some other packages.\n\n# Load Earth relief grids (topography and bathymetry) in various resolutions.\n# get contour grid\ngrid = pygmt.datasets.load_earth_relief(resolution=\"15m\", region=[42, 102, -12, 32])\nfig = pygmt.Figure()\n# add the heatmap first\nfig.grdimage(topo.topo, region='42/102/-12/32', cmap='topo', projection='M4i') # mercator projection\n# and then the contour map on top of it\nfig.grdcontour(grid=grid,\n              annotation='1000+f6p', # font 6p\n              interval=1000,\n              limit=[-5000, 0],\n              projection='M4i',\n              pen=\"a0.15p\") # 0.15 point\nfig.show()\n\n\n\n\n\n# coastline only\nfig2 = pygmt.Figure()\nfig2.grdimage(topo.topo, region='42/102/-12/32', cmap='topo', projection='M4i')\nfig2.coast(\n    region='42/102/-12/32',\n    projection='M4i',\n    shorelines=True\n)\nfig2.colorbar()\nfig2.show()\n\n\n\n\n\n# wind direction\ngrid = pygmt.datasets.load_earth_relief(resolution=\"15m\", region=[42, 102, -12, 32])\n\nfig3 = pygmt.Figure()\nfig3.grdimage(ds.v_wind.isel(time=0), region='42/102/-12/32', projection='M4i')\nfig3.grdcontour(grid=grid,\n              annotation='1000+f6p', # font 6p\n              interval=1000, # every 1000m elevation\n              limit=[-5000, 0],\n              projection='M4i',\n              pen=\"a0.15p\") # 0.15 point\nfig3.coast(\n    region='42/102/-12/32',\n    projection='M4i',\n    shorelines=\"1p,black,solid\"\n)\nfig3.colorbar()\nfig3.show()\n\n\n\n\n\n# wind direction\nfig3 = pygmt.Figure()\nfig3.grdimage(ds.direction.isel(time=0), region='42/102/-12/32', projection='M4i')\nfig3.coast(\n    region='42/102/-12/32',\n    projection='M4i',\n    shorelines=True\n)\nfig3.colorbar()\nfig3.show()\n\n\nfig, ax = plt.subplots(subplot_kw=dict(projection=cartopy.crs.PlateCarree()))\nds.wind_dir.isel(time=0).plot.imshow(ax=ax)\nax.set_aspect('equal')\nax.add_feature(cartopy.feature.COASTLINE)\n\n&lt;cartopy.mpl.feature_artist.FeatureArtist at 0x7fb9c8dc6670&gt;"
  },
  {
    "objectID": "python-notebooks/02_Using_Data.html#about-our-product",
    "href": "python-notebooks/02_Using_Data.html#about-our-product",
    "title": "IO Zarr file",
    "section": "About our product",
    "text": "About our product\nOur final product INDIAN_OCEAN_025GRID_DAILY.zarr is a blended dataset formated as a .zarr file, containing daily cleaned and interpolated data from fifteen variables across multiple sources, mostly from processed NASA/NOAA and Copernicus collections: * adt: sea surface height above geoid (m) * air_temp: air temperature (K) * CHL: chlorophyll-a concentration (mg/m3) * CHL_uncertainty: chlorophyll-a concentration uncertainty (%) * mlotst: mean ocean mixed layer thickness (m) * sla: sea level anomaly (m) * so: sea salinity concentration (m-3 or PSL) * sst: sea surface temperature (K) * topo: topography (m) * u_curr: u-component of total surface currents (m/s) * v_curr: v-component of total surface currents (m/s) * ug_curr: u-component of geostrophic surface currents (m/s) * vg_curr: v-component of geostrophic surface currents (m/s) * u_wind: u-component of surface wind (m/s) * v_wind: v-component of surface wind (m/s)\nWe also computed four additional components, namely: * curr_speed: total current speed (m/s) * curr_dir: total current direction (degrees) * wind_speed: surface wind speed (m/s) * wind_dir: surface wind direction (degrees)\nAll variables have been broadcasted to all fit in the temporal range we have. Therefore, not all variable data are available at any time. Check out each individual variable before use."
  },
  {
    "objectID": "python-notebooks/02_Using_Data.html#import-necessary-libraries",
    "href": "python-notebooks/02_Using_Data.html#import-necessary-libraries",
    "title": "IO Zarr file",
    "section": "Import necessary libraries",
    "text": "Import necessary libraries\n\n! pip install cmocean\n\nCollecting cmocean\n  Using cached cmocean-3.0.3-py3-none-any.whl (222 kB)\nRequirement already satisfied: matplotlib in /srv/conda/envs/notebook/lib/python3.9/site-packages (from cmocean) (3.7.1)\nRequirement already satisfied: numpy in /srv/conda/envs/notebook/lib/python3.9/site-packages (from cmocean) (1.23.5)\nRequirement already satisfied: packaging in /srv/conda/envs/notebook/lib/python3.9/site-packages (from cmocean) (23.1)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from matplotlib-&gt;cmocean) (1.0.7)\nRequirement already satisfied: cycler&gt;=0.10 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from matplotlib-&gt;cmocean) (0.11.0)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from matplotlib-&gt;cmocean) (4.39.4)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from matplotlib-&gt;cmocean) (1.4.4)\nRequirement already satisfied: pillow&gt;=6.2.0 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from matplotlib-&gt;cmocean) (9.5.0)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from matplotlib-&gt;cmocean) (3.0.9)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from matplotlib-&gt;cmocean) (2.8.2)\nRequirement already satisfied: importlib-resources&gt;=3.2.0 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from matplotlib-&gt;cmocean) (5.12.0)\nRequirement already satisfied: zipp&gt;=3.1.0 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from importlib-resources&gt;=3.2.0-&gt;matplotlib-&gt;cmocean) (3.15.0)\nRequirement already satisfied: six&gt;=1.5 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib-&gt;cmocean) (1.16.0)\nInstalling collected packages: cmocean\nSuccessfully installed cmocean-3.0.3\n\n\n\nimport xarray as xr\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt # simple plotting\nimport holoviews as hv # simple plotting\nimport hvplot.xarray # simple plotting"
  },
  {
    "objectID": "python-notebooks/02_Using_Data.html#read-data",
    "href": "python-notebooks/02_Using_Data.html#read-data",
    "title": "IO Zarr file",
    "section": "Read data",
    "text": "Read data\n\nds = xr.open_zarr('/home/jovyan/shared/data/INDIAN_OCEAN_025GRID_DAILY.zarr/')\n\n\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:          (time: 16071, lat: 177, lon: 241)\nCoordinates:\n  * lat              (lat) float32 32.0 31.75 31.5 31.25 ... -11.5 -11.75 -12.0\n  * lon              (lon) float32 42.0 42.25 42.5 42.75 ... 101.5 101.8 102.0\n  * time             (time) datetime64[ns] 1979-01-01 1979-01-02 ... 2022-12-31\nData variables: (12/19)\n    CHL              (time, lat, lon) float32 dask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\n    CHL_uncertainty  (time, lat, lon) float32 dask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\n    adt              (time, lat, lon) float32 dask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\n    air_temp         (time, lat, lon) float32 dask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\n    curr_dir         (time, lat, lon) float32 dask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\n    curr_speed       (time, lat, lon) float32 dask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\n    ...               ...\n    ug_curr          (time, lat, lon) float32 dask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\n    v_curr           (time, lat, lon) float32 dask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\n    v_wind           (time, lat, lon) float32 dask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\n    vg_curr          (time, lat, lon) float32 dask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\n    wind_dir         (time, lat, lon) float32 dask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\n    wind_speed       (time, lat, lon) float32 dask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\nAttributes: (12/17)\n    creator_email:              minhphan@uw.edu\n    creator_name:               Minh Phan\n    creator_type:               person\n    date_created:               2023-07-19\n    geospatial_lat_max:         32.0\n    geospatial_lat_min:         -12.0\n    ...                         ...\n    geospatial_lon_units:       degrees_east\n    source:                     Earth & Space Research (ESR), Copernicus Clim...\n    summary:                    Daily mean of 0.25 x 0.25 degrees gridded dat...\n    time_coverage_end:          2022-12-31T23:59:59\n    time_coverage_start:        1979-01-01T00:00:00\n    title:                      Climate Data for Coastal Upwelling Machine Le...xarray.DatasetDimensions:time: 16071lat: 177lon: 241Coordinates: (3)lat(lat)float3232.0 31.75 31.5 ... -11.75 -12.0long_name :latitudestandard_name :latitudeunits :degrees_northarray([ 32.  ,  31.75,  31.5 ,  31.25,  31.  ,  30.75,  30.5 ,  30.25,  30.  ,\n        29.75,  29.5 ,  29.25,  29.  ,  28.75,  28.5 ,  28.25,  28.  ,  27.75,\n        27.5 ,  27.25,  27.  ,  26.75,  26.5 ,  26.25,  26.  ,  25.75,  25.5 ,\n        25.25,  25.  ,  24.75,  24.5 ,  24.25,  24.  ,  23.75,  23.5 ,  23.25,\n        23.  ,  22.75,  22.5 ,  22.25,  22.  ,  21.75,  21.5 ,  21.25,  21.  ,\n        20.75,  20.5 ,  20.25,  20.  ,  19.75,  19.5 ,  19.25,  19.  ,  18.75,\n        18.5 ,  18.25,  18.  ,  17.75,  17.5 ,  17.25,  17.  ,  16.75,  16.5 ,\n        16.25,  16.  ,  15.75,  15.5 ,  15.25,  15.  ,  14.75,  14.5 ,  14.25,\n        14.  ,  13.75,  13.5 ,  13.25,  13.  ,  12.75,  12.5 ,  12.25,  12.  ,\n        11.75,  11.5 ,  11.25,  11.  ,  10.75,  10.5 ,  10.25,  10.  ,   9.75,\n         9.5 ,   9.25,   9.  ,   8.75,   8.5 ,   8.25,   8.  ,   7.75,   7.5 ,\n         7.25,   7.  ,   6.75,   6.5 ,   6.25,   6.  ,   5.75,   5.5 ,   5.25,\n         5.  ,   4.75,   4.5 ,   4.25,   4.  ,   3.75,   3.5 ,   3.25,   3.  ,\n         2.75,   2.5 ,   2.25,   2.  ,   1.75,   1.5 ,   1.25,   1.  ,   0.75,\n         0.5 ,   0.25,   0.  ,  -0.25,  -0.5 ,  -0.75,  -1.  ,  -1.25,  -1.5 ,\n        -1.75,  -2.  ,  -2.25,  -2.5 ,  -2.75,  -3.  ,  -3.25,  -3.5 ,  -3.75,\n        -4.  ,  -4.25,  -4.5 ,  -4.75,  -5.  ,  -5.25,  -5.5 ,  -5.75,  -6.  ,\n        -6.25,  -6.5 ,  -6.75,  -7.  ,  -7.25,  -7.5 ,  -7.75,  -8.  ,  -8.25,\n        -8.5 ,  -8.75,  -9.  ,  -9.25,  -9.5 ,  -9.75, -10.  , -10.25, -10.5 ,\n       -10.75, -11.  , -11.25, -11.5 , -11.75, -12.  ], dtype=float32)lon(lon)float3242.0 42.25 42.5 ... 101.8 102.0long_name :longitudestandard_name :longitudeunits :degrees_eastarray([ 42.  ,  42.25,  42.5 , ..., 101.5 , 101.75, 102.  ], dtype=float32)time(time)datetime64[ns]1979-01-01 ... 2022-12-31axis :Tcomment :Data is averaged over the daylong_name :time centered on the daystandard_name :timetime_bounds :2000-01-01 00:00:00 to 2000-01-01 23:59:59array(['1979-01-01T00:00:00.000000000', '1979-01-02T00:00:00.000000000',\n       '1979-01-03T00:00:00.000000000', ..., '2022-12-29T00:00:00.000000000',\n       '2022-12-30T00:00:00.000000000', '2022-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (19)CHL(time, lat, lon)float32dask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;_ChunkSizes :[1, 256, 256]ancillary_variables :flags CHL_uncertaintycoverage_content_type :modelResultinput_files_reprocessings :Processors versions: MODIS R2022.0NRT/VIIRSN R2022.0NRT/OLCIA 07.02/VIIRSJ1 R2022.0NRT/OLCIB 07.02long_name :Chlorophyll-a concentration - Mean of the binned pixelsstandard_name :mass_concentration_of_chlorophyll_a_in_sea_watertype :surfaceunits :milligram m-3valid_max :1000.0valid_min :0.0\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.55 GiB\n16.27 MiB\n\n\nShape\n(16071, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n161 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nCHL_uncertainty\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\n\n\n\n\n_ChunkSizes :\n\n[1, 256, 256]\n\ncoverage_content_type :\n\nqualityInformation\n\nlong_name :\n\nChlorophyll-a concentration - Uncertainty estimation\n\nunits :\n\n%\n\nvalid_max :\n\n32767\n\nvalid_min :\n\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.55 GiB\n16.27 MiB\n\n\nShape\n(16071, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n161 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nadt\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nThe absolute dynamic topography is the sea surface height above geoid; the adt is obtained as follows: adt=sla+mdt where mdt is the mean dynamic topography; see the product user manual for details\n\ngrid_mapping :\n\ncrs\n\nlong_name :\n\nAbsolute dynamic topography\n\nstandard_name :\n\nsea_surface_height_above_geoid\n\nunits :\n\nm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.55 GiB\n16.27 MiB\n\n\nShape\n(16071, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n161 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nair_temp\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n2 metre temperature\n\nnameCDM :\n\n2_metre_temperature_surface\n\nnameECMWF :\n\n2 metre temperature\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n2t\n\nstandard_name :\n\nair_temperature\n\nunits :\n\nK\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.55 GiB\n16.27 MiB\n\n\nShape\n(16071, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n161 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\ncurr_dir\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\n\n\n\n\ncomments :\n\nComputed from total surface current velocity elements. Velocities are an average over the top 30m of the mixed layer\n\ndepth :\n\n15m\n\nlong_name :\n\naverage direction of total surface currents\n\nunits :\n\ndegrees\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.55 GiB\n16.27 MiB\n\n\nShape\n(16071, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n161 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\ncurr_speed\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\n\n\n\n\ncomments :\n\nVelocities are an average over the top 30m of the mixed layer\n\ndepth :\n\n15m\n\nlong_name :\n\naverage total surface current speed\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.55 GiB\n16.27 MiB\n\n\nShape\n(16071, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n161 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nmlotst\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(500, 177, 241), meta=np.ndarray&gt;\n\n\n\n\n_ChunkSizes :\n\n[1, 681, 1440]\n\ncell_methods :\n\narea: mean\n\nlong_name :\n\nDensity ocean mixed layer thickness\n\nstandard_name :\n\nocean_mixed_layer_thickness_defined_by_sigma_theta\n\nunit_long :\n\nMeters\n\nunits :\n\nm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.55 GiB\n81.36 MiB\n\n\nShape\n(16071, 177, 241)\n(500, 177, 241)\n\n\nDask graph\n33 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nsla\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nancillary_variables :\n\nerr_sla\n\ncomment :\n\nThe sea level anomaly is the sea surface height above mean sea surface; it is referenced to the [1993, 2012] period; see the product user manual for details\n\ngrid_mapping :\n\ncrs\n\nlong_name :\n\nSea level anomaly\n\nstandard_name :\n\nsea_surface_height_above_sea_level\n\nunits :\n\nm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.55 GiB\n16.27 MiB\n\n\nShape\n(16071, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n161 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nso\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(500, 177, 241), meta=np.ndarray&gt;\n\n\n\n\n_ChunkSizes :\n\n[1, 7, 341, 720]\n\ncell_methods :\n\narea: mean\n\nlong_name :\n\nmean sea water salinity at 0.49 metres below ocean surface\n\nstandard_name :\n\nsea_water_salinity\n\nunit_long :\n\nPractical Salinity Unit\n\nunits :\n\n1e-3\n\nvalid_max :\n\n28336\n\nvalid_min :\n\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.55 GiB\n81.36 MiB\n\n\nShape\n(16071, 177, 241)\n(500, 177, 241)\n\n\nDask graph\n33 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nsst\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nSea surface temperature\n\nnameCDM :\n\nSea_surface_temperature_surface\n\nnameECMWF :\n\nSea surface temperature\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\nsst\n\nstandard_name :\n\nsea_surface_temperature\n\nunits :\n\nK\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.55 GiB\n16.27 MiB\n\n\nShape\n(16071, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n161 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\ntopo\n\n\n(lat, lon)\n\n\nfloat64\n\n\ndask.array&lt;chunksize=(177, 241), meta=np.ndarray&gt;\n\n\n\n\ncolorBarMaximum :\n\n8000.0\n\ncolorBarMinimum :\n\n-8000.0\n\ncolorBarPalette :\n\nTopography\n\ngrid_mapping :\n\nGDAL_Geographics\n\nioos_category :\n\nLocation\n\nlong_name :\n\nTopography\n\nstandard_name :\n\naltitude\n\nunits :\n\nmeters\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n333.26 kiB\n333.26 kiB\n\n\nShape\n(177, 241)\n(177, 241)\n\n\nDask graph\n1 chunks in 2 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n\n\n\n\n\nu_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nVelocities are an average over the top 30m of the mixed layer\n\ncoverage_content_type :\n\nmodelResult\n\ndepth :\n\n15m\n\nlong_name :\n\nzonal total surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148 ; WIND source: ECMWF ERA5 10m wind DOI: 10.24381/cds.adbb2d47 ; SST source: CMC 0.2 deg SST V2.0 DOI: 10.5067/GHCMC-4FM02\n\nstandard_name :\n\neastward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.55 GiB\n16.27 MiB\n\n\nShape\n(16071, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n161 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nu_wind\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre U wind component\n\nnameCDM :\n\n10_metre_U_wind_component_surface\n\nnameECMWF :\n\n10 metre U wind component\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n10u\n\nstandard_name :\n\neastward_wind\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.55 GiB\n16.27 MiB\n\n\nShape\n(16071, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n161 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nug_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nGeostrophic velocities calculated from absolute dynamic topography\n\ndepth :\n\n15m\n\nlong_name :\n\nzonal geostrophic surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148\n\nstandard_name :\n\ngeostrophic_eastward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.55 GiB\n16.27 MiB\n\n\nShape\n(16071, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n161 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nv_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nVelocities are an average over the top 30m of the mixed layer\n\ncoverage_content_type :\n\nmodelResult\n\ndepth :\n\n15m\n\nlong_name :\n\nmeridional total surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148 ; WIND source: ECMWF ERA5 10m wind DOI: 10.24381/cds.adbb2d47 ; SST source: CMC 0.2 deg SST V2.0 DOI: 10.5067/GHCMC-4FM02\n\nstandard_name :\n\nnorthward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.55 GiB\n16.27 MiB\n\n\nShape\n(16071, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n161 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nv_wind\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre V wind component\n\nnameCDM :\n\n10_metre_V_wind_component_surface\n\nnameECMWF :\n\n10 metre V wind component\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n10v\n\nstandard_name :\n\nnorthward_wind\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.55 GiB\n16.27 MiB\n\n\nShape\n(16071, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n161 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nvg_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nGeostrophic velocities calculated from absolute dynamic topography\n\ndepth :\n\n15m\n\nlong_name :\n\nmeridional geostrophic surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148\n\nstandard_name :\n\ngeostrophic_northward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.55 GiB\n16.27 MiB\n\n\nShape\n(16071, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n161 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nwind_dir\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre wind direction\n\nunits :\n\ndegrees\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.55 GiB\n16.27 MiB\n\n\nShape\n(16071, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n161 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nwind_speed\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre absolute speed\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.55 GiB\n16.27 MiB\n\n\nShape\n(16071, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n161 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nIndexes: (3)latPandasIndexPandasIndex(Index([  32.0,  31.75,   31.5,  31.25,   31.0,  30.75,   30.5,  30.25,   30.0,\n        29.75,\n       ...\n        -9.75,  -10.0, -10.25,  -10.5, -10.75,  -11.0, -11.25,  -11.5, -11.75,\n        -12.0],\n      dtype='float32', name='lat', length=177))lonPandasIndexPandasIndex(Index([  42.0,  42.25,   42.5,  42.75,   43.0,  43.25,   43.5,  43.75,   44.0,\n        44.25,\n       ...\n        99.75,  100.0, 100.25,  100.5, 100.75,  101.0, 101.25,  101.5, 101.75,\n        102.0],\n      dtype='float32', name='lon', length=241))timePandasIndexPandasIndex(DatetimeIndex(['1979-01-01', '1979-01-02', '1979-01-03', '1979-01-04',\n               '1979-01-05', '1979-01-06', '1979-01-07', '1979-01-08',\n               '1979-01-09', '1979-01-10',\n               ...\n               '2022-12-22', '2022-12-23', '2022-12-24', '2022-12-25',\n               '2022-12-26', '2022-12-27', '2022-12-28', '2022-12-29',\n               '2022-12-30', '2022-12-31'],\n              dtype='datetime64[ns]', name='time', length=16071, freq=None))Attributes: (17)creator_email :minhphan@uw.educreator_name :Minh Phancreator_type :persondate_created :2023-07-19geospatial_lat_max :32.0geospatial_lat_min :-12.0geospatial_lat_resolution :0.25geospatial_lat_units :degrees_northgeospatial_lon_max :102.0geospatial_lon_min :42.0geospatial_lon_resolution :0.25geospatial_lon_units :degrees_eastsource :Earth & Space Research (ESR), Copernicus Climate Change Service (C3S), Copernicus Marine Environment Monitoring Service (CMEMS), United States Geological Survey (USGS)summary :Daily mean of 0.25 x 0.25 degrees gridded data from multiple climate variables that may influence the patterns of coastal upwelling in the focused areatime_coverage_end :2022-12-31T23:59:59time_coverage_start :1979-01-01T00:00:00title :Climate Data for Coastal Upwelling Machine Learning Project in Indian Ocean\n\n\nWe can slice data by the dimensions (latitude, longitude, time) and data variables.\n\n# slice by latitude\n# notice how we specify the range in reverse\nds.sel(lat=slice(0, -12))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:          (time: 16071, lat: 49, lon: 241)\nCoordinates:\n  * lat              (lat) float32 0.0 -0.25 -0.5 -0.75 ... -11.5 -11.75 -12.0\n  * lon              (lon) float32 42.0 42.25 42.5 42.75 ... 101.5 101.8 102.0\n  * time             (time) datetime64[ns] 1979-01-01 1979-01-02 ... 2022-12-31\nData variables: (12/19)\n    CHL              (time, lat, lon) float32 dask.array&lt;chunksize=(100, 49, 241), meta=np.ndarray&gt;\n    CHL_uncertainty  (time, lat, lon) float32 dask.array&lt;chunksize=(100, 49, 241), meta=np.ndarray&gt;\n    adt              (time, lat, lon) float32 dask.array&lt;chunksize=(100, 49, 241), meta=np.ndarray&gt;\n    air_temp         (time, lat, lon) float32 dask.array&lt;chunksize=(100, 49, 241), meta=np.ndarray&gt;\n    curr_dir         (time, lat, lon) float32 dask.array&lt;chunksize=(100, 49, 241), meta=np.ndarray&gt;\n    curr_speed       (time, lat, lon) float32 dask.array&lt;chunksize=(100, 49, 241), meta=np.ndarray&gt;\n    ...               ...\n    ug_curr          (time, lat, lon) float32 dask.array&lt;chunksize=(100, 49, 241), meta=np.ndarray&gt;\n    v_curr           (time, lat, lon) float32 dask.array&lt;chunksize=(100, 49, 241), meta=np.ndarray&gt;\n    v_wind           (time, lat, lon) float32 dask.array&lt;chunksize=(100, 49, 241), meta=np.ndarray&gt;\n    vg_curr          (time, lat, lon) float32 dask.array&lt;chunksize=(100, 49, 241), meta=np.ndarray&gt;\n    wind_dir         (time, lat, lon) float32 dask.array&lt;chunksize=(100, 49, 241), meta=np.ndarray&gt;\n    wind_speed       (time, lat, lon) float32 dask.array&lt;chunksize=(100, 49, 241), meta=np.ndarray&gt;\nAttributes: (12/17)\n    creator_email:              minhphan@uw.edu\n    creator_name:               Minh Phan\n    creator_type:               person\n    date_created:               2023-07-19\n    geospatial_lat_max:         32.0\n    geospatial_lat_min:         -12.0\n    ...                         ...\n    geospatial_lon_units:       degrees_east\n    source:                     Earth & Space Research (ESR), Copernicus Clim...\n    summary:                    Daily mean of 0.25 x 0.25 degrees gridded dat...\n    time_coverage_end:          2022-12-31T23:59:59\n    time_coverage_start:        1979-01-01T00:00:00\n    title:                      Climate Data for Coastal Upwelling Machine Le...xarray.DatasetDimensions:time: 16071lat: 49lon: 241Coordinates: (3)lat(lat)float320.0 -0.25 -0.5 ... -11.75 -12.0long_name :latitudestandard_name :latitudeunits :degrees_northarray([  0.  ,  -0.25,  -0.5 ,  -0.75,  -1.  ,  -1.25,  -1.5 ,  -1.75,  -2.  ,\n        -2.25,  -2.5 ,  -2.75,  -3.  ,  -3.25,  -3.5 ,  -3.75,  -4.  ,  -4.25,\n        -4.5 ,  -4.75,  -5.  ,  -5.25,  -5.5 ,  -5.75,  -6.  ,  -6.25,  -6.5 ,\n        -6.75,  -7.  ,  -7.25,  -7.5 ,  -7.75,  -8.  ,  -8.25,  -8.5 ,  -8.75,\n        -9.  ,  -9.25,  -9.5 ,  -9.75, -10.  , -10.25, -10.5 , -10.75, -11.  ,\n       -11.25, -11.5 , -11.75, -12.  ], dtype=float32)lon(lon)float3242.0 42.25 42.5 ... 101.8 102.0long_name :longitudestandard_name :longitudeunits :degrees_eastarray([ 42.  ,  42.25,  42.5 , ..., 101.5 , 101.75, 102.  ], dtype=float32)time(time)datetime64[ns]1979-01-01 ... 2022-12-31axis :Tcomment :Data is averaged over the daylong_name :time centered on the daystandard_name :timetime_bounds :2000-01-01 00:00:00 to 2000-01-01 23:59:59array(['1979-01-01T00:00:00.000000000', '1979-01-02T00:00:00.000000000',\n       '1979-01-03T00:00:00.000000000', ..., '2022-12-29T00:00:00.000000000',\n       '2022-12-30T00:00:00.000000000', '2022-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (19)CHL(time, lat, lon)float32dask.array&lt;chunksize=(100, 49, 241), meta=np.ndarray&gt;_ChunkSizes :[1, 256, 256]ancillary_variables :flags CHL_uncertaintycoverage_content_type :modelResultinput_files_reprocessings :Processors versions: MODIS R2022.0NRT/VIIRSN R2022.0NRT/OLCIA 07.02/VIIRSJ1 R2022.0NRT/OLCIB 07.02long_name :Chlorophyll-a concentration - Mean of the binned pixelsstandard_name :mass_concentration_of_chlorophyll_a_in_sea_watertype :surfaceunits :milligram m-3valid_max :1000.0valid_min :0.0\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n723.96 MiB\n4.50 MiB\n\n\nShape\n(16071, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nCHL_uncertainty\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 49, 241), meta=np.ndarray&gt;\n\n\n\n\n_ChunkSizes :\n\n[1, 256, 256]\n\ncoverage_content_type :\n\nqualityInformation\n\nlong_name :\n\nChlorophyll-a concentration - Uncertainty estimation\n\nunits :\n\n%\n\nvalid_max :\n\n32767\n\nvalid_min :\n\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n723.96 MiB\n4.50 MiB\n\n\nShape\n(16071, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nadt\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 49, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nThe absolute dynamic topography is the sea surface height above geoid; the adt is obtained as follows: adt=sla+mdt where mdt is the mean dynamic topography; see the product user manual for details\n\ngrid_mapping :\n\ncrs\n\nlong_name :\n\nAbsolute dynamic topography\n\nstandard_name :\n\nsea_surface_height_above_geoid\n\nunits :\n\nm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n723.96 MiB\n4.50 MiB\n\n\nShape\n(16071, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nair_temp\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 49, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n2 metre temperature\n\nnameCDM :\n\n2_metre_temperature_surface\n\nnameECMWF :\n\n2 metre temperature\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n2t\n\nstandard_name :\n\nair_temperature\n\nunits :\n\nK\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n723.96 MiB\n4.50 MiB\n\n\nShape\n(16071, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\ncurr_dir\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 49, 241), meta=np.ndarray&gt;\n\n\n\n\ncomments :\n\nComputed from total surface current velocity elements. Velocities are an average over the top 30m of the mixed layer\n\ndepth :\n\n15m\n\nlong_name :\n\naverage direction of total surface currents\n\nunits :\n\ndegrees\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n723.96 MiB\n4.50 MiB\n\n\nShape\n(16071, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\ncurr_speed\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 49, 241), meta=np.ndarray&gt;\n\n\n\n\ncomments :\n\nVelocities are an average over the top 30m of the mixed layer\n\ndepth :\n\n15m\n\nlong_name :\n\naverage total surface current speed\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n723.96 MiB\n4.50 MiB\n\n\nShape\n(16071, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nmlotst\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(500, 49, 241), meta=np.ndarray&gt;\n\n\n\n\n_ChunkSizes :\n\n[1, 681, 1440]\n\ncell_methods :\n\narea: mean\n\nlong_name :\n\nDensity ocean mixed layer thickness\n\nstandard_name :\n\nocean_mixed_layer_thickness_defined_by_sigma_theta\n\nunit_long :\n\nMeters\n\nunits :\n\nm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n723.96 MiB\n22.52 MiB\n\n\nShape\n(16071, 49, 241)\n(500, 49, 241)\n\n\nDask graph\n33 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nsla\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 49, 241), meta=np.ndarray&gt;\n\n\n\n\nancillary_variables :\n\nerr_sla\n\ncomment :\n\nThe sea level anomaly is the sea surface height above mean sea surface; it is referenced to the [1993, 2012] period; see the product user manual for details\n\ngrid_mapping :\n\ncrs\n\nlong_name :\n\nSea level anomaly\n\nstandard_name :\n\nsea_surface_height_above_sea_level\n\nunits :\n\nm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n723.96 MiB\n4.50 MiB\n\n\nShape\n(16071, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nso\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(500, 49, 241), meta=np.ndarray&gt;\n\n\n\n\n_ChunkSizes :\n\n[1, 7, 341, 720]\n\ncell_methods :\n\narea: mean\n\nlong_name :\n\nmean sea water salinity at 0.49 metres below ocean surface\n\nstandard_name :\n\nsea_water_salinity\n\nunit_long :\n\nPractical Salinity Unit\n\nunits :\n\n1e-3\n\nvalid_max :\n\n28336\n\nvalid_min :\n\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n723.96 MiB\n22.52 MiB\n\n\nShape\n(16071, 49, 241)\n(500, 49, 241)\n\n\nDask graph\n33 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nsst\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 49, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nSea surface temperature\n\nnameCDM :\n\nSea_surface_temperature_surface\n\nnameECMWF :\n\nSea surface temperature\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\nsst\n\nstandard_name :\n\nsea_surface_temperature\n\nunits :\n\nK\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n723.96 MiB\n4.50 MiB\n\n\nShape\n(16071, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\ntopo\n\n\n(lat, lon)\n\n\nfloat64\n\n\ndask.array&lt;chunksize=(49, 241), meta=np.ndarray&gt;\n\n\n\n\ncolorBarMaximum :\n\n8000.0\n\ncolorBarMinimum :\n\n-8000.0\n\ncolorBarPalette :\n\nTopography\n\ngrid_mapping :\n\nGDAL_Geographics\n\nioos_category :\n\nLocation\n\nlong_name :\n\nTopography\n\nstandard_name :\n\naltitude\n\nunits :\n\nmeters\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n92.26 kiB\n92.26 kiB\n\n\nShape\n(49, 241)\n(49, 241)\n\n\nDask graph\n1 chunks in 3 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n\n\n\n\n\nu_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 49, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nVelocities are an average over the top 30m of the mixed layer\n\ncoverage_content_type :\n\nmodelResult\n\ndepth :\n\n15m\n\nlong_name :\n\nzonal total surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148 ; WIND source: ECMWF ERA5 10m wind DOI: 10.24381/cds.adbb2d47 ; SST source: CMC 0.2 deg SST V2.0 DOI: 10.5067/GHCMC-4FM02\n\nstandard_name :\n\neastward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n723.96 MiB\n4.50 MiB\n\n\nShape\n(16071, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nu_wind\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 49, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre U wind component\n\nnameCDM :\n\n10_metre_U_wind_component_surface\n\nnameECMWF :\n\n10 metre U wind component\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n10u\n\nstandard_name :\n\neastward_wind\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n723.96 MiB\n4.50 MiB\n\n\nShape\n(16071, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nug_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 49, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nGeostrophic velocities calculated from absolute dynamic topography\n\ndepth :\n\n15m\n\nlong_name :\n\nzonal geostrophic surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148\n\nstandard_name :\n\ngeostrophic_eastward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n723.96 MiB\n4.50 MiB\n\n\nShape\n(16071, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nv_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 49, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nVelocities are an average over the top 30m of the mixed layer\n\ncoverage_content_type :\n\nmodelResult\n\ndepth :\n\n15m\n\nlong_name :\n\nmeridional total surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148 ; WIND source: ECMWF ERA5 10m wind DOI: 10.24381/cds.adbb2d47 ; SST source: CMC 0.2 deg SST V2.0 DOI: 10.5067/GHCMC-4FM02\n\nstandard_name :\n\nnorthward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n723.96 MiB\n4.50 MiB\n\n\nShape\n(16071, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nv_wind\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 49, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre V wind component\n\nnameCDM :\n\n10_metre_V_wind_component_surface\n\nnameECMWF :\n\n10 metre V wind component\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n10v\n\nstandard_name :\n\nnorthward_wind\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n723.96 MiB\n4.50 MiB\n\n\nShape\n(16071, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nvg_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 49, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nGeostrophic velocities calculated from absolute dynamic topography\n\ndepth :\n\n15m\n\nlong_name :\n\nmeridional geostrophic surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148\n\nstandard_name :\n\ngeostrophic_northward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n723.96 MiB\n4.50 MiB\n\n\nShape\n(16071, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nwind_dir\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 49, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre wind direction\n\nunits :\n\ndegrees\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n723.96 MiB\n4.50 MiB\n\n\nShape\n(16071, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nwind_speed\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 49, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre absolute speed\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n723.96 MiB\n4.50 MiB\n\n\nShape\n(16071, 49, 241)\n(100, 49, 241)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nIndexes: (3)latPandasIndexPandasIndex(Index([   0.0,  -0.25,   -0.5,  -0.75,   -1.0,  -1.25,   -1.5,  -1.75,   -2.0,\n        -2.25,   -2.5,  -2.75,   -3.0,  -3.25,   -3.5,  -3.75,   -4.0,  -4.25,\n         -4.5,  -4.75,   -5.0,  -5.25,   -5.5,  -5.75,   -6.0,  -6.25,   -6.5,\n        -6.75,   -7.0,  -7.25,   -7.5,  -7.75,   -8.0,  -8.25,   -8.5,  -8.75,\n         -9.0,  -9.25,   -9.5,  -9.75,  -10.0, -10.25,  -10.5, -10.75,  -11.0,\n       -11.25,  -11.5, -11.75,  -12.0],\n      dtype='float32', name='lat'))lonPandasIndexPandasIndex(Index([  42.0,  42.25,   42.5,  42.75,   43.0,  43.25,   43.5,  43.75,   44.0,\n        44.25,\n       ...\n        99.75,  100.0, 100.25,  100.5, 100.75,  101.0, 101.25,  101.5, 101.75,\n        102.0],\n      dtype='float32', name='lon', length=241))timePandasIndexPandasIndex(DatetimeIndex(['1979-01-01', '1979-01-02', '1979-01-03', '1979-01-04',\n               '1979-01-05', '1979-01-06', '1979-01-07', '1979-01-08',\n               '1979-01-09', '1979-01-10',\n               ...\n               '2022-12-22', '2022-12-23', '2022-12-24', '2022-12-25',\n               '2022-12-26', '2022-12-27', '2022-12-28', '2022-12-29',\n               '2022-12-30', '2022-12-31'],\n              dtype='datetime64[ns]', name='time', length=16071, freq=None))Attributes: (17)creator_email :minhphan@uw.educreator_name :Minh Phancreator_type :persondate_created :2023-07-19geospatial_lat_max :32.0geospatial_lat_min :-12.0geospatial_lat_resolution :0.25geospatial_lat_units :degrees_northgeospatial_lon_max :102.0geospatial_lon_min :42.0geospatial_lon_resolution :0.25geospatial_lon_units :degrees_eastsource :Earth & Space Research (ESR), Copernicus Climate Change Service (C3S), Copernicus Marine Environment Monitoring Service (CMEMS), United States Geological Survey (USGS)summary :Daily mean of 0.25 x 0.25 degrees gridded data from multiple climate variables that may influence the patterns of coastal upwelling in the focused areatime_coverage_end :2022-12-31T23:59:59time_coverage_start :1979-01-01T00:00:00title :Climate Data for Coastal Upwelling Machine Learning Project in Indian Ocean\n\n\n\n# slice by longitude\nds.sel(lon=slice(42, 45))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:          (time: 16071, lat: 177, lon: 13)\nCoordinates:\n  * lat              (lat) float32 32.0 31.75 31.5 31.25 ... -11.5 -11.75 -12.0\n  * lon              (lon) float32 42.0 42.25 42.5 42.75 ... 44.5 44.75 45.0\n  * time             (time) datetime64[ns] 1979-01-01 1979-01-02 ... 2022-12-31\nData variables: (12/19)\n    CHL              (time, lat, lon) float32 dask.array&lt;chunksize=(100, 177, 13), meta=np.ndarray&gt;\n    CHL_uncertainty  (time, lat, lon) float32 dask.array&lt;chunksize=(100, 177, 13), meta=np.ndarray&gt;\n    adt              (time, lat, lon) float32 dask.array&lt;chunksize=(100, 177, 13), meta=np.ndarray&gt;\n    air_temp         (time, lat, lon) float32 dask.array&lt;chunksize=(100, 177, 13), meta=np.ndarray&gt;\n    curr_dir         (time, lat, lon) float32 dask.array&lt;chunksize=(100, 177, 13), meta=np.ndarray&gt;\n    curr_speed       (time, lat, lon) float32 dask.array&lt;chunksize=(100, 177, 13), meta=np.ndarray&gt;\n    ...               ...\n    ug_curr          (time, lat, lon) float32 dask.array&lt;chunksize=(100, 177, 13), meta=np.ndarray&gt;\n    v_curr           (time, lat, lon) float32 dask.array&lt;chunksize=(100, 177, 13), meta=np.ndarray&gt;\n    v_wind           (time, lat, lon) float32 dask.array&lt;chunksize=(100, 177, 13), meta=np.ndarray&gt;\n    vg_curr          (time, lat, lon) float32 dask.array&lt;chunksize=(100, 177, 13), meta=np.ndarray&gt;\n    wind_dir         (time, lat, lon) float32 dask.array&lt;chunksize=(100, 177, 13), meta=np.ndarray&gt;\n    wind_speed       (time, lat, lon) float32 dask.array&lt;chunksize=(100, 177, 13), meta=np.ndarray&gt;\nAttributes: (12/17)\n    creator_email:              minhphan@uw.edu\n    creator_name:               Minh Phan\n    creator_type:               person\n    date_created:               2023-07-19\n    geospatial_lat_max:         32.0\n    geospatial_lat_min:         -12.0\n    ...                         ...\n    geospatial_lon_units:       degrees_east\n    source:                     Earth & Space Research (ESR), Copernicus Clim...\n    summary:                    Daily mean of 0.25 x 0.25 degrees gridded dat...\n    time_coverage_end:          2022-12-31T23:59:59\n    time_coverage_start:        1979-01-01T00:00:00\n    title:                      Climate Data for Coastal Upwelling Machine Le...xarray.DatasetDimensions:time: 16071lat: 177lon: 13Coordinates: (3)lat(lat)float3232.0 31.75 31.5 ... -11.75 -12.0long_name :latitudestandard_name :latitudeunits :degrees_northarray([ 32.  ,  31.75,  31.5 ,  31.25,  31.  ,  30.75,  30.5 ,  30.25,  30.  ,\n        29.75,  29.5 ,  29.25,  29.  ,  28.75,  28.5 ,  28.25,  28.  ,  27.75,\n        27.5 ,  27.25,  27.  ,  26.75,  26.5 ,  26.25,  26.  ,  25.75,  25.5 ,\n        25.25,  25.  ,  24.75,  24.5 ,  24.25,  24.  ,  23.75,  23.5 ,  23.25,\n        23.  ,  22.75,  22.5 ,  22.25,  22.  ,  21.75,  21.5 ,  21.25,  21.  ,\n        20.75,  20.5 ,  20.25,  20.  ,  19.75,  19.5 ,  19.25,  19.  ,  18.75,\n        18.5 ,  18.25,  18.  ,  17.75,  17.5 ,  17.25,  17.  ,  16.75,  16.5 ,\n        16.25,  16.  ,  15.75,  15.5 ,  15.25,  15.  ,  14.75,  14.5 ,  14.25,\n        14.  ,  13.75,  13.5 ,  13.25,  13.  ,  12.75,  12.5 ,  12.25,  12.  ,\n        11.75,  11.5 ,  11.25,  11.  ,  10.75,  10.5 ,  10.25,  10.  ,   9.75,\n         9.5 ,   9.25,   9.  ,   8.75,   8.5 ,   8.25,   8.  ,   7.75,   7.5 ,\n         7.25,   7.  ,   6.75,   6.5 ,   6.25,   6.  ,   5.75,   5.5 ,   5.25,\n         5.  ,   4.75,   4.5 ,   4.25,   4.  ,   3.75,   3.5 ,   3.25,   3.  ,\n         2.75,   2.5 ,   2.25,   2.  ,   1.75,   1.5 ,   1.25,   1.  ,   0.75,\n         0.5 ,   0.25,   0.  ,  -0.25,  -0.5 ,  -0.75,  -1.  ,  -1.25,  -1.5 ,\n        -1.75,  -2.  ,  -2.25,  -2.5 ,  -2.75,  -3.  ,  -3.25,  -3.5 ,  -3.75,\n        -4.  ,  -4.25,  -4.5 ,  -4.75,  -5.  ,  -5.25,  -5.5 ,  -5.75,  -6.  ,\n        -6.25,  -6.5 ,  -6.75,  -7.  ,  -7.25,  -7.5 ,  -7.75,  -8.  ,  -8.25,\n        -8.5 ,  -8.75,  -9.  ,  -9.25,  -9.5 ,  -9.75, -10.  , -10.25, -10.5 ,\n       -10.75, -11.  , -11.25, -11.5 , -11.75, -12.  ], dtype=float32)lon(lon)float3242.0 42.25 42.5 ... 44.5 44.75 45.0long_name :longitudestandard_name :longitudeunits :degrees_eastarray([42.  , 42.25, 42.5 , 42.75, 43.  , 43.25, 43.5 , 43.75, 44.  , 44.25,\n       44.5 , 44.75, 45.  ], dtype=float32)time(time)datetime64[ns]1979-01-01 ... 2022-12-31axis :Tcomment :Data is averaged over the daylong_name :time centered on the daystandard_name :timetime_bounds :2000-01-01 00:00:00 to 2000-01-01 23:59:59array(['1979-01-01T00:00:00.000000000', '1979-01-02T00:00:00.000000000',\n       '1979-01-03T00:00:00.000000000', ..., '2022-12-29T00:00:00.000000000',\n       '2022-12-30T00:00:00.000000000', '2022-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (19)CHL(time, lat, lon)float32dask.array&lt;chunksize=(100, 177, 13), meta=np.ndarray&gt;_ChunkSizes :[1, 256, 256]ancillary_variables :flags CHL_uncertaintycoverage_content_type :modelResultinput_files_reprocessings :Processors versions: MODIS R2022.0NRT/VIIRSN R2022.0NRT/OLCIA 07.02/VIIRSJ1 R2022.0NRT/OLCIB 07.02long_name :Chlorophyll-a concentration - Mean of the binned pixelsstandard_name :mass_concentration_of_chlorophyll_a_in_sea_watertype :surfaceunits :milligram m-3valid_max :1000.0valid_min :0.0\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n141.07 MiB\n898.83 kiB\n\n\nShape\n(16071, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nCHL_uncertainty\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 13), meta=np.ndarray&gt;\n\n\n\n\n_ChunkSizes :\n\n[1, 256, 256]\n\ncoverage_content_type :\n\nqualityInformation\n\nlong_name :\n\nChlorophyll-a concentration - Uncertainty estimation\n\nunits :\n\n%\n\nvalid_max :\n\n32767\n\nvalid_min :\n\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n141.07 MiB\n898.83 kiB\n\n\nShape\n(16071, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nadt\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 13), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nThe absolute dynamic topography is the sea surface height above geoid; the adt is obtained as follows: adt=sla+mdt where mdt is the mean dynamic topography; see the product user manual for details\n\ngrid_mapping :\n\ncrs\n\nlong_name :\n\nAbsolute dynamic topography\n\nstandard_name :\n\nsea_surface_height_above_geoid\n\nunits :\n\nm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n141.07 MiB\n898.83 kiB\n\n\nShape\n(16071, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nair_temp\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 13), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n2 metre temperature\n\nnameCDM :\n\n2_metre_temperature_surface\n\nnameECMWF :\n\n2 metre temperature\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n2t\n\nstandard_name :\n\nair_temperature\n\nunits :\n\nK\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n141.07 MiB\n898.83 kiB\n\n\nShape\n(16071, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\ncurr_dir\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 13), meta=np.ndarray&gt;\n\n\n\n\ncomments :\n\nComputed from total surface current velocity elements. Velocities are an average over the top 30m of the mixed layer\n\ndepth :\n\n15m\n\nlong_name :\n\naverage direction of total surface currents\n\nunits :\n\ndegrees\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n141.07 MiB\n898.83 kiB\n\n\nShape\n(16071, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\ncurr_speed\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 13), meta=np.ndarray&gt;\n\n\n\n\ncomments :\n\nVelocities are an average over the top 30m of the mixed layer\n\ndepth :\n\n15m\n\nlong_name :\n\naverage total surface current speed\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n141.07 MiB\n898.83 kiB\n\n\nShape\n(16071, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nmlotst\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(500, 177, 13), meta=np.ndarray&gt;\n\n\n\n\n_ChunkSizes :\n\n[1, 681, 1440]\n\ncell_methods :\n\narea: mean\n\nlong_name :\n\nDensity ocean mixed layer thickness\n\nstandard_name :\n\nocean_mixed_layer_thickness_defined_by_sigma_theta\n\nunit_long :\n\nMeters\n\nunits :\n\nm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n141.07 MiB\n4.39 MiB\n\n\nShape\n(16071, 177, 13)\n(500, 177, 13)\n\n\nDask graph\n33 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nsla\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 13), meta=np.ndarray&gt;\n\n\n\n\nancillary_variables :\n\nerr_sla\n\ncomment :\n\nThe sea level anomaly is the sea surface height above mean sea surface; it is referenced to the [1993, 2012] period; see the product user manual for details\n\ngrid_mapping :\n\ncrs\n\nlong_name :\n\nSea level anomaly\n\nstandard_name :\n\nsea_surface_height_above_sea_level\n\nunits :\n\nm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n141.07 MiB\n898.83 kiB\n\n\nShape\n(16071, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nso\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(500, 177, 13), meta=np.ndarray&gt;\n\n\n\n\n_ChunkSizes :\n\n[1, 7, 341, 720]\n\ncell_methods :\n\narea: mean\n\nlong_name :\n\nmean sea water salinity at 0.49 metres below ocean surface\n\nstandard_name :\n\nsea_water_salinity\n\nunit_long :\n\nPractical Salinity Unit\n\nunits :\n\n1e-3\n\nvalid_max :\n\n28336\n\nvalid_min :\n\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n141.07 MiB\n4.39 MiB\n\n\nShape\n(16071, 177, 13)\n(500, 177, 13)\n\n\nDask graph\n33 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nsst\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 13), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nSea surface temperature\n\nnameCDM :\n\nSea_surface_temperature_surface\n\nnameECMWF :\n\nSea surface temperature\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\nsst\n\nstandard_name :\n\nsea_surface_temperature\n\nunits :\n\nK\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n141.07 MiB\n898.83 kiB\n\n\nShape\n(16071, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\ntopo\n\n\n(lat, lon)\n\n\nfloat64\n\n\ndask.array&lt;chunksize=(177, 13), meta=np.ndarray&gt;\n\n\n\n\ncolorBarMaximum :\n\n8000.0\n\ncolorBarMinimum :\n\n-8000.0\n\ncolorBarPalette :\n\nTopography\n\ngrid_mapping :\n\nGDAL_Geographics\n\nioos_category :\n\nLocation\n\nlong_name :\n\nTopography\n\nstandard_name :\n\naltitude\n\nunits :\n\nmeters\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n17.98 kiB\n17.98 kiB\n\n\nShape\n(177, 13)\n(177, 13)\n\n\nDask graph\n1 chunks in 3 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n\n\n\n\n\nu_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 13), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nVelocities are an average over the top 30m of the mixed layer\n\ncoverage_content_type :\n\nmodelResult\n\ndepth :\n\n15m\n\nlong_name :\n\nzonal total surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148 ; WIND source: ECMWF ERA5 10m wind DOI: 10.24381/cds.adbb2d47 ; SST source: CMC 0.2 deg SST V2.0 DOI: 10.5067/GHCMC-4FM02\n\nstandard_name :\n\neastward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n141.07 MiB\n898.83 kiB\n\n\nShape\n(16071, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nu_wind\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 13), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre U wind component\n\nnameCDM :\n\n10_metre_U_wind_component_surface\n\nnameECMWF :\n\n10 metre U wind component\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n10u\n\nstandard_name :\n\neastward_wind\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n141.07 MiB\n898.83 kiB\n\n\nShape\n(16071, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nug_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 13), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nGeostrophic velocities calculated from absolute dynamic topography\n\ndepth :\n\n15m\n\nlong_name :\n\nzonal geostrophic surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148\n\nstandard_name :\n\ngeostrophic_eastward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n141.07 MiB\n898.83 kiB\n\n\nShape\n(16071, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nv_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 13), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nVelocities are an average over the top 30m of the mixed layer\n\ncoverage_content_type :\n\nmodelResult\n\ndepth :\n\n15m\n\nlong_name :\n\nmeridional total surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148 ; WIND source: ECMWF ERA5 10m wind DOI: 10.24381/cds.adbb2d47 ; SST source: CMC 0.2 deg SST V2.0 DOI: 10.5067/GHCMC-4FM02\n\nstandard_name :\n\nnorthward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n141.07 MiB\n898.83 kiB\n\n\nShape\n(16071, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nv_wind\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 13), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre V wind component\n\nnameCDM :\n\n10_metre_V_wind_component_surface\n\nnameECMWF :\n\n10 metre V wind component\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n10v\n\nstandard_name :\n\nnorthward_wind\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n141.07 MiB\n898.83 kiB\n\n\nShape\n(16071, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nvg_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 13), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nGeostrophic velocities calculated from absolute dynamic topography\n\ndepth :\n\n15m\n\nlong_name :\n\nmeridional geostrophic surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148\n\nstandard_name :\n\ngeostrophic_northward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n141.07 MiB\n898.83 kiB\n\n\nShape\n(16071, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nwind_dir\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 13), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre wind direction\n\nunits :\n\ndegrees\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n141.07 MiB\n898.83 kiB\n\n\nShape\n(16071, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nwind_speed\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 13), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre absolute speed\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n141.07 MiB\n898.83 kiB\n\n\nShape\n(16071, 177, 13)\n(100, 177, 13)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nIndexes: (3)latPandasIndexPandasIndex(Index([  32.0,  31.75,   31.5,  31.25,   31.0,  30.75,   30.5,  30.25,   30.0,\n        29.75,\n       ...\n        -9.75,  -10.0, -10.25,  -10.5, -10.75,  -11.0, -11.25,  -11.5, -11.75,\n        -12.0],\n      dtype='float32', name='lat', length=177))lonPandasIndexPandasIndex(Index([ 42.0, 42.25,  42.5, 42.75,  43.0, 43.25,  43.5, 43.75,  44.0, 44.25,\n        44.5, 44.75,  45.0],\n      dtype='float32', name='lon'))timePandasIndexPandasIndex(DatetimeIndex(['1979-01-01', '1979-01-02', '1979-01-03', '1979-01-04',\n               '1979-01-05', '1979-01-06', '1979-01-07', '1979-01-08',\n               '1979-01-09', '1979-01-10',\n               ...\n               '2022-12-22', '2022-12-23', '2022-12-24', '2022-12-25',\n               '2022-12-26', '2022-12-27', '2022-12-28', '2022-12-29',\n               '2022-12-30', '2022-12-31'],\n              dtype='datetime64[ns]', name='time', length=16071, freq=None))Attributes: (17)creator_email :minhphan@uw.educreator_name :Minh Phancreator_type :persondate_created :2023-07-19geospatial_lat_max :32.0geospatial_lat_min :-12.0geospatial_lat_resolution :0.25geospatial_lat_units :degrees_northgeospatial_lon_max :102.0geospatial_lon_min :42.0geospatial_lon_resolution :0.25geospatial_lon_units :degrees_eastsource :Earth & Space Research (ESR), Copernicus Climate Change Service (C3S), Copernicus Marine Environment Monitoring Service (CMEMS), United States Geological Survey (USGS)summary :Daily mean of 0.25 x 0.25 degrees gridded data from multiple climate variables that may influence the patterns of coastal upwelling in the focused areatime_coverage_end :2022-12-31T23:59:59time_coverage_start :1979-01-01T00:00:00title :Climate Data for Coastal Upwelling Machine Learning Project in Indian Ocean\n\n\n\n# slice by time\nds.sel(time=slice('1998', '1999'))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:          (time: 730, lat: 177, lon: 241)\nCoordinates:\n  * lat              (lat) float32 32.0 31.75 31.5 31.25 ... -11.5 -11.75 -12.0\n  * lon              (lon) float32 42.0 42.25 42.5 42.75 ... 101.5 101.8 102.0\n  * time             (time) datetime64[ns] 1998-01-01 1998-01-02 ... 1999-12-31\nData variables: (12/19)\n    CHL              (time, lat, lon) float32 dask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\n    CHL_uncertainty  (time, lat, lon) float32 dask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\n    adt              (time, lat, lon) float32 dask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\n    air_temp         (time, lat, lon) float32 dask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\n    curr_dir         (time, lat, lon) float32 dask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\n    curr_speed       (time, lat, lon) float32 dask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\n    ...               ...\n    ug_curr          (time, lat, lon) float32 dask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\n    v_curr           (time, lat, lon) float32 dask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\n    v_wind           (time, lat, lon) float32 dask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\n    vg_curr          (time, lat, lon) float32 dask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\n    wind_dir         (time, lat, lon) float32 dask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\n    wind_speed       (time, lat, lon) float32 dask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\nAttributes: (12/17)\n    creator_email:              minhphan@uw.edu\n    creator_name:               Minh Phan\n    creator_type:               person\n    date_created:               2023-07-19\n    geospatial_lat_max:         32.0\n    geospatial_lat_min:         -12.0\n    ...                         ...\n    geospatial_lon_units:       degrees_east\n    source:                     Earth & Space Research (ESR), Copernicus Clim...\n    summary:                    Daily mean of 0.25 x 0.25 degrees gridded dat...\n    time_coverage_end:          2022-12-31T23:59:59\n    time_coverage_start:        1979-01-01T00:00:00\n    title:                      Climate Data for Coastal Upwelling Machine Le...xarray.DatasetDimensions:time: 730lat: 177lon: 241Coordinates: (3)lat(lat)float3232.0 31.75 31.5 ... -11.75 -12.0long_name :latitudestandard_name :latitudeunits :degrees_northarray([ 32.  ,  31.75,  31.5 ,  31.25,  31.  ,  30.75,  30.5 ,  30.25,  30.  ,\n        29.75,  29.5 ,  29.25,  29.  ,  28.75,  28.5 ,  28.25,  28.  ,  27.75,\n        27.5 ,  27.25,  27.  ,  26.75,  26.5 ,  26.25,  26.  ,  25.75,  25.5 ,\n        25.25,  25.  ,  24.75,  24.5 ,  24.25,  24.  ,  23.75,  23.5 ,  23.25,\n        23.  ,  22.75,  22.5 ,  22.25,  22.  ,  21.75,  21.5 ,  21.25,  21.  ,\n        20.75,  20.5 ,  20.25,  20.  ,  19.75,  19.5 ,  19.25,  19.  ,  18.75,\n        18.5 ,  18.25,  18.  ,  17.75,  17.5 ,  17.25,  17.  ,  16.75,  16.5 ,\n        16.25,  16.  ,  15.75,  15.5 ,  15.25,  15.  ,  14.75,  14.5 ,  14.25,\n        14.  ,  13.75,  13.5 ,  13.25,  13.  ,  12.75,  12.5 ,  12.25,  12.  ,\n        11.75,  11.5 ,  11.25,  11.  ,  10.75,  10.5 ,  10.25,  10.  ,   9.75,\n         9.5 ,   9.25,   9.  ,   8.75,   8.5 ,   8.25,   8.  ,   7.75,   7.5 ,\n         7.25,   7.  ,   6.75,   6.5 ,   6.25,   6.  ,   5.75,   5.5 ,   5.25,\n         5.  ,   4.75,   4.5 ,   4.25,   4.  ,   3.75,   3.5 ,   3.25,   3.  ,\n         2.75,   2.5 ,   2.25,   2.  ,   1.75,   1.5 ,   1.25,   1.  ,   0.75,\n         0.5 ,   0.25,   0.  ,  -0.25,  -0.5 ,  -0.75,  -1.  ,  -1.25,  -1.5 ,\n        -1.75,  -2.  ,  -2.25,  -2.5 ,  -2.75,  -3.  ,  -3.25,  -3.5 ,  -3.75,\n        -4.  ,  -4.25,  -4.5 ,  -4.75,  -5.  ,  -5.25,  -5.5 ,  -5.75,  -6.  ,\n        -6.25,  -6.5 ,  -6.75,  -7.  ,  -7.25,  -7.5 ,  -7.75,  -8.  ,  -8.25,\n        -8.5 ,  -8.75,  -9.  ,  -9.25,  -9.5 ,  -9.75, -10.  , -10.25, -10.5 ,\n       -10.75, -11.  , -11.25, -11.5 , -11.75, -12.  ], dtype=float32)lon(lon)float3242.0 42.25 42.5 ... 101.8 102.0long_name :longitudestandard_name :longitudeunits :degrees_eastarray([ 42.  ,  42.25,  42.5 , ..., 101.5 , 101.75, 102.  ], dtype=float32)time(time)datetime64[ns]1998-01-01 ... 1999-12-31axis :Tcomment :Data is averaged over the daylong_name :time centered on the daystandard_name :timetime_bounds :2000-01-01 00:00:00 to 2000-01-01 23:59:59array(['1998-01-01T00:00:00.000000000', '1998-01-02T00:00:00.000000000',\n       '1998-01-03T00:00:00.000000000', ..., '1999-12-29T00:00:00.000000000',\n       '1999-12-30T00:00:00.000000000', '1999-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (19)CHL(time, lat, lon)float32dask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;_ChunkSizes :[1, 256, 256]ancillary_variables :flags CHL_uncertaintycoverage_content_type :modelResultinput_files_reprocessings :Processors versions: MODIS R2022.0NRT/VIIRSN R2022.0NRT/OLCIA 07.02/VIIRSJ1 R2022.0NRT/OLCIB 07.02long_name :Chlorophyll-a concentration - Mean of the binned pixelsstandard_name :mass_concentration_of_chlorophyll_a_in_sea_watertype :surfaceunits :milligram m-3valid_max :1000.0valid_min :0.0\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n8 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nCHL_uncertainty\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\n\n\n\n\n_ChunkSizes :\n\n[1, 256, 256]\n\ncoverage_content_type :\n\nqualityInformation\n\nlong_name :\n\nChlorophyll-a concentration - Uncertainty estimation\n\nunits :\n\n%\n\nvalid_max :\n\n32767\n\nvalid_min :\n\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n8 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nadt\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nThe absolute dynamic topography is the sea surface height above geoid; the adt is obtained as follows: adt=sla+mdt where mdt is the mean dynamic topography; see the product user manual for details\n\ngrid_mapping :\n\ncrs\n\nlong_name :\n\nAbsolute dynamic topography\n\nstandard_name :\n\nsea_surface_height_above_geoid\n\nunits :\n\nm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n8 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nair_temp\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n2 metre temperature\n\nnameCDM :\n\n2_metre_temperature_surface\n\nnameECMWF :\n\n2 metre temperature\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n2t\n\nstandard_name :\n\nair_temperature\n\nunits :\n\nK\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n8 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\ncurr_dir\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\n\n\n\n\ncomments :\n\nComputed from total surface current velocity elements. Velocities are an average over the top 30m of the mixed layer\n\ndepth :\n\n15m\n\nlong_name :\n\naverage direction of total surface currents\n\nunits :\n\ndegrees\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n8 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\ncurr_speed\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\n\n\n\n\ncomments :\n\nVelocities are an average over the top 30m of the mixed layer\n\ndepth :\n\n15m\n\nlong_name :\n\naverage total surface current speed\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n8 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nmlotst\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\n\n\n\n\n_ChunkSizes :\n\n[1, 681, 1440]\n\ncell_methods :\n\narea: mean\n\nlong_name :\n\nDensity ocean mixed layer thickness\n\nstandard_name :\n\nocean_mixed_layer_thickness_defined_by_sigma_theta\n\nunit_long :\n\nMeters\n\nunits :\n\nm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n81.36 MiB\n\n\nShape\n(730, 177, 241)\n(500, 177, 241)\n\n\nDask graph\n3 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nsla\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nancillary_variables :\n\nerr_sla\n\ncomment :\n\nThe sea level anomaly is the sea surface height above mean sea surface; it is referenced to the [1993, 2012] period; see the product user manual for details\n\ngrid_mapping :\n\ncrs\n\nlong_name :\n\nSea level anomaly\n\nstandard_name :\n\nsea_surface_height_above_sea_level\n\nunits :\n\nm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n8 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nso\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\n\n\n\n\n_ChunkSizes :\n\n[1, 7, 341, 720]\n\ncell_methods :\n\narea: mean\n\nlong_name :\n\nmean sea water salinity at 0.49 metres below ocean surface\n\nstandard_name :\n\nsea_water_salinity\n\nunit_long :\n\nPractical Salinity Unit\n\nunits :\n\n1e-3\n\nvalid_max :\n\n28336\n\nvalid_min :\n\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n81.36 MiB\n\n\nShape\n(730, 177, 241)\n(500, 177, 241)\n\n\nDask graph\n3 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nsst\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nSea surface temperature\n\nnameCDM :\n\nSea_surface_temperature_surface\n\nnameECMWF :\n\nSea surface temperature\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\nsst\n\nstandard_name :\n\nsea_surface_temperature\n\nunits :\n\nK\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n8 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\ntopo\n\n\n(lat, lon)\n\n\nfloat64\n\n\ndask.array&lt;chunksize=(177, 241), meta=np.ndarray&gt;\n\n\n\n\ncolorBarMaximum :\n\n8000.0\n\ncolorBarMinimum :\n\n-8000.0\n\ncolorBarPalette :\n\nTopography\n\ngrid_mapping :\n\nGDAL_Geographics\n\nioos_category :\n\nLocation\n\nlong_name :\n\nTopography\n\nstandard_name :\n\naltitude\n\nunits :\n\nmeters\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n333.26 kiB\n333.26 kiB\n\n\nShape\n(177, 241)\n(177, 241)\n\n\nDask graph\n1 chunks in 2 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n\n\n\n\n\nu_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nVelocities are an average over the top 30m of the mixed layer\n\ncoverage_content_type :\n\nmodelResult\n\ndepth :\n\n15m\n\nlong_name :\n\nzonal total surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148 ; WIND source: ECMWF ERA5 10m wind DOI: 10.24381/cds.adbb2d47 ; SST source: CMC 0.2 deg SST V2.0 DOI: 10.5067/GHCMC-4FM02\n\nstandard_name :\n\neastward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n8 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nu_wind\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre U wind component\n\nnameCDM :\n\n10_metre_U_wind_component_surface\n\nnameECMWF :\n\n10 metre U wind component\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n10u\n\nstandard_name :\n\neastward_wind\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n8 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nug_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nGeostrophic velocities calculated from absolute dynamic topography\n\ndepth :\n\n15m\n\nlong_name :\n\nzonal geostrophic surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148\n\nstandard_name :\n\ngeostrophic_eastward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n8 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nv_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nVelocities are an average over the top 30m of the mixed layer\n\ncoverage_content_type :\n\nmodelResult\n\ndepth :\n\n15m\n\nlong_name :\n\nmeridional total surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148 ; WIND source: ECMWF ERA5 10m wind DOI: 10.24381/cds.adbb2d47 ; SST source: CMC 0.2 deg SST V2.0 DOI: 10.5067/GHCMC-4FM02\n\nstandard_name :\n\nnorthward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n8 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nv_wind\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre V wind component\n\nnameCDM :\n\n10_metre_V_wind_component_surface\n\nnameECMWF :\n\n10 metre V wind component\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n10v\n\nstandard_name :\n\nnorthward_wind\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n8 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nvg_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nGeostrophic velocities calculated from absolute dynamic topography\n\ndepth :\n\n15m\n\nlong_name :\n\nmeridional geostrophic surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148\n\nstandard_name :\n\ngeostrophic_northward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n8 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nwind_dir\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre wind direction\n\nunits :\n\ndegrees\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n8 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nwind_speed\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(60, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre absolute speed\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n118.79 MiB\n16.27 MiB\n\n\nShape\n(730, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n8 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nIndexes: (3)latPandasIndexPandasIndex(Index([  32.0,  31.75,   31.5,  31.25,   31.0,  30.75,   30.5,  30.25,   30.0,\n        29.75,\n       ...\n        -9.75,  -10.0, -10.25,  -10.5, -10.75,  -11.0, -11.25,  -11.5, -11.75,\n        -12.0],\n      dtype='float32', name='lat', length=177))lonPandasIndexPandasIndex(Index([  42.0,  42.25,   42.5,  42.75,   43.0,  43.25,   43.5,  43.75,   44.0,\n        44.25,\n       ...\n        99.75,  100.0, 100.25,  100.5, 100.75,  101.0, 101.25,  101.5, 101.75,\n        102.0],\n      dtype='float32', name='lon', length=241))timePandasIndexPandasIndex(DatetimeIndex(['1998-01-01', '1998-01-02', '1998-01-03', '1998-01-04',\n               '1998-01-05', '1998-01-06', '1998-01-07', '1998-01-08',\n               '1998-01-09', '1998-01-10',\n               ...\n               '1999-12-22', '1999-12-23', '1999-12-24', '1999-12-25',\n               '1999-12-26', '1999-12-27', '1999-12-28', '1999-12-29',\n               '1999-12-30', '1999-12-31'],\n              dtype='datetime64[ns]', name='time', length=730, freq=None))Attributes: (17)creator_email :minhphan@uw.educreator_name :Minh Phancreator_type :persondate_created :2023-07-19geospatial_lat_max :32.0geospatial_lat_min :-12.0geospatial_lat_resolution :0.25geospatial_lat_units :degrees_northgeospatial_lon_max :102.0geospatial_lon_min :42.0geospatial_lon_resolution :0.25geospatial_lon_units :degrees_eastsource :Earth & Space Research (ESR), Copernicus Climate Change Service (C3S), Copernicus Marine Environment Monitoring Service (CMEMS), United States Geological Survey (USGS)summary :Daily mean of 0.25 x 0.25 degrees gridded data from multiple climate variables that may influence the patterns of coastal upwelling in the focused areatime_coverage_end :2022-12-31T23:59:59time_coverage_start :1979-01-01T00:00:00title :Climate Data for Coastal Upwelling Machine Learning Project in Indian Ocean\n\n\n\n# slice by variable\nds[['u_curr', 'u_wind']]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (time: 16071, lat: 177, lon: 241)\nCoordinates:\n  * lat      (lat) float32 32.0 31.75 31.5 31.25 ... -11.25 -11.5 -11.75 -12.0\n  * lon      (lon) float32 42.0 42.25 42.5 42.75 ... 101.2 101.5 101.8 102.0\n  * time     (time) datetime64[ns] 1979-01-01 1979-01-02 ... 2022-12-31\nData variables:\n    u_curr   (time, lat, lon) float32 dask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\n    u_wind   (time, lat, lon) float32 dask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\nAttributes: (12/17)\n    creator_email:              minhphan@uw.edu\n    creator_name:               Minh Phan\n    creator_type:               person\n    date_created:               2023-07-19\n    geospatial_lat_max:         32.0\n    geospatial_lat_min:         -12.0\n    ...                         ...\n    geospatial_lon_units:       degrees_east\n    source:                     Earth & Space Research (ESR), Copernicus Clim...\n    summary:                    Daily mean of 0.25 x 0.25 degrees gridded dat...\n    time_coverage_end:          2022-12-31T23:59:59\n    time_coverage_start:        1979-01-01T00:00:00\n    title:                      Climate Data for Coastal Upwelling Machine Le...xarray.DatasetDimensions:time: 16071lat: 177lon: 241Coordinates: (3)lat(lat)float3232.0 31.75 31.5 ... -11.75 -12.0long_name :latitudestandard_name :latitudeunits :degrees_northarray([ 32.  ,  31.75,  31.5 ,  31.25,  31.  ,  30.75,  30.5 ,  30.25,  30.  ,\n        29.75,  29.5 ,  29.25,  29.  ,  28.75,  28.5 ,  28.25,  28.  ,  27.75,\n        27.5 ,  27.25,  27.  ,  26.75,  26.5 ,  26.25,  26.  ,  25.75,  25.5 ,\n        25.25,  25.  ,  24.75,  24.5 ,  24.25,  24.  ,  23.75,  23.5 ,  23.25,\n        23.  ,  22.75,  22.5 ,  22.25,  22.  ,  21.75,  21.5 ,  21.25,  21.  ,\n        20.75,  20.5 ,  20.25,  20.  ,  19.75,  19.5 ,  19.25,  19.  ,  18.75,\n        18.5 ,  18.25,  18.  ,  17.75,  17.5 ,  17.25,  17.  ,  16.75,  16.5 ,\n        16.25,  16.  ,  15.75,  15.5 ,  15.25,  15.  ,  14.75,  14.5 ,  14.25,\n        14.  ,  13.75,  13.5 ,  13.25,  13.  ,  12.75,  12.5 ,  12.25,  12.  ,\n        11.75,  11.5 ,  11.25,  11.  ,  10.75,  10.5 ,  10.25,  10.  ,   9.75,\n         9.5 ,   9.25,   9.  ,   8.75,   8.5 ,   8.25,   8.  ,   7.75,   7.5 ,\n         7.25,   7.  ,   6.75,   6.5 ,   6.25,   6.  ,   5.75,   5.5 ,   5.25,\n         5.  ,   4.75,   4.5 ,   4.25,   4.  ,   3.75,   3.5 ,   3.25,   3.  ,\n         2.75,   2.5 ,   2.25,   2.  ,   1.75,   1.5 ,   1.25,   1.  ,   0.75,\n         0.5 ,   0.25,   0.  ,  -0.25,  -0.5 ,  -0.75,  -1.  ,  -1.25,  -1.5 ,\n        -1.75,  -2.  ,  -2.25,  -2.5 ,  -2.75,  -3.  ,  -3.25,  -3.5 ,  -3.75,\n        -4.  ,  -4.25,  -4.5 ,  -4.75,  -5.  ,  -5.25,  -5.5 ,  -5.75,  -6.  ,\n        -6.25,  -6.5 ,  -6.75,  -7.  ,  -7.25,  -7.5 ,  -7.75,  -8.  ,  -8.25,\n        -8.5 ,  -8.75,  -9.  ,  -9.25,  -9.5 ,  -9.75, -10.  , -10.25, -10.5 ,\n       -10.75, -11.  , -11.25, -11.5 , -11.75, -12.  ], dtype=float32)lon(lon)float3242.0 42.25 42.5 ... 101.8 102.0long_name :longitudestandard_name :longitudeunits :degrees_eastarray([ 42.  ,  42.25,  42.5 , ..., 101.5 , 101.75, 102.  ], dtype=float32)time(time)datetime64[ns]1979-01-01 ... 2022-12-31axis :Tcomment :Data is averaged over the daylong_name :time centered on the daystandard_name :timetime_bounds :2000-01-01 00:00:00 to 2000-01-01 23:59:59array(['1979-01-01T00:00:00.000000000', '1979-01-02T00:00:00.000000000',\n       '1979-01-03T00:00:00.000000000', ..., '2022-12-29T00:00:00.000000000',\n       '2022-12-30T00:00:00.000000000', '2022-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (2)u_curr(time, lat, lon)float32dask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;comment :Velocities are an average over the top 30m of the mixed layercoverage_content_type :modelResultdepth :15mlong_name :zonal total surface currentsource :SSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148 ; WIND source: ECMWF ERA5 10m wind DOI: 10.24381/cds.adbb2d47 ; SST source: CMC 0.2 deg SST V2.0 DOI: 10.5067/GHCMC-4FM02standard_name :eastward_sea_water_velocityunits :m s-1valid_max :3.0valid_min :-3.0\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.55 GiB\n16.27 MiB\n\n\nShape\n(16071, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n161 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nu_wind\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 177, 241), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre U wind component\n\nnameCDM :\n\n10_metre_U_wind_component_surface\n\nnameECMWF :\n\n10 metre U wind component\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n10u\n\nstandard_name :\n\neastward_wind\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.55 GiB\n16.27 MiB\n\n\nShape\n(16071, 177, 241)\n(100, 177, 241)\n\n\nDask graph\n161 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nIndexes: (3)latPandasIndexPandasIndex(Index([  32.0,  31.75,   31.5,  31.25,   31.0,  30.75,   30.5,  30.25,   30.0,\n        29.75,\n       ...\n        -9.75,  -10.0, -10.25,  -10.5, -10.75,  -11.0, -11.25,  -11.5, -11.75,\n        -12.0],\n      dtype='float32', name='lat', length=177))lonPandasIndexPandasIndex(Index([  42.0,  42.25,   42.5,  42.75,   43.0,  43.25,   43.5,  43.75,   44.0,\n        44.25,\n       ...\n        99.75,  100.0, 100.25,  100.5, 100.75,  101.0, 101.25,  101.5, 101.75,\n        102.0],\n      dtype='float32', name='lon', length=241))timePandasIndexPandasIndex(DatetimeIndex(['1979-01-01', '1979-01-02', '1979-01-03', '1979-01-04',\n               '1979-01-05', '1979-01-06', '1979-01-07', '1979-01-08',\n               '1979-01-09', '1979-01-10',\n               ...\n               '2022-12-22', '2022-12-23', '2022-12-24', '2022-12-25',\n               '2022-12-26', '2022-12-27', '2022-12-28', '2022-12-29',\n               '2022-12-30', '2022-12-31'],\n              dtype='datetime64[ns]', name='time', length=16071, freq=None))Attributes: (17)creator_email :minhphan@uw.educreator_name :Minh Phancreator_type :persondate_created :2023-07-19geospatial_lat_max :32.0geospatial_lat_min :-12.0geospatial_lat_resolution :0.25geospatial_lat_units :degrees_northgeospatial_lon_max :102.0geospatial_lon_min :42.0geospatial_lon_resolution :0.25geospatial_lon_units :degrees_eastsource :Earth & Space Research (ESR), Copernicus Climate Change Service (C3S), Copernicus Marine Environment Monitoring Service (CMEMS), United States Geological Survey (USGS)summary :Daily mean of 0.25 x 0.25 degrees gridded data from multiple climate variables that may influence the patterns of coastal upwelling in the focused areatime_coverage_end :2022-12-31T23:59:59time_coverage_start :1979-01-01T00:00:00title :Climate Data for Coastal Upwelling Machine Learning Project in Indian Ocean\n\n\n\n# combine multiple slicing options all at once\nds[['u_curr', 'u_wind']].sel(time=slice('1998', '1999'), \n                             lat=slice(0, -12), \n                             lon=slice(42, 45))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (time: 730, lat: 49, lon: 13)\nCoordinates:\n  * lat      (lat) float32 0.0 -0.25 -0.5 -0.75 ... -11.25 -11.5 -11.75 -12.0\n  * lon      (lon) float32 42.0 42.25 42.5 42.75 43.0 ... 44.25 44.5 44.75 45.0\n  * time     (time) datetime64[ns] 1998-01-01 1998-01-02 ... 1999-12-31\nData variables:\n    u_curr   (time, lat, lon) float32 dask.array&lt;chunksize=(60, 49, 13), meta=np.ndarray&gt;\n    u_wind   (time, lat, lon) float32 dask.array&lt;chunksize=(60, 49, 13), meta=np.ndarray&gt;\nAttributes: (12/17)\n    creator_email:              minhphan@uw.edu\n    creator_name:               Minh Phan\n    creator_type:               person\n    date_created:               2023-07-19\n    geospatial_lat_max:         32.0\n    geospatial_lat_min:         -12.0\n    ...                         ...\n    geospatial_lon_units:       degrees_east\n    source:                     Earth & Space Research (ESR), Copernicus Clim...\n    summary:                    Daily mean of 0.25 x 0.25 degrees gridded dat...\n    time_coverage_end:          2022-12-31T23:59:59\n    time_coverage_start:        1979-01-01T00:00:00\n    title:                      Climate Data for Coastal Upwelling Machine Le...xarray.DatasetDimensions:time: 730lat: 49lon: 13Coordinates: (3)lat(lat)float320.0 -0.25 -0.5 ... -11.75 -12.0long_name :latitudestandard_name :latitudeunits :degrees_northarray([  0.  ,  -0.25,  -0.5 ,  -0.75,  -1.  ,  -1.25,  -1.5 ,  -1.75,  -2.  ,\n        -2.25,  -2.5 ,  -2.75,  -3.  ,  -3.25,  -3.5 ,  -3.75,  -4.  ,  -4.25,\n        -4.5 ,  -4.75,  -5.  ,  -5.25,  -5.5 ,  -5.75,  -6.  ,  -6.25,  -6.5 ,\n        -6.75,  -7.  ,  -7.25,  -7.5 ,  -7.75,  -8.  ,  -8.25,  -8.5 ,  -8.75,\n        -9.  ,  -9.25,  -9.5 ,  -9.75, -10.  , -10.25, -10.5 , -10.75, -11.  ,\n       -11.25, -11.5 , -11.75, -12.  ], dtype=float32)lon(lon)float3242.0 42.25 42.5 ... 44.5 44.75 45.0long_name :longitudestandard_name :longitudeunits :degrees_eastarray([42.  , 42.25, 42.5 , 42.75, 43.  , 43.25, 43.5 , 43.75, 44.  , 44.25,\n       44.5 , 44.75, 45.  ], dtype=float32)time(time)datetime64[ns]1998-01-01 ... 1999-12-31axis :Tcomment :Data is averaged over the daylong_name :time centered on the daystandard_name :timetime_bounds :2000-01-01 00:00:00 to 2000-01-01 23:59:59array(['1998-01-01T00:00:00.000000000', '1998-01-02T00:00:00.000000000',\n       '1998-01-03T00:00:00.000000000', ..., '1999-12-29T00:00:00.000000000',\n       '1999-12-30T00:00:00.000000000', '1999-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (2)u_curr(time, lat, lon)float32dask.array&lt;chunksize=(60, 49, 13), meta=np.ndarray&gt;comment :Velocities are an average over the top 30m of the mixed layercoverage_content_type :modelResultdepth :15mlong_name :zonal total surface currentsource :SSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148 ; WIND source: ECMWF ERA5 10m wind DOI: 10.24381/cds.adbb2d47 ; SST source: CMC 0.2 deg SST V2.0 DOI: 10.5067/GHCMC-4FM02standard_name :eastward_sea_water_velocityunits :m s-1valid_max :3.0valid_min :-3.0\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.77 MiB\n248.83 kiB\n\n\nShape\n(730, 49, 13)\n(100, 49, 13)\n\n\nDask graph\n8 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nu_wind\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(60, 49, 13), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre U wind component\n\nnameCDM :\n\n10_metre_U_wind_component_surface\n\nnameECMWF :\n\n10 metre U wind component\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n10u\n\nstandard_name :\n\neastward_wind\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.77 MiB\n248.83 kiB\n\n\nShape\n(730, 49, 13)\n(100, 49, 13)\n\n\nDask graph\n8 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nIndexes: (3)latPandasIndexPandasIndex(Index([   0.0,  -0.25,   -0.5,  -0.75,   -1.0,  -1.25,   -1.5,  -1.75,   -2.0,\n        -2.25,   -2.5,  -2.75,   -3.0,  -3.25,   -3.5,  -3.75,   -4.0,  -4.25,\n         -4.5,  -4.75,   -5.0,  -5.25,   -5.5,  -5.75,   -6.0,  -6.25,   -6.5,\n        -6.75,   -7.0,  -7.25,   -7.5,  -7.75,   -8.0,  -8.25,   -8.5,  -8.75,\n         -9.0,  -9.25,   -9.5,  -9.75,  -10.0, -10.25,  -10.5, -10.75,  -11.0,\n       -11.25,  -11.5, -11.75,  -12.0],\n      dtype='float32', name='lat'))lonPandasIndexPandasIndex(Index([ 42.0, 42.25,  42.5, 42.75,  43.0, 43.25,  43.5, 43.75,  44.0, 44.25,\n        44.5, 44.75,  45.0],\n      dtype='float32', name='lon'))timePandasIndexPandasIndex(DatetimeIndex(['1998-01-01', '1998-01-02', '1998-01-03', '1998-01-04',\n               '1998-01-05', '1998-01-06', '1998-01-07', '1998-01-08',\n               '1998-01-09', '1998-01-10',\n               ...\n               '1999-12-22', '1999-12-23', '1999-12-24', '1999-12-25',\n               '1999-12-26', '1999-12-27', '1999-12-28', '1999-12-29',\n               '1999-12-30', '1999-12-31'],\n              dtype='datetime64[ns]', name='time', length=730, freq=None))Attributes: (17)creator_email :minhphan@uw.educreator_name :Minh Phancreator_type :persondate_created :2023-07-19geospatial_lat_max :32.0geospatial_lat_min :-12.0geospatial_lat_resolution :0.25geospatial_lat_units :degrees_northgeospatial_lon_max :102.0geospatial_lon_min :42.0geospatial_lon_resolution :0.25geospatial_lon_units :degrees_eastsource :Earth & Space Research (ESR), Copernicus Climate Change Service (C3S), Copernicus Marine Environment Monitoring Service (CMEMS), United States Geological Survey (USGS)summary :Daily mean of 0.25 x 0.25 degrees gridded data from multiple climate variables that may influence the patterns of coastal upwelling in the focused areatime_coverage_end :2022-12-31T23:59:59time_coverage_start :1979-01-01T00:00:00title :Climate Data for Coastal Upwelling Machine Learning Project in Indian Ocean\n\n\nWe can also graph the data right from slicing, especially heatmaps from 2D arrays, or line charts. This is especially useful when we want to inspect elements on the go.\n\n# make sure that the array you slice for a heatmap visualization is a 2D array\nheatmap_arr = ds['wind_speed'].sel(time='2000-01-02')\nheatmap_arr\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'wind_speed' (lat: 177, lon: 241)&gt;\ndask.array&lt;getitem, shape=(177, 241), dtype=float32, chunksize=(177, 241), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * lat      (lat) float32 32.0 31.75 31.5 31.25 ... -11.25 -11.5 -11.75 -12.0\n  * lon      (lon) float32 42.0 42.25 42.5 42.75 ... 101.2 101.5 101.8 102.0\n    time     datetime64[ns] 2000-01-02\nAttributes:\n    long_name:  10 metre absolute speed\n    units:      m s**-1xarray.DataArray'wind_speed'lat: 177lon: 241dask.array&lt;chunksize=(177, 241), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n166.63 kiB\n166.63 kiB\n\n\nShape\n(177, 241)\n(177, 241)\n\n\nDask graph\n1 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\nCoordinates: (3)lat(lat)float3232.0 31.75 31.5 ... -11.75 -12.0long_name :latitudestandard_name :latitudeunits :degrees_northarray([ 32.  ,  31.75,  31.5 ,  31.25,  31.  ,  30.75,  30.5 ,  30.25,  30.  ,\n        29.75,  29.5 ,  29.25,  29.  ,  28.75,  28.5 ,  28.25,  28.  ,  27.75,\n        27.5 ,  27.25,  27.  ,  26.75,  26.5 ,  26.25,  26.  ,  25.75,  25.5 ,\n        25.25,  25.  ,  24.75,  24.5 ,  24.25,  24.  ,  23.75,  23.5 ,  23.25,\n        23.  ,  22.75,  22.5 ,  22.25,  22.  ,  21.75,  21.5 ,  21.25,  21.  ,\n        20.75,  20.5 ,  20.25,  20.  ,  19.75,  19.5 ,  19.25,  19.  ,  18.75,\n        18.5 ,  18.25,  18.  ,  17.75,  17.5 ,  17.25,  17.  ,  16.75,  16.5 ,\n        16.25,  16.  ,  15.75,  15.5 ,  15.25,  15.  ,  14.75,  14.5 ,  14.25,\n        14.  ,  13.75,  13.5 ,  13.25,  13.  ,  12.75,  12.5 ,  12.25,  12.  ,\n        11.75,  11.5 ,  11.25,  11.  ,  10.75,  10.5 ,  10.25,  10.  ,   9.75,\n         9.5 ,   9.25,   9.  ,   8.75,   8.5 ,   8.25,   8.  ,   7.75,   7.5 ,\n         7.25,   7.  ,   6.75,   6.5 ,   6.25,   6.  ,   5.75,   5.5 ,   5.25,\n         5.  ,   4.75,   4.5 ,   4.25,   4.  ,   3.75,   3.5 ,   3.25,   3.  ,\n         2.75,   2.5 ,   2.25,   2.  ,   1.75,   1.5 ,   1.25,   1.  ,   0.75,\n         0.5 ,   0.25,   0.  ,  -0.25,  -0.5 ,  -0.75,  -1.  ,  -1.25,  -1.5 ,\n        -1.75,  -2.  ,  -2.25,  -2.5 ,  -2.75,  -3.  ,  -3.25,  -3.5 ,  -3.75,\n        -4.  ,  -4.25,  -4.5 ,  -4.75,  -5.  ,  -5.25,  -5.5 ,  -5.75,  -6.  ,\n        -6.25,  -6.5 ,  -6.75,  -7.  ,  -7.25,  -7.5 ,  -7.75,  -8.  ,  -8.25,\n        -8.5 ,  -8.75,  -9.  ,  -9.25,  -9.5 ,  -9.75, -10.  , -10.25, -10.5 ,\n       -10.75, -11.  , -11.25, -11.5 , -11.75, -12.  ], dtype=float32)lon(lon)float3242.0 42.25 42.5 ... 101.8 102.0long_name :longitudestandard_name :longitudeunits :degrees_eastarray([ 42.  ,  42.25,  42.5 , ..., 101.5 , 101.75, 102.  ], dtype=float32)time()datetime64[ns]2000-01-02axis :Tcomment :Data is averaged over the daylong_name :time centered on the daystandard_name :timetime_bounds :2000-01-01 00:00:00 to 2000-01-01 23:59:59array('2000-01-02T00:00:00.000000000', dtype='datetime64[ns]')Indexes: (2)latPandasIndexPandasIndex(Index([  32.0,  31.75,   31.5,  31.25,   31.0,  30.75,   30.5,  30.25,   30.0,\n        29.75,\n       ...\n        -9.75,  -10.0, -10.25,  -10.5, -10.75,  -11.0, -11.25,  -11.5, -11.75,\n        -12.0],\n      dtype='float32', name='lat', length=177))lonPandasIndexPandasIndex(Index([  42.0,  42.25,   42.5,  42.75,   43.0,  43.25,   43.5,  43.75,   44.0,\n        44.25,\n       ...\n        99.75,  100.0, 100.25,  100.5, 100.75,  101.0, 101.25,  101.5, 101.75,\n        102.0],\n      dtype='float32', name='lon', length=241))Attributes: (2)long_name :10 metre absolute speedunits :m s**-1\n\n\n\nheatmap_arr.plot.imshow()\n\n&lt;matplotlib.image.AxesImage at 0x7f0104555e80&gt;\n\n\n\n\n\n\n# contour map with no filling\nheatmap_arr.plot.contour()\n\n&lt;matplotlib.contour.QuadContourSet at 0x7f01043b1af0&gt;\n\n\n\n\n\n\n# contour map with color filling\nheatmap_arr.plot.contourf()\n\n&lt;matplotlib.contour.QuadContourSet at 0x7f010428a6d0&gt;\n\n\n\n\n\n\nheatmap_arr.plot.surface()\n\n&lt;mpl_toolkits.mplot3d.art3d.Poly3DCollection at 0x7f01041560d0&gt;\n\n\n\n\n\n\nLine plots\n\nds['wind_speed'].mean(dim=['lat', 'lon']).plot(figsize=(10, 5))\n\n\n\n\nWe can add in parameters to customize our graphs, as additional arguments are passed to the underlying matplotlib plot() function.\n\nds['air_temp'].mean(dim=['lat', 'lon']).sel(time=slice('2007', '2009')).plot.line('r-o', figsize=(10,5), markersize=1)\n\n\n\n\n\n\nHistogram\n\n# creating a new Axe object if there is no currently\n# available one\nax = plt.gca() \nds['wind_dir'].plot.hist(ax = ax)\nax.set_xlabel('10 metre wind direction (degrees east)')\nax.set_ylabel('frequency')\nax.set_title('Daily average wind direction distribution over covered area (1979-2022)')\n\nText(0.5, 1.0, 'Daily average wind direction distribution over covered area (1979-2022)')\n\n\n\n\n\n\n# scroll along the axes to stretch the graph\nheatmap_arr.hvplot().options(cmap='bgy', width=600, height=500)"
  },
  {
    "objectID": "python-notebooks/02_Using_Data.html#resampling",
    "href": "python-notebooks/02_Using_Data.html#resampling",
    "title": "IO Zarr file",
    "section": "Resampling",
    "text": "Resampling\n\nWith xarray\nIf your xarray version works just fine with resample(), here‚Äôs the most straightforward way to resample your data temporally. It may take a long while for the data to finish resampling, especially if your dataset is big and your resampling frequency is small.\n\nds_resampled = ds['CHL'].resample(time='1M').mean()\n\n\n\nWithout xarray\nIf not, which was the case we encountered for a bit during the documentation, we developed a roundabout using Pandas‚Äô equivalent function with the same name, which also happened to also be the underlying function that the library itself depends on (of course, more optimized), to help us perform resampling on our dataset.\n\n# resampling is having issues, so this is a temporary solution using Pandas \n# to perform resampling by month on one range...\n\n# choose a small range just in case overloadding happens...\n\nds_to_resample = ds['CHL'].sel(lat=slice(10, 5), lon=slice(75, 80)).to_dataframe()\n\n\ndf_resampled = ds_to_resample.groupby([pd.Grouper(freq='1M', level='time'),\n                        pd.Grouper(level='lon'),\n                        pd.Grouper(level='lat')]).mean()\n\n\nds_resampled = xr.Dataset.from_dataframe(df_resampled)\n\nWe can see that after resampling, our time dimension size is reduced from 8523 (days) to 280 (months). Resampling is successful!\n\n# original ds_to_resample object before converting to dataframe\nds['CHL'].sel(lat=slice(10, 5), lon=slice(75, 80))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'CHL' (time: 16071, lat: 21, lon: 21)&gt;\ndask.array&lt;getitem, shape=(16071, 21, 21), dtype=float32, chunksize=(100, 21, 21), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * lat      (lat) float32 10.0 9.75 9.5 9.25 9.0 8.75 ... 6.0 5.75 5.5 5.25 5.0\n  * lon      (lon) float32 75.0 75.25 75.5 75.75 76.0 ... 79.25 79.5 79.75 80.0\n  * time     (time) datetime64[ns] 1979-01-01 1979-01-02 ... 2022-12-31\nAttributes:\n    _ChunkSizes:                [1, 256, 256]\n    ancillary_variables:        flags CHL_uncertainty\n    coverage_content_type:      modelResult\n    input_files_reprocessings:  Processors versions: MODIS R2022.0NRT/VIIRSN ...\n    long_name:                  Chlorophyll-a concentration - Mean of the bin...\n    standard_name:              mass_concentration_of_chlorophyll_a_in_sea_water\n    type:                       surface\n    units:                      milligram m-3\n    valid_max:                  1000.0\n    valid_min:                  0.0xarray.DataArray'CHL'time: 16071lat: 21lon: 21dask.array&lt;chunksize=(100, 21, 21), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n27.04 MiB\n172.27 kiB\n\n\nShape\n(16071, 21, 21)\n(100, 21, 21)\n\n\nDask graph\n161 chunks in 3 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\nCoordinates: (3)lat(lat)float3210.0 9.75 9.5 9.25 ... 5.5 5.25 5.0long_name :latitudestandard_name :latitudeunits :degrees_northarray([10.  ,  9.75,  9.5 ,  9.25,  9.  ,  8.75,  8.5 ,  8.25,  8.  ,  7.75,\n        7.5 ,  7.25,  7.  ,  6.75,  6.5 ,  6.25,  6.  ,  5.75,  5.5 ,  5.25,\n        5.  ], dtype=float32)lon(lon)float3275.0 75.25 75.5 ... 79.5 79.75 80.0long_name :longitudestandard_name :longitudeunits :degrees_eastarray([75.  , 75.25, 75.5 , 75.75, 76.  , 76.25, 76.5 , 76.75, 77.  , 77.25,\n       77.5 , 77.75, 78.  , 78.25, 78.5 , 78.75, 79.  , 79.25, 79.5 , 79.75,\n       80.  ], dtype=float32)time(time)datetime64[ns]1979-01-01 ... 2022-12-31axis :Tcomment :Data is averaged over the daylong_name :time centered on the daystandard_name :timetime_bounds :2000-01-01 00:00:00 to 2000-01-01 23:59:59array(['1979-01-01T00:00:00.000000000', '1979-01-02T00:00:00.000000000',\n       '1979-01-03T00:00:00.000000000', ..., '2022-12-29T00:00:00.000000000',\n       '2022-12-30T00:00:00.000000000', '2022-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')Indexes: (3)latPandasIndexPandasIndex(Index([10.0, 9.75,  9.5, 9.25,  9.0, 8.75,  8.5, 8.25,  8.0, 7.75,  7.5, 7.25,\n        7.0, 6.75,  6.5, 6.25,  6.0, 5.75,  5.5, 5.25,  5.0],\n      dtype='float32', name='lat'))lonPandasIndexPandasIndex(Index([ 75.0, 75.25,  75.5, 75.75,  76.0, 76.25,  76.5, 76.75,  77.0, 77.25,\n        77.5, 77.75,  78.0, 78.25,  78.5, 78.75,  79.0, 79.25,  79.5, 79.75,\n        80.0],\n      dtype='float32', name='lon'))timePandasIndexPandasIndex(DatetimeIndex(['1979-01-01', '1979-01-02', '1979-01-03', '1979-01-04',\n               '1979-01-05', '1979-01-06', '1979-01-07', '1979-01-08',\n               '1979-01-09', '1979-01-10',\n               ...\n               '2022-12-22', '2022-12-23', '2022-12-24', '2022-12-25',\n               '2022-12-26', '2022-12-27', '2022-12-28', '2022-12-29',\n               '2022-12-30', '2022-12-31'],\n              dtype='datetime64[ns]', name='time', length=16071, freq=None))Attributes: (10)_ChunkSizes :[1, 256, 256]ancillary_variables :flags CHL_uncertaintycoverage_content_type :modelResultinput_files_reprocessings :Processors versions: MODIS R2022.0NRT/VIIRSN R2022.0NRT/OLCIA 07.02/VIIRSJ1 R2022.0NRT/OLCIB 07.02long_name :Chlorophyll-a concentration - Mean of the binned pixelsstandard_name :mass_concentration_of_chlorophyll_a_in_sea_watertype :surfaceunits :milligram m-3valid_max :1000.0valid_min :0.0\n\n\n\nds_resampled\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (time: 528, lon: 21, lat: 21)\nCoordinates:\n  * time     (time) datetime64[ns] 1979-01-31 1979-02-28 ... 2022-12-31\n  * lon      (lon) float32 75.0 75.25 75.5 75.75 76.0 ... 79.25 79.5 79.75 80.0\n  * lat      (lat) float32 5.0 5.25 5.5 5.75 6.0 6.25 ... 9.0 9.25 9.5 9.75 10.0\nData variables:\n    CHL      (time, lon, lat) float32 nan nan nan nan ... 3.221 2.768 nan 1.353xarray.DatasetDimensions:time: 528lon: 21lat: 21Coordinates: (3)time(time)datetime64[ns]1979-01-31 ... 2022-12-31array(['1979-01-31T00:00:00.000000000', '1979-02-28T00:00:00.000000000',\n       '1979-03-31T00:00:00.000000000', ..., '2022-10-31T00:00:00.000000000',\n       '2022-11-30T00:00:00.000000000', '2022-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')lon(lon)float3275.0 75.25 75.5 ... 79.5 79.75 80.0array([75.  , 75.25, 75.5 , 75.75, 76.  , 76.25, 76.5 , 76.75, 77.  , 77.25,\n       77.5 , 77.75, 78.  , 78.25, 78.5 , 78.75, 79.  , 79.25, 79.5 , 79.75,\n       80.  ], dtype=float32)lat(lat)float325.0 5.25 5.5 5.75 ... 9.5 9.75 10.0array([ 5.  ,  5.25,  5.5 ,  5.75,  6.  ,  6.25,  6.5 ,  6.75,  7.  ,  7.25,\n        7.5 ,  7.75,  8.  ,  8.25,  8.5 ,  8.75,  9.  ,  9.25,  9.5 ,  9.75,\n       10.  ], dtype=float32)Data variables: (1)CHL(time, lon, lat)float32nan nan nan nan ... 2.768 nan 1.353array([[[       nan,        nan,        nan, ...,        nan,\n                nan,        nan],\n        [       nan,        nan,        nan, ...,        nan,\n                nan,        nan],\n        [       nan,        nan,        nan, ...,        nan,\n                nan,        nan],\n        ...,\n        [       nan,        nan,        nan, ...,        nan,\n                nan,        nan],\n        [       nan,        nan,        nan, ...,        nan,\n                nan,        nan],\n        [       nan,        nan,        nan, ...,        nan,\n                nan,        nan]],\n\n       [[       nan,        nan,        nan, ...,        nan,\n                nan,        nan],\n        [       nan,        nan,        nan, ...,        nan,\n                nan,        nan],\n        [       nan,        nan,        nan, ...,        nan,\n                nan,        nan],\n...\n        [0.2705628 , 0.25798783, 0.25561604, ..., 1.4041072 ,\n         1.4939387 , 1.8776367 ],\n        [0.32376617, 0.2919578 , 0.28656372, ..., 1.7322022 ,\n         1.8477138 , 1.8143107 ],\n        [0.30853522, 0.3436496 , 0.3281598 , ..., 2.5401766 ,\n                nan, 1.3396835 ]],\n\n       [[0.23880072, 0.24270938, 0.24283718, ..., 0.20560512,\n         0.22738725, 0.3117161 ],\n        [0.2311706 , 0.2247422 , 0.23030376, ..., 0.21133746,\n         0.24978474, 0.3144625 ],\n        [0.22994642, 0.2216043 , 0.21443008, ..., 0.23380966,\n         0.27335462, 0.33697778],\n        ...,\n        [0.2801102 , 0.27076766, 0.28496662, ..., 1.4887002 ,\n         1.4666241 , 1.7220622 ],\n        [0.3205243 , 0.2954306 , 0.31341806, ..., 1.9154898 ,\n         1.42821   , 1.6312736 ],\n        [0.45734957, 0.3265505 , 0.33142075, ..., 2.7675717 ,\n                nan, 1.3527689 ]]], dtype=float32)Indexes: (3)timePandasIndexPandasIndex(DatetimeIndex(['1979-01-31', '1979-02-28', '1979-03-31', '1979-04-30',\n               '1979-05-31', '1979-06-30', '1979-07-31', '1979-08-31',\n               '1979-09-30', '1979-10-31',\n               ...\n               '2022-03-31', '2022-04-30', '2022-05-31', '2022-06-30',\n               '2022-07-31', '2022-08-31', '2022-09-30', '2022-10-31',\n               '2022-11-30', '2022-12-31'],\n              dtype='datetime64[ns]', name='time', length=528, freq='M'))lonPandasIndexPandasIndex(Index([ 75.0, 75.25,  75.5, 75.75,  76.0, 76.25,  76.5, 76.75,  77.0, 77.25,\n        77.5, 77.75,  78.0, 78.25,  78.5, 78.75,  79.0, 79.25,  79.5, 79.75,\n        80.0],\n      dtype='float32', name='lon'))latPandasIndexPandasIndex(Index([ 5.0, 5.25,  5.5, 5.75,  6.0, 6.25,  6.5, 6.75,  7.0, 7.25,  7.5, 7.75,\n        8.0, 8.25,  8.5, 8.75,  9.0, 9.25,  9.5, 9.75, 10.0],\n      dtype='float32', name='lat'))Attributes: (0)\n\n\n‚Ä¶ and as a matter of fact, we can graph this data, too!\n\nCHL_month = ds_resampled.mean(dim=['lat', 'lon']).hvplot(label='monthly resampling').options(color='red', )\n\n\nCHL_day = ds['CHL'].sel(lat=slice(10, 5), lon=slice(75, 80)).mean(dim=['lat', 'lon']).hvplot(label='daily resampling').options(color='blue')\n\n\n(CHL_day*CHL_month).options(title='Monthly vs Daily resampling of chlorophyll-a levels', xlabel='year')"
  },
  {
    "objectID": "python-notebooks/02_Xarray.html#why-do-we-need-xarray",
    "href": "python-notebooks/02_Xarray.html#why-do-we-need-xarray",
    "title": "03. Introduction to xarray",
    "section": "Why do we need xarray?",
    "text": "Why do we need xarray?\nAs Geoscientists, we often work with time series of data with two or more dimensions: a time series of calibrated, orthorectified satellite images; two-dimensional grids of surface air temperature from an atmospheric reanalysis; or three-dimensional (level, x, y) cubes of ocean salinity from an ocean model. These data are often provided in GeoTIFF, NetCDF or HDF format with rich and useful metadata that we want to retain, or even use in our analysis. Common analyses include calculating means, standard deviations and anomalies over time or one or more spatial dimensions (e.g.¬†zonal means). Model output often includes multiple variables that you want to apply similar analyses to.\n\n\n\nA schematic of multi-dimensional data\n\n\nThe schematic above shows a typical data structure for multi-dimensional data. There are two data cubes, one for temperature and one for precipitation. Common coordinate variables, in this case latitude, longitude and time are associated with each variable. Each variable, including coordinate variables, will have a set of attributes: name, units, missing value, etc. The file containing the data may also have attributes: source of the data, model name coordinate reference system if the data are projected. Writing code using low-level packages such as netcdf4 and numpy to read the data, then perform analysis, and write the results to file is time consuming and prone to errors."
  },
  {
    "objectID": "python-notebooks/02_Xarray.html#what-is-xarray",
    "href": "python-notebooks/02_Xarray.html#what-is-xarray",
    "title": "03. Introduction to xarray",
    "section": "What is xarray",
    "text": "What is xarray\nxarray is an open-source project and python package to work with labelled multi-dimensional arrays. It is leverages numpy, pandas, matplotlib and dask to build Dataset and DataArray objects with built-in methods to subset, analyze, interpolate, and plot multi-dimensional data. It makes working with multi-dimensional data cubes efficient and fun. It will change your life for the better. You‚Äôll be more attractive, more interesting, and better equiped to take on lifes challenges."
  },
  {
    "objectID": "python-notebooks/02_Xarray.html#what-you-will-learn-from-this-tutorial",
    "href": "python-notebooks/02_Xarray.html#what-you-will-learn-from-this-tutorial",
    "title": "03. Introduction to xarray",
    "section": "What you will learn from this tutorial",
    "text": "What you will learn from this tutorial\nIn this tutorial you will learn how to:\n\nload a netcdf file into xarray\ninterrogate the Dataset and understand the difference between DataArray and Dataset\nsubset a Dataset\ncalculate annual and monthly mean fields\ncalculate a time series of zonal means\nplot these results\n\nAs always, we‚Äôll start by importing xarray. We‚Äôll follow convention by giving the module the shortname xr\n\nimport xarray as xr\nxr.set_options(keep_attrs=True)\n\nI‚Äôm going to use one of xarray‚Äôs tutorial datasets. In this case, air temperature from the NCEP reanalysis. I‚Äôll assign the result of the open_dataset to ds. I may change this to access a dataset directly\n\nds = xr.tutorial.open_dataset(\"air_temperature\")\n\nAs we are in an interactive environment, we can just type ds to see what we have.\n\nds\n\nFirst thing to notice is that ds is an xarray.Dataset object. It has dimensions, lat, lon, and time. It also has coordinate variables with the same names as these dimensions. These coordinate variables are 1-dimensional. This is a NetCDF convention. The Dataset contains one data variable, air. This has dimensions (time, lat, lon).\nClicking on the document icon reveals attributes for each variable. Clicking on the disk icon reveals a representation of the data.\nEach of the data and coordinate variables can be accessed and examined using the variable name as a key.\n\nds.air\n\n\nds['air']\n\nThese are xarray.DataArray objects. This is the basic building block for xarray.\nVariables can also be accessed as attributes of ds.\n\nds.time\n\nA major difference between accessing a variable as an attribute versus using a key is that the attribute is read-only but the key method can be used to update the variable. For example, if I want to convert the units of air from Kelvin to degrees Celsius.\n\nds['air'] = ds.air - 273.15\n\nThis approach can also be used to add new variables\n\nds['air_kelvin'] = ds.air + 273.15\n\nIt is helpful to update attributes such as units, this saves time, confusion and mistakes, especially when you save the dataset.\n\nds['air'].attrs['units'] = 'degC'\n\n\nds"
  },
  {
    "objectID": "python-notebooks/02_Xarray.html#subsetting-and-indexing",
    "href": "python-notebooks/02_Xarray.html#subsetting-and-indexing",
    "title": "03. Introduction to xarray",
    "section": "Subsetting and Indexing",
    "text": "Subsetting and Indexing\nSubsetting and indexing methods depend on whether you are working with a Dataset or DataArray. A DataArray can be accessed using positional indexing just like a numpy array. To access the temperature field for the first time step, you do the following.\n\nds['air'][0,:,:]\n\nNote this returns a DataArray with coordinates but not attributes.\nHowever, the real power is being able to access variables using coordinate variables. I can get the same subset using the following. (It‚Äôs also more explicit about what is being selected and robust in case I modify the DataArray and expect the same output.)\n\nds['air'].sel(time='2013-01-01').time\n\n\nds.air.sel(time='2013-01-01')\n\nI can also do slices. I‚Äôll extract temperatures for the state of Colorado. The bounding box for the state is [-109 E, -102 E, 37 N, 41 N].\nIn the code below, pay attention to both the order of the coordinates and the range of values. The first value of the lat coordinate variable is 41 N, the second value is 37 N. Unfortunately, xarray expects slices of coordinates to be in the same order as the coordinates. Note lon is 0 to 360 not -180 to 180, and I let python calculate it for me within the slice.\n\nds.air.sel(lat=slice(41.,37.), lon=slice(360-109,360-102))\n\nWhat if we want temperature for a point, for example Denver, CO (39.72510678889283 N, -104.98785545855408 E). xarray can handle this! If we just want data from the nearest grid point, we can use sel and specify the method as ‚Äúnearest‚Äù.\n\ndenver_lat, denver_lon = 39.72510678889283, -104.98785545855408\n\n\nds.air.sel(lat=denver_lat, lon=360+denver_lon, method='nearest')\n\nIf we want to interpolate, we can use interp(). In this case I use linear or bilinear interpolation.\ninterp() can also be used to resample data to a new grid and even reproject data\n\nds.air.interp(lat=denver_lat, lon=360+denver_lon, method='linear')\n\nsel() and interp() can also be used on Dataset objects.\n\nds.sel(lat=slice(41.,37.), lon=slice(360-109,360-102))\n\n\nds.interp(lat=denver_lat, lon=360+denver_lon, method='linear')"
  },
  {
    "objectID": "python-notebooks/02_Xarray.html#analysis",
    "href": "python-notebooks/02_Xarray.html#analysis",
    "title": "03. Introduction to xarray",
    "section": "Analysis",
    "text": "Analysis\nAs a simple example, let‚Äôs try to calculate a mean field for the whole time range.\n\nds.mean(dim='time')\n\nWe can also calculate a zonal mean (averaging over longitude)\n\nds.mean(dim='lon')\n\nOther aggregation methods include min(), max(), std(), along with others.\n\nds.std(dim='time')\n\nThe data we have are in 6h timesteps. This can be resampled to daily or monthly. If you are familiar with pandas, xarray uses the same methods.\n\nds.resample(time='M').mean()\n\n\nds_mon = ds.resample(time='M').mean()\nds_mon\n\nThis is a really short time series but as an example, let‚Äôs calculate a monthly climatology (at least for 2 months). For this we can use groupby()\n\nds_clim = ds_mon.groupby(ds_mon.time.dt.month).mean()"
  },
  {
    "objectID": "python-notebooks/02_Xarray.html#plot-results",
    "href": "python-notebooks/02_Xarray.html#plot-results",
    "title": "03. Introduction to xarray",
    "section": "Plot results",
    "text": "Plot results\nFinally, let‚Äôs plot the results! This will plot the lat/lon axes of the original ds DataArray.\n\nds_clim.air.sel(month=10).plot()"
  },
  {
    "objectID": "python-notebooks/02_Xarray_hvplot.html#why-do-we-need-xarray",
    "href": "python-notebooks/02_Xarray_hvplot.html#why-do-we-need-xarray",
    "title": "03. Introduction to xarray",
    "section": "Why do we need xarray?",
    "text": "Why do we need xarray?\nAs Geoscientists, we often work with time series of data with two or more dimensions: a time series of calibrated, orthorectified satellite images; two-dimensional grids of surface air temperature from an atmospheric reanalysis; or three-dimensional (level, x, y) cubes of ocean salinity from an ocean model. These data are often provided in GeoTIFF, NetCDF or HDF format with rich and useful metadata that we want to retain, or even use in our analysis. Common analyses include calculating means, standard deviations and anomalies over time or one or more spatial dimensions (e.g.¬†zonal means). Model output often includes multiple variables that you want to apply similar analyses to.\n\n\n\nA schematic of multi-dimensional data\n\n\nThe schematic above shows a typical data structure for multi-dimensional data. There are two data cubes, one for temperature and one for precipitation. Common coordinate variables, in this case latitude, longitude and time are associated with each variable. Each variable, including coordinate variables, will have a set of attributes: name, units, missing value, etc. The file containing the data may also have attributes: source of the data, model name coordinate reference system if the data are projected. Writing code using low-level packages such as netcdf4 and numpy to read the data, then perform analysis, and write the results to file is time consuming and prone to errors."
  },
  {
    "objectID": "python-notebooks/02_Xarray_hvplot.html#what-is-xarray",
    "href": "python-notebooks/02_Xarray_hvplot.html#what-is-xarray",
    "title": "03. Introduction to xarray",
    "section": "What is xarray",
    "text": "What is xarray\nxarray is an open-source project and python package to work with labelled multi-dimensional arrays. It is leverages numpy, pandas, matplotlib and dask to build Dataset and DataArray objects with built-in methods to subset, analyze, interpolate, and plot multi-dimensional data. It makes working with multi-dimensional data cubes efficient and fun. It will change your life for the better. You‚Äôll be more attractive, more interesting, and better equiped to take on lifes challenges."
  },
  {
    "objectID": "python-notebooks/02_Xarray_hvplot.html#what-you-will-learn-from-this-tutorial",
    "href": "python-notebooks/02_Xarray_hvplot.html#what-you-will-learn-from-this-tutorial",
    "title": "03. Introduction to xarray",
    "section": "What you will learn from this tutorial",
    "text": "What you will learn from this tutorial\nIn this tutorial you will learn how to:\n\nload a netcdf file into xarray\ninterrogate the Dataset and understand the difference between DataArray and Dataset\nsubset a Dataset\ncalculate annual and monthly mean fields\ncalculate a time series of zonal means\nplot these results\n\nAs always, we‚Äôll start by importing xarray. We‚Äôll follow convention by giving the module the shortname xr\n\nimport xarray as xr\nxr.set_options(keep_attrs=True)\nimport hvplot.xarray\n\n\n\n\n\n\n\n\n\n\n\nI‚Äôm going to use one of xarray‚Äôs tutorial datasets. In this case, air temperature from the NCEP reanalysis. I‚Äôll assign the result of the open_dataset to ds. I may change this to access a dataset directly\n\nds = xr.tutorial.open_dataset(\"air_temperature\")\n\nAs we are in an interactive environment, we can just type ds to see what we have.\n\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (lat: 25, time: 2920, lon: 53)\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nData variables:\n    air      (time, lat, lon) float32 ...\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...xarray.DatasetDimensions:lat: 25time: 2920lon: 53Coordinates: (3)lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (1)air(time, lat, lon)float32...long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ][3869000 values with dtype=float32]Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis\n(4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html\n\n\nFirst thing to notice is that ds is an xarray.Dataset object. It has dimensions, lat, lon, and time. It also has coordinate variables with the same names as these dimensions. These coordinate variables are 1-dimensional. This is a NetCDF convention. The Dataset contains one data variable, air. This has dimensions (time, lat, lon).\nClicking on the document icon reveals attributes for each variable. Clicking on the disk icon reveals a representation of the data.\nEach of the data and coordinate variables can be accessed and examined using the variable name as a key.\n\nds.air\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'air' (time: 2920, lat: 25, lon: 53)&gt;\n[3869000 values with dtype=float32]\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degK\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]xarray.DataArray'air'time: 2920lat: 25lon: 53...[3869000 values with dtype=float32]Coordinates: (3)lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Attributes: (11)long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]\n\n\n\nds['air']\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'air' (time: 2920, lat: 25, lon: 53)&gt;\n[3869000 values with dtype=float32]\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degK\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]xarray.DataArray'air'time: 2920lat: 25lon: 53...[3869000 values with dtype=float32]Coordinates: (3)lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Attributes: (11)long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]\n\n\nThese are xarray.DataArray objects. This is the basic building block for xarray.\nVariables can also be accessed as attributes of ds.\n\nds.time\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'time' (time: 2920)&gt;\narray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')\nCoordinates:\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nAttributes:\n    standard_name:  time\n    long_name:      Timexarray.DataArray'time'time: 29202013-01-01 2013-01-01T06:00:00 ... 2014-12-31T18:00:00array(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Coordinates: (1)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Attributes: (2)standard_name :timelong_name :Time\n\n\nA major difference between accessing a variable as an attribute versus using a key is that the attribute is read-only but the key method can be used to update the variable. For example, if I want to convert the units of air from Kelvin to degrees Celsius.\n\nds['air'] = ds.air - 273.15\n\nThis approach can also be used to add new variables\n\nds['air_kelvin'] = ds.air + 273.15\n\nIt is helpful to update attributes such as units, this saves time, confusion and mistakes, especially when you save the dataset.\n\nds['air'].attrs['units'] = 'degC'\n\n\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:     (lat: 25, time: 2920, lon: 53)\nCoordinates:\n  * lat         (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 22.5 20.0 17.5 15.0\n  * lon         (lon) float32 200.0 202.5 205.0 207.5 ... 325.0 327.5 330.0\n  * time        (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nData variables:\n    air         (time, lat, lon) float32 -31.95 -30.65 -29.65 ... 23.04 22.54\n    air_kelvin  (time, lat, lon) float32 241.2 242.5 243.5 ... 296.5 296.2 295.7\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...xarray.DatasetDimensions:lat: 25time: 2920lon: 53Coordinates: (3)lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (2)air(time, lat, lon)float32-31.95 -30.65 ... 23.04 22.54long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[[-31.949997, -30.649994, -29.649994, ..., -40.350006,\n         -37.649994, -34.550003],\n        [-29.350006, -28.649994, -28.449997, ..., -40.350006,\n         -37.850006, -33.850006],\n        [-23.149994, -23.350006, -24.259995, ..., -39.949997,\n         -36.759995, -31.449997],\n        ...,\n        [ 23.450012,  23.049988,  23.25    , ...,  22.25    ,\n          21.950012,  21.549988],\n        [ 22.75    ,  23.049988,  23.640015, ...,  22.75    ,\n          22.75    ,  22.049988],\n        [ 23.140015,  23.640015,  23.950012, ...,  23.75    ,\n          23.640015,  23.450012]],\n\n       [[-31.050003, -30.449997, -30.050003, ..., -41.149994,\n         -39.550003, -37.350006],\n        [-29.550003, -29.050003, -28.949997, ..., -42.149994,\n         -40.649994, -37.449997],\n        [-19.949997, -20.259995, -21.050003, ..., -42.350006,\n         -39.759995, -34.649994],\n...\n        [ 20.540009,  20.73999 ,  22.23999 , ...,  21.940002,\n          21.540009,  21.140015],\n        [ 23.140015,  24.040009,  24.440002, ...,  22.140015,\n          21.940002,  21.23999 ],\n        [ 24.640015,  25.23999 ,  25.339996, ...,  22.540009,\n          22.339996,  22.040009]],\n\n       [[-28.059998, -28.86    , -29.86    , ..., -31.460007,\n         -31.660004, -31.36    ],\n        [-23.259995, -23.86    , -24.759995, ..., -33.559998,\n         -32.86    , -31.460007],\n        [-10.160004, -10.959991, -11.76001 , ..., -33.259995,\n         -30.559998, -26.86    ],\n        ...,\n        [ 20.640015,  20.540009,  21.940002, ...,  22.140015,\n          21.940002,  21.540009],\n        [ 22.940002,  23.73999 ,  24.040009, ...,  22.540009,\n          22.540009,  22.040009],\n        [ 24.540009,  24.940002,  24.940002, ...,  23.339996,\n          23.040009,  22.540009]]], dtype=float32)air_kelvin(time, lat, lon)float32241.2 242.5 243.5 ... 296.2 295.7long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[[241.2    , 242.5    , 243.5    , ..., 232.79999, 235.5    ,\n         238.59999],\n        [243.79999, 244.5    , 244.7    , ..., 232.79999, 235.29999,\n         239.29999],\n        [250.     , 249.79999, 248.89   , ..., 233.2    , 236.39   ,\n         241.7    ],\n        ...,\n        [296.6    , 296.19998, 296.4    , ..., 295.4    , 295.1    ,\n         294.69998],\n        [295.9    , 296.19998, 296.79   , ..., 295.9    , 295.9    ,\n         295.19998],\n        [296.29   , 296.79   , 297.1    , ..., 296.9    , 296.79   ,\n         296.6    ]],\n\n       [[242.09999, 242.7    , 243.09999, ..., 232.     , 233.59999,\n         235.79999],\n        [243.59999, 244.09999, 244.2    , ..., 231.     , 232.5    ,\n         235.7    ],\n        [253.2    , 252.89   , 252.09999, ..., 230.79999, 233.39   ,\n         238.5    ],\n...\n        [293.69   , 293.88998, 295.38998, ..., 295.09   , 294.69   ,\n         294.29   ],\n        [296.29   , 297.19   , 297.59   , ..., 295.29   , 295.09   ,\n         294.38998],\n        [297.79   , 298.38998, 298.49   , ..., 295.69   , 295.49   ,\n         295.19   ]],\n\n       [[245.09   , 244.29   , 243.29   , ..., 241.68999, 241.48999,\n         241.79   ],\n        [249.89   , 249.29   , 248.39   , ..., 239.59   , 240.29   ,\n         241.68999],\n        [262.99   , 262.19   , 261.38998, ..., 239.89   , 242.59   ,\n         246.29   ],\n        ...,\n        [293.79   , 293.69   , 295.09   , ..., 295.29   , 295.09   ,\n         294.69   ],\n        [296.09   , 296.88998, 297.19   , ..., 295.69   , 295.69   ,\n         295.19   ],\n        [297.69   , 298.09   , 298.09   , ..., 296.49   , 296.19   ,\n         295.69   ]]], dtype=float32)Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis\n(4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html"
  },
  {
    "objectID": "python-notebooks/02_Xarray_hvplot.html#subsetting-and-indexing",
    "href": "python-notebooks/02_Xarray_hvplot.html#subsetting-and-indexing",
    "title": "03. Introduction to xarray",
    "section": "Subsetting and Indexing",
    "text": "Subsetting and Indexing\nSubsetting and indexing methods depend on whether you are working with a Dataset or DataArray. A DataArray can be accessed using positional indexing just like a numpy array. To access the temperature field for the first time step, you do the following.\n\nds['air'][0,:,:]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'air' (lat: 25, lon: 53)&gt;\narray([[-31.949997, -30.649994, -29.649994, ..., -40.350006, -37.649994,\n        -34.550003],\n       [-29.350006, -28.649994, -28.449997, ..., -40.350006, -37.850006,\n        -33.850006],\n       [-23.149994, -23.350006, -24.259995, ..., -39.949997, -36.759995,\n        -31.449997],\n       ...,\n       [ 23.450012,  23.049988,  23.25    , ...,  22.25    ,  21.950012,\n         21.549988],\n       [ 22.75    ,  23.049988,  23.640015, ...,  22.75    ,  22.75    ,\n         22.049988],\n       [ 23.140015,  23.640015,  23.950012, ...,  23.75    ,  23.640015,\n         23.450012]], dtype=float32)\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n    time     datetime64[ns] 2013-01-01\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degC\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]xarray.DataArray'air'lat: 25lon: 53-31.95 -30.65 -29.65 -29.15 -29.05 ... 24.64 24.45 23.75 23.64 23.45array([[-31.949997, -30.649994, -29.649994, ..., -40.350006, -37.649994,\n        -34.550003],\n       [-29.350006, -28.649994, -28.449997, ..., -40.350006, -37.850006,\n        -33.850006],\n       [-23.149994, -23.350006, -24.259995, ..., -39.949997, -36.759995,\n        -31.449997],\n       ...,\n       [ 23.450012,  23.049988,  23.25    , ...,  22.25    ,  21.950012,\n         21.549988],\n       [ 22.75    ,  23.049988,  23.640015, ...,  22.75    ,  22.75    ,\n         22.049988],\n       [ 23.140015,  23.640015,  23.950012, ...,  23.75    ,  23.640015,\n         23.450012]], dtype=float32)Coordinates: (3)lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)time()datetime64[ns]2013-01-01standard_name :timelong_name :Timearray('2013-01-01T00:00:00.000000000', dtype='datetime64[ns]')Attributes: (11)long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]\n\n\nNote this returns a DataArray with coordinates but not attributes.\nHowever, the real power is being able to access variables using coordinate variables. I can get the same subset using the following. (It‚Äôs also more explicit about what is being selected and robust in case I modify the DataArray and expect the same output.)\n\nds['air'].sel(time='2013-01-01').time\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'time' (time: 4)&gt;\narray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', '2013-01-01T18:00:00.000000000'],\n      dtype='datetime64[ns]')\nCoordinates:\n  * time     (time) datetime64[ns] 2013-01-01 ... 2013-01-01T18:00:00\nAttributes:\n    standard_name:  time\n    long_name:      Timexarray.DataArray'time'time: 42013-01-01 2013-01-01T06:00:00 2013-01-01T12:00:00 2013-01-01T18:00:00array(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', '2013-01-01T18:00:00.000000000'],\n      dtype='datetime64[ns]')Coordinates: (1)time(time)datetime64[ns]2013-01-01 ... 2013-01-01T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', '2013-01-01T18:00:00.000000000'],\n      dtype='datetime64[ns]')Attributes: (2)standard_name :timelong_name :Time\n\n\n\nds.air.sel(time='2013-01-01')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'air' (time: 4, lat: 25, lon: 53)&gt;\narray([[[-31.949997, -30.649994, -29.649994, ..., -40.350006,\n         -37.649994, -34.550003],\n        [-29.350006, -28.649994, -28.449997, ..., -40.350006,\n         -37.850006, -33.850006],\n        [-23.149994, -23.350006, -24.259995, ..., -39.949997,\n         -36.759995, -31.449997],\n        ...,\n        [ 23.450012,  23.049988,  23.25    , ...,  22.25    ,\n          21.950012,  21.549988],\n        [ 22.75    ,  23.049988,  23.640015, ...,  22.75    ,\n          22.75    ,  22.049988],\n        [ 23.140015,  23.640015,  23.950012, ...,  23.75    ,\n          23.640015,  23.450012]],\n\n       [[-31.050003, -30.449997, -30.050003, ..., -41.149994,\n         -39.550003, -37.350006],\n        [-29.550003, -29.050003, -28.949997, ..., -42.149994,\n         -40.649994, -37.449997],\n        [-19.949997, -20.259995, -21.050003, ..., -42.350006,\n         -39.759995, -34.649994],\n...\n        [ 22.450012,  22.25    ,  22.25    , ...,  23.140015,\n          22.140015,  21.850006],\n        [ 23.049988,  23.350006,  23.140015, ...,  23.25    ,\n          22.850006,  22.450012],\n        [ 23.25    ,  23.140015,  23.25    , ...,  23.850006,\n          23.850006,  23.640015]],\n\n       [[-31.259995, -31.350006, -31.350006, ..., -38.759995,\n         -37.649994, -35.550003],\n        [-26.850006, -27.850006, -28.949997, ..., -42.259995,\n         -41.649994, -38.649994],\n        [-16.549988, -18.449997, -21.050003, ..., -42.449997,\n         -41.350006, -37.050003],\n        ...,\n        [ 23.450012,  23.25    ,  22.850006, ...,  23.350006,\n          22.640015,  22.140015],\n        [ 23.850006,  24.350006,  23.950012, ...,  23.640015,\n          23.450012,  23.140015],\n        [ 24.350006,  24.549988,  24.350006, ...,  24.640015,\n          24.850006,  24.75    ]]], dtype=float32)\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * time     (time) datetime64[ns] 2013-01-01 ... 2013-01-01T18:00:00\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degC\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]xarray.DataArray'air'time: 4lat: 25lon: 53-31.95 -30.65 -29.65 -29.15 -29.05 ... 25.45 25.05 24.64 24.85 24.75array([[[-31.949997, -30.649994, -29.649994, ..., -40.350006,\n         -37.649994, -34.550003],\n        [-29.350006, -28.649994, -28.449997, ..., -40.350006,\n         -37.850006, -33.850006],\n        [-23.149994, -23.350006, -24.259995, ..., -39.949997,\n         -36.759995, -31.449997],\n        ...,\n        [ 23.450012,  23.049988,  23.25    , ...,  22.25    ,\n          21.950012,  21.549988],\n        [ 22.75    ,  23.049988,  23.640015, ...,  22.75    ,\n          22.75    ,  22.049988],\n        [ 23.140015,  23.640015,  23.950012, ...,  23.75    ,\n          23.640015,  23.450012]],\n\n       [[-31.050003, -30.449997, -30.050003, ..., -41.149994,\n         -39.550003, -37.350006],\n        [-29.550003, -29.050003, -28.949997, ..., -42.149994,\n         -40.649994, -37.449997],\n        [-19.949997, -20.259995, -21.050003, ..., -42.350006,\n         -39.759995, -34.649994],\n...\n        [ 22.450012,  22.25    ,  22.25    , ...,  23.140015,\n          22.140015,  21.850006],\n        [ 23.049988,  23.350006,  23.140015, ...,  23.25    ,\n          22.850006,  22.450012],\n        [ 23.25    ,  23.140015,  23.25    , ...,  23.850006,\n          23.850006,  23.640015]],\n\n       [[-31.259995, -31.350006, -31.350006, ..., -38.759995,\n         -37.649994, -35.550003],\n        [-26.850006, -27.850006, -28.949997, ..., -42.259995,\n         -41.649994, -38.649994],\n        [-16.549988, -18.449997, -21.050003, ..., -42.449997,\n         -41.350006, -37.050003],\n        ...,\n        [ 23.450012,  23.25    ,  22.850006, ...,  23.350006,\n          22.640015,  22.140015],\n        [ 23.850006,  24.350006,  23.950012, ...,  23.640015,\n          23.450012,  23.140015],\n        [ 24.350006,  24.549988,  24.350006, ...,  24.640015,\n          24.850006,  24.75    ]]], dtype=float32)Coordinates: (3)lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)time(time)datetime64[ns]2013-01-01 ... 2013-01-01T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', '2013-01-01T18:00:00.000000000'],\n      dtype='datetime64[ns]')Attributes: (11)long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]\n\n\nI can also do slices. I‚Äôll extract temperatures for the state of Colorado. The bounding box for the state is [-109 E, -102 E, 37 N, 41 N].\nIn the code below, pay attention to both the order of the coordinates and the range of values. The first value of the lat coordinate variable is 41 N, the second value is 37 N. Unfortunately, xarray expects slices of coordinates to be in the same order as the coordinates. Note lon is 0 to 360 not -180 to 180, and I let python calculate it for me within the slice.\n\nds.air.sel(lat=slice(41.,37.), lon=slice(360-109,360-102))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'air' (time: 2920, lat: 2, lon: 3)&gt;\narray([[[-10.049988 ,  -9.25     ,  -8.75     ],\n        [ -6.25     ,  -6.549988 ,  -6.3599854]],\n\n       [[-18.149994 , -14.950012 ,  -9.950012 ],\n        [-13.649994 , -11.049988 ,  -7.25     ]],\n\n       [[-20.449997 , -18.649994 , -13.359985 ],\n        [-19.350006 , -16.950012 , -11.25     ]],\n\n       ...,\n\n       [[-24.460007 , -28.259995 , -25.759995 ],\n        [-16.959991 , -24.059998 , -24.059998 ]],\n\n       [[-24.36     , -26.160004 , -23.460007 ],\n        [-15.959991 , -22.86     , -22.960007 ]],\n\n       [[-17.559998 , -15.359985 , -13.660004 ],\n        [-13.76001  , -15.959991 , -14.459991 ]]], dtype=float32)\nCoordinates:\n  * lat      (lat) float32 40.0 37.5\n  * lon      (lon) float32 252.5 255.0 257.5\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degC\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]xarray.DataArray'air'time: 2920lat: 2lon: 3-10.05 -9.25 -8.75 -6.25 -6.55 ... -15.36 -13.66 -13.76 -15.96 -14.46array([[[-10.049988 ,  -9.25     ,  -8.75     ],\n        [ -6.25     ,  -6.549988 ,  -6.3599854]],\n\n       [[-18.149994 , -14.950012 ,  -9.950012 ],\n        [-13.649994 , -11.049988 ,  -7.25     ]],\n\n       [[-20.449997 , -18.649994 , -13.359985 ],\n        [-19.350006 , -16.950012 , -11.25     ]],\n\n       ...,\n\n       [[-24.460007 , -28.259995 , -25.759995 ],\n        [-16.959991 , -24.059998 , -24.059998 ]],\n\n       [[-24.36     , -26.160004 , -23.460007 ],\n        [-15.959991 , -22.86     , -22.960007 ]],\n\n       [[-17.559998 , -15.359985 , -13.660004 ],\n        [-13.76001  , -15.959991 , -14.459991 ]]], dtype=float32)Coordinates: (3)lat(lat)float3240.0 37.5standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([40. , 37.5], dtype=float32)lon(lon)float32252.5 255.0 257.5standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([252.5, 255. , 257.5], dtype=float32)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Attributes: (11)long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]\n\n\nWhat if we want temperature for a point, for example Denver, CO (39.72510678889283 N, -104.98785545855408 E). xarray can handle this! If we just want data from the nearest grid point, we can use sel and specify the method as ‚Äúnearest‚Äù.\n\ndenver_lat, denver_lon = 39.72510678889283, -104.98785545855408\n\n\nds.air.sel(lat=denver_lat, lon=360+denver_lon, method='nearest').hvplot()\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nIf we want to interpolate, we can use interp(). In this case I use linear or bilinear interpolation.\ninterp() can also be used to resample data to a new grid and even reproject data\n\nds.air.interp(lat=denver_lat, lon=360+denver_lon, method='linear')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'air' (time: 2920)&gt;\narray([ -8.95085077, -14.49752791, -18.43715163, ..., -27.78736503,\n       -25.78552388, -15.41780902])\nCoordinates:\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\n    lat      float64 39.73\n    lon      float64 255.0\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degC\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]xarray.DataArray'air'time: 2920-8.951 -14.5 -18.44 -11.33 -8.942 ... -22.4 -27.79 -25.79 -15.42array([ -8.95085077, -14.49752791, -18.43715163, ..., -27.78736503,\n       -25.78552388, -15.41780902])Coordinates: (3)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')lat()float6439.73standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray(39.72510679)lon()float64255.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray(255.01214454)Attributes: (11)long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]\n\n\nsel() and interp() can also be used on Dataset objects.\n\nds.sel(lat=slice(41,37), lon=slice(360-109,360-102))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:     (lat: 2, time: 2920, lon: 3)\nCoordinates:\n  * lat         (lat) float32 40.0 37.5\n  * lon         (lon) float32 252.5 255.0 257.5\n  * time        (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nData variables:\n    air         (time, lat, lon) float32 -10.05 -9.25 -8.75 ... -15.96 -14.46\n    air_kelvin  (time, lat, lon) float32 263.1 263.9 264.4 ... 259.4 257.2 258.7\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...xarray.DatasetDimensions:lat: 2time: 2920lon: 3Coordinates: (3)lat(lat)float3240.0 37.5standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([40. , 37.5], dtype=float32)lon(lon)float32252.5 255.0 257.5standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([252.5, 255. , 257.5], dtype=float32)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (2)air(time, lat, lon)float32-10.05 -9.25 ... -15.96 -14.46long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[[-10.049988 ,  -9.25     ,  -8.75     ],\n        [ -6.25     ,  -6.549988 ,  -6.3599854]],\n\n       [[-18.149994 , -14.950012 ,  -9.950012 ],\n        [-13.649994 , -11.049988 ,  -7.25     ]],\n\n       [[-20.449997 , -18.649994 , -13.359985 ],\n        [-19.350006 , -16.950012 , -11.25     ]],\n\n       ...,\n\n       [[-24.460007 , -28.259995 , -25.759995 ],\n        [-16.959991 , -24.059998 , -24.059998 ]],\n\n       [[-24.36     , -26.160004 , -23.460007 ],\n        [-15.959991 , -22.86     , -22.960007 ]],\n\n       [[-17.559998 , -15.359985 , -13.660004 ],\n        [-13.76001  , -15.959991 , -14.459991 ]]], dtype=float32)air_kelvin(time, lat, lon)float32263.1 263.9 264.4 ... 257.2 258.7long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[[263.1    , 263.9    , 264.4    ],\n        [266.9    , 266.6    , 266.79   ]],\n\n       [[255.     , 258.19998, 263.19998],\n        [259.5    , 262.1    , 265.9    ]],\n\n       [[252.7    , 254.5    , 259.79   ],\n        [253.79999, 256.19998, 261.9    ]],\n\n       ...,\n\n       [[248.68999, 244.89   , 247.39   ],\n        [256.19   , 249.09   , 249.09   ]],\n\n       [[248.79   , 246.98999, 249.68999],\n        [257.19   , 250.29   , 250.18999]],\n\n       [[255.59   , 257.79   , 259.49   ],\n        [259.38998, 257.19   , 258.69   ]]], dtype=float32)Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis\n(4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html\n\n\n\nds.interp(lat=denver_lat, lon=360+denver_lon, method='linear')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:     (time: 2920)\nCoordinates:\n  * time        (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\n    lat         float64 39.73\n    lon         float64 255.0\nData variables:\n    air         (time) float64 -8.951 -14.5 -18.44 ... -27.79 -25.79 -15.42\n    air_kelvin  (time) float64 264.2 258.7 254.7 261.8 ... 245.4 247.4 257.7\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...xarray.DatasetDimensions:time: 2920Coordinates: (3)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')lat()float6439.73standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray(39.72510679)lon()float64255.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray(255.01214454)Data variables: (2)air(time)float64-8.951 -14.5 ... -25.79 -15.42long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([ -8.95085077, -14.49752791, -18.43715163, ..., -27.78736503,\n       -25.78552388, -15.41780902])air_kelvin(time)float64264.2 258.7 254.7 ... 247.4 257.7long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([264.19914312, 258.65246598, 254.71284227, ..., 245.36262886,\n       247.36447002, 257.73218487])Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis\n(4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html"
  },
  {
    "objectID": "python-notebooks/02_Xarray_hvplot.html#analysis",
    "href": "python-notebooks/02_Xarray_hvplot.html#analysis",
    "title": "03. Introduction to xarray",
    "section": "Analysis",
    "text": "Analysis\nAs a simple example, let‚Äôs try to calculate a mean field for the whole time range.\n\nds.mean(dim='time').hvplot()\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nWe can also calculate a zonal mean (averaging over longitude)\n\nds.mean(dim='lon').hvplot()\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nOther aggregation methods include min(), max(), std(), along with others.\n\nds.std(dim='time').hvplot()\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nThe data we have are in 6h timesteps. This can be resampled to daily or monthly. If you are familiar with pandas, xarray uses the same methods.\n\nds.resample(time='M').mean().hvplot()\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\nds_mon = ds.resample(time='M').mean()\nds_mon\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:     (time: 24, lat: 25, lon: 53)\nCoordinates:\n  * time        (time) datetime64[ns] 2013-01-31 2013-02-28 ... 2014-12-31\n  * lat         (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 22.5 20.0 17.5 15.0\n  * lon         (lon) float32 200.0 202.5 205.0 207.5 ... 325.0 327.5 330.0\nData variables:\n    air         (time, lat, lon) float32 -28.68 -28.49 -28.48 ... 24.57 24.56\n    air_kelvin  (time, lat, lon) float32 244.5 244.7 244.7 ... 297.7 297.7 297.7\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...xarray.DatasetDimensions:time: 24lat: 25lon: 53Coordinates: (3)time(time)datetime64[ns]2013-01-31 ... 2014-12-31array(['2013-01-31T00:00:00.000000000', '2013-02-28T00:00:00.000000000',\n       '2013-03-31T00:00:00.000000000', '2013-04-30T00:00:00.000000000',\n       '2013-05-31T00:00:00.000000000', '2013-06-30T00:00:00.000000000',\n       '2013-07-31T00:00:00.000000000', '2013-08-31T00:00:00.000000000',\n       '2013-09-30T00:00:00.000000000', '2013-10-31T00:00:00.000000000',\n       '2013-11-30T00:00:00.000000000', '2013-12-31T00:00:00.000000000',\n       '2014-01-31T00:00:00.000000000', '2014-02-28T00:00:00.000000000',\n       '2014-03-31T00:00:00.000000000', '2014-04-30T00:00:00.000000000',\n       '2014-05-31T00:00:00.000000000', '2014-06-30T00:00:00.000000000',\n       '2014-07-31T00:00:00.000000000', '2014-08-31T00:00:00.000000000',\n       '2014-09-30T00:00:00.000000000', '2014-10-31T00:00:00.000000000',\n       '2014-11-30T00:00:00.000000000', '2014-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)Data variables: (2)air(time, lat, lon)float32-28.68 -28.49 ... 24.57 24.56long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[[-28.68323  , -28.486452 , -28.479755 , ..., -30.658554 ,\n         -29.743628 , -28.474194 ],\n        [-26.076784 , -26.127504 , -26.4225   , ..., -32.5679   ,\n         -31.105167 , -28.442825 ],\n        [-22.770565 , -23.31516  , -24.042498 , ..., -31.165657 ,\n         -28.38291  , -24.144924 ],\n        ...,\n        [ 22.688152 ,  22.00097  ,  21.773153 , ...,  22.218397 ,\n          21.734531 ,  21.118395 ],\n        [ 23.31952  ,  23.16702  ,  22.698233 , ...,  22.43775  ,\n          22.190727 ,  21.715578 ],\n        [ 23.903486 ,  23.89203  ,  23.585333 , ...,  23.154608 ,\n          22.947426 ,  22.889124 ]],\n\n       [[-32.41607  , -32.44866  , -32.738483 , ..., -31.54482  ,\n         -30.430185 , -29.205448 ],\n        [-31.216885 , -31.08063  , -31.236965 , ..., -32.135708 ,\n         -30.825186 , -28.42241  ],\n        [-27.826433 , -28.123934 , -28.78045  , ..., -29.734114 ,\n         -27.383936 , -23.491434 ],\n...\n        [ 24.899088 ,  24.200085 ,  24.072004 , ...,  24.861843 ,\n          24.510258 ,  23.995668 ],\n        [ 25.815008 ,  25.661922 ,  25.121607 , ...,  24.954088 ,\n          25.071083 ,  24.735588 ],\n        [ 26.023424 ,  26.06767  ,  25.74576  , ...,  25.566338 ,\n          25.591848 ,  25.630259 ]],\n\n       [[-26.348473 , -26.260897 , -26.380894 , ..., -33.07903  ,\n         -32.067986 , -30.868315 ],\n        [-25.419994 , -24.849277 , -24.405483 , ..., -34.531376 ,\n         -32.82783  , -30.179682 ],\n        [-23.181051 , -23.56476  , -23.574757 , ..., -35.446938 ,\n         -31.91259  , -26.923311 ],\n        ...,\n        [ 23.299198 ,  22.541454 ,  22.60839  , ...,  23.378307 ,\n          23.067505 ,  22.662996 ],\n        [ 24.295895 ,  24.286139 ,  24.031782 , ...,  23.80259  ,\n          23.908312 ,  23.579037 ],\n        [ 24.897346 ,  25.076134 ,  24.909689 , ...,  24.547583 ,\n          24.573233 ,  24.560413 ]]], dtype=float32)air_kelvin(time, lat, lon)float32244.5 244.7 244.7 ... 297.7 297.7long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[[244.4667 , 244.66354, 244.67027, ..., 242.49142, 243.40633,\n         244.67577],\n        [247.07323, 247.02248, 246.7275 , ..., 240.58205, 242.04489,\n         244.70726],\n        [250.37941, 249.83484, 249.10748, ..., 241.98434, 244.76712,\n         249.00505],\n        ...,\n        [295.83795, 295.15085, 294.9229 , ..., 295.36826, 294.88437,\n         294.26828],\n        [296.46942, 296.31686, 295.84802, ..., 295.5876 , 295.34058,\n         294.86536],\n        [297.05316, 297.0418 , 296.73517, ..., 296.30438, 296.09732,\n         296.0389 ]],\n\n       [[240.73384, 240.7013 , 240.4115 , ..., 241.60518, 242.71988,\n         243.94455],\n        [241.93309, 242.06935, 241.913  , ..., 241.01428, 242.32481,\n         244.72758],\n        [245.32361, 245.0261 , 244.36955, ..., 243.41588, 245.7661 ,\n         249.65858],\n...\n        [298.04895, 297.35007, 297.22195, ..., 298.01172, 297.66013,\n         297.14554],\n        [298.96484, 298.81186, 298.27136, ..., 298.10403, 298.22104,\n         297.88547],\n        [299.17334, 299.2175 , 298.89566, ..., 298.71625, 298.74167,\n         298.7802 ]],\n\n       [[246.80156, 246.88907, 246.76907, ..., 240.07089, 241.08206,\n         242.2817 ],\n        [247.72998, 248.30064, 248.74443, ..., 238.61859, 240.3222 ,\n         242.97026],\n        [249.96893, 249.58516, 249.57521, ..., 237.70308, 241.23743,\n         246.22667],\n        ...,\n        [296.4491 , 295.6914 , 295.75824, ..., 296.52817, 296.21747,\n         295.8128 ],\n        [297.44586, 297.43613, 297.1817 , ..., 296.95242, 297.05823,\n         296.72897],\n        [298.0472 , 298.22598, 298.0595 , ..., 297.6975 , 297.72318,\n         297.71024]]], dtype=float32)Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis\n(4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html\n\n\nThis is a really short time series but as an example, let‚Äôs calculate a monthly climatology (at least for 2 months). For this we can use groupby()\n\nds_clim = ds_mon.groupby(ds_mon.time.dt.month).mean()"
  },
  {
    "objectID": "python-notebooks/02_Xarray_hvplot.html#plot-results",
    "href": "python-notebooks/02_Xarray_hvplot.html#plot-results",
    "title": "03. Introduction to xarray",
    "section": "Plot results",
    "text": "Plot results\nFinally, let‚Äôs plot the results! This will plot the lat/lon axes of the original ds DataArray.\n\nds_clim.air.sel(month=10).plot()\n\n&lt;matplotlib.collections.QuadMesh at 0x7f22bb7acd90&gt;"
  },
  {
    "objectID": "python-notebooks/02_Zarr_Access_netCDF.html#summary",
    "href": "python-notebooks/02_Zarr_Access_netCDF.html#summary",
    "title": "Zarr Access for NetCDF4 files",
    "section": "Summary",
    "text": "Summary\nZarr is an open source library for storing N-dimensional array data. It supports multidimensional arrays with attributes and dimensions similar to NetCDF4, and it can be read by XArray. Zarr is often used for data held in cloud object storage (like Amazon S3), because it is better optimized for these situations than NetCDF4.\nDO NOT USE zarr-eodis-store is depends on only XArray\nThe zarr-eosdis-store library allows NASA EOSDIS NetCDF4 files to be read more efficiently by transferring only file metadata and data needed for computation in a small number of requests, rather than moving the whole file or making many small requests. It works by making the files directly readable by the Zarr Python library and XArray across a network. To use it, files must have a corresponding metadata file ending in .dmrpp, which increasingly true for cloud-accessible EOSDIS data. https://github.com/nasa/zarr-eosdis-store\nThe zarr-eosdis-store library provides several benefits over downloading EOSDIS data files and accessing them using XArray, NetCDF4, or HDF5 Python libraries:\n\nIt only downloads the chunks of data you actually read, so if you don‚Äôt read all variables or the full spatiotemporal extent of a file, you usually won‚Äôt spend time downloading those portions of the file\nIt parallelizes and optimizes downloads for the portions of files you do read, so download speeds can be faster in general\nIt automatically interoperates with Earthdata Login if you have a .netrc file set up\nIt is aware of some EOSDIS cloud implementation quirks and provides caching that can save time for repeated requests to individual files\n\nIt can also be faster than using XArray pointing NetCDF4 files with s3:// URLs, depending on the file‚Äôs internal structure, and is often more convenient.\nConsider using this library when: 1. The portion of the data file you need to use is much smaller than the full file, e.g.¬†in cases of spatial subsets or reading a single variable from a file containing several 1. s3:// URLs are not readily available 1. Code need to run outside of the AWS cloud or us-west-2 region or in a hybrid cloud / non-cloud manner 1. s3:// access using XArray seems slower than you would expect (possibly due to unoptimized internal file structure) 1. No readily-available, public, cloud-optimized version of the data exists already. The example we show is also available as an AWS Public Dataset: https://registry.opendata.aws/mur/ 1. Adding ‚Äú.dmrpp‚Äù to the end of a data URL returns a file\n\nObjectives\n\nBuild on prior knowledge from CMR and Earthdata Login tutorials\nWork through an example of using the EOSDIS Zarr Store to access data using XArray\nLearn about the Zarr format and library for accessing data in the cloud"
  },
  {
    "objectID": "python-notebooks/02_Zarr_Access_netCDF.html#exercise",
    "href": "python-notebooks/02_Zarr_Access_netCDF.html#exercise",
    "title": "Zarr Access for NetCDF4 files",
    "section": "Exercise",
    "text": "Exercise\nIn this exercise, we will be using the eosdis-zarr-store library to aggregate and analyze a month of sea surface temperature for the Great Lakes region\n\nSet up\n\nImport Required Packages\n\n# Core libraries for this tutorial\n# Available via `pip install zarr zarr-eosdis-store`\nfrom eosdis_store import EosdisStore\nimport xarray as xr\n\n# Other Python libraries\nimport requests\nfrom pqdm.threads import pqdm\nfrom matplotlib import animation, pyplot as plt\nfrom IPython.core.display import display, HTML\n\n# Python standard library imports\nfrom pprint import pprint\n\nAlso set the width / height for plots we show\n\nplt.rcParams['figure.figsize'] = 12, 6\n\n\n\nSet Dataset, Time, and Region of Interest\nLook in PO.DAAC‚Äôs cloud archive for Group for High Resolution Sea Surface Temperature (GHRSST) Level 4 Multiscale Ultrahigh Resolution (MUR) data\n\ndata_provider = 'POCLOUD'\nmur_short_name = 'MUR-JPL-L4-GLOB-v4.1'\n\nLooking for data from the month of September over the Great Lakes\n\nstart_time = '2021-09-01T21:00:00Z'\nend_time = '2021-09-30T20:59:59Z'\n\n# Bounding box around the Great Lakes\nlats = slice(41, 49)\nlons = slice(-93, -76)\n\n# Some other possibly interesting bounding boxes:\n\n# Hawaiian Islands\n# lats = slice(18, 22.5)\n# lons = slice(-161, -154)\n\n# Mediterranean Sea\n# lats = slice(29, 45)\n# lons = slice(-7, 37)\n\n\n\n\nFind URLs for the dataset and AOI\nSet up a CMR granules search for our area of interest, as we saw in prior tutorials\n\ncmr_url = 'https://cmr.earthdata.nasa.gov/search/granules.json'\n\nSearch for granules in our area of interest, expecting one granule per day of September\n\nresponse = requests.get(cmr_url, \n                        params={\n                            'provider': data_provider,\n                            'short_name': mur_short_name, \n                            'temporal': f'{start_time},{end_time}',\n                            'bounding_box': f'{lons.start},{lats.start},{lons.stop},{lats.stop}',\n                            'page_size': 2000,\n                            }\n                       )\n\n\ngranules = response.json()['feed']['entry']\n\nfor granule in granules:\n    print(granule['title'])\n\n20210901090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210902090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210903090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210904090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210905090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210906090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210907090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210908090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210909090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210910090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210911090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210912090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210913090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210914090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210915090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210916090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210917090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210918090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210919090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210920090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210921090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210922090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210923090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210924090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210925090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210926090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210927090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210928090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210929090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210930090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n\n\n\npprint(granules[0])\n\n{'boxes': ['-90 -180 90 180'],\n 'browse_flag': False,\n 'collection_concept_id': 'C1996881146-POCLOUD',\n 'coordinate_system': 'CARTESIAN',\n 'data_center': 'POCLOUD',\n 'dataset_id': 'GHRSST Level 4 MUR Global Foundation Sea Surface Temperature '\n               'Analysis (v4.1)',\n 'day_night_flag': 'UNSPECIFIED',\n 'granule_size': '9.059906005859375E-5',\n 'id': 'G2113241213-POCLOUD',\n 'links': [{'href': 's3://podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210901090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule.'},\n           {'href': 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210901090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download '\n                     '20210901090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc'},\n           {'href': 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-public/MUR-JPL-L4-GLOB-v4.1/20210901090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc.md5',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n            'title': 'Download '\n                     '20210901090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc.md5'},\n           {'href': 'https://archive.podaac.earthdata.nasa.gov/s3credentials',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n            'title': 'api endpoint to retrieve temporary credentials valid for '\n                     'same-region direct s3 access'},\n           {'href': 'https://opendap.earthdata.nasa.gov/collections/C1996881146-POCLOUD/granules/20210901090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/service#',\n            'title': 'OPeNDAP request URL'},\n           {'href': 'https://github.com/nasa/podaac_tools_and_services/tree/master/subset_opendap',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://ghrsst.jpl.nasa.gov',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://earthdata.nasa.gov/esds/competitive-programs/measures/mur-sst',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#'},\n           {'href': 'http://journals.ametsoc.org/doi/abs/10.1175/1520-0426%281998%29015%3C0741:BSHWSS%3E2.0.CO;2',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://podaac-tools.jpl.nasa.gov/drive/files/OceanTemperature/ghrsst/docs/GDS20r5.pdf',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://github.com/podaac/data-readers',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://doi.org/10.1016/j.rse.2017.07.029',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://registry.opendata.aws/mur/#usageexa',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#'},\n           {'href': 'http://www.ghrsst.org',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://podaac.jpl.nasa.gov/CitingPODAAC',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://cmr.earthdata.nasa.gov/virtual-directory/collections/C1996881146-POCLOUD ',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'length': '300.0MB',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'},\n           {'href': ' '\n                    'https://search.earthdata.nasa.gov/search/granules?p=C1996881146-POCLOUD ',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'length': '700.0MB',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'},\n           {'href': 'https://podaac.jpl.nasa.gov/MEaSUREs-MUR',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://github.com/podaac/tutorials/blob/master/notebooks/SWOT-EA-2021/Colocate_satellite_insitu_ocean.ipynb',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'}],\n 'online_access_flag': True,\n 'original_format': 'UMM_JSON',\n 'time_end': '2021-09-01T21:00:00.000Z',\n 'time_start': '2021-08-31T21:00:00.000Z',\n 'title': '20210901090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1',\n 'updated': '2021-09-10T07:29:40.511Z'}\n\n\n\nurls = []\nfor granule in granules:\n    for link in granule['links']:\n        if link['rel'].endswith('/data#'):\n            urls.append(link['href'])\n            break\npprint(urls)\n\n['https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210901090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210902090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210903090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210904090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210905090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210906090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210907090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210908090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210909090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210910090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210911090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210912090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210913090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210914090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210915090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210916090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210917090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210918090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210919090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210920090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210921090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210922090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210923090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210924090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210925090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210926090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210927090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210928090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210929090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210930090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc']\n\n\n\n\nOpen and view our AOI without downloading a whole file\n\nCheck to see if we can use an efficient partial-access technique\n\nresponse = requests.head(f'{urls[0]}.dmrpp')\n\nprint('Can we use EosdisZarrStore and XArray to access these files more efficiently?')\nprint('Yes' if response.ok else 'No')\n\nCan we use EosdisZarrStore and XArray to access these files more efficiently?\nYes\n\n\nOpen our first URL using the Zarr library\n\nurl = urls[0]\n\nds = xr.open_zarr(EosdisStore(url), consolidated=False)\n\nThat‚Äôs it! No downloads, temporary credentials, or S3 filesystems. Hereafter, we interact with the ds variable as with any XArray dataset. We need not worry about the EosdisStore anymore.\nView the file‚Äôs variable structure\n\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:           (time: 1, lat: 17999, lon: 36000)\nCoordinates:\n  * lat               (lat) float32 -89.99 -89.98 -89.97 ... 89.97 89.98 89.99\n  * lon               (lon) float32 -180.0 -180.0 -180.0 ... 180.0 180.0 180.0\n  * time              (time) datetime64[ns] 2021-09-01T09:00:00\nData variables:\n    analysed_sst      (time, lat, lon) float32 dask.array&lt;chunksize=(1, 1023, 2047), meta=np.ndarray&gt;\n    analysis_error    (time, lat, lon) float32 dask.array&lt;chunksize=(1, 1023, 2047), meta=np.ndarray&gt;\n    dt_1km_data       (time, lat, lon) timedelta64[ns] dask.array&lt;chunksize=(1, 1447, 2895), meta=np.ndarray&gt;\n    mask              (time, lat, lon) float32 dask.array&lt;chunksize=(1, 1447, 2895), meta=np.ndarray&gt;\n    sea_ice_fraction  (time, lat, lon) float32 dask.array&lt;chunksize=(1, 1447, 2895), meta=np.ndarray&gt;\n    sst_anomaly       (time, lat, lon) float32 dask.array&lt;chunksize=(1, 1023, 2047), meta=np.ndarray&gt;\nAttributes: (12/47)\n    Conventions:                CF-1.7\n    title:                      Daily MUR SST, Final product\n    summary:                    A merged, multi-sensor L4 Foundation SST anal...\n    references:                 http://podaac.jpl.nasa.gov/Multi-scale_Ultra-...\n    institution:                Jet Propulsion Laboratory\n    history:                    created at nominal 4-day latency; replaced nr...\n    ...                         ...\n    project:                    NASA Making Earth Science Data Records for Us...\n    publisher_name:             GHRSST Project Office\n    publisher_url:              http://www.ghrsst.org\n    publisher_email:            ghrsst-po@nceo.ac.uk\n    processing_level:           L4\n    cdm_data_type:              gridxarray.DatasetDimensions:time: 1lat: 17999lon: 36000Coordinates: (3)lat(lat)float32-89.99 -89.98 ... 89.98 89.99long_name :latitudestandard_name :latitudeaxis :Yunits :degrees_northvalid_min :-90.0valid_max :90.0comment :geolocations inherited from the input data without correctionarray([-89.99, -89.98, -89.97, ...,  89.97,  89.98,  89.99], dtype=float32)lon(lon)float32-180.0 -180.0 ... 180.0 180.0long_name :longitudestandard_name :longitudeaxis :Xunits :degrees_eastvalid_min :-180.0valid_max :180.0comment :geolocations inherited from the input data without correctionarray([-179.99, -179.98, -179.97, ...,  179.98,  179.99,  180.  ],\n      dtype=float32)time(time)datetime64[ns]2021-09-01T09:00:00long_name :reference time of sst fieldstandard_name :timeaxis :Tcomment :Nominal time of analyzed fieldsarray(['2021-09-01T09:00:00.000000000'], dtype='datetime64[ns]')Data variables: (6)analysed_sst(time, lat, lon)float32dask.array&lt;chunksize=(1, 1023, 2047), meta=np.ndarray&gt;long_name :analysed sea surface temperaturestandard_name :sea_surface_foundation_temperatureunits :kelvinvalid_min :-32767valid_max :32767comment :\\\"Final\\\" version using Multi-Resolution Variational Analysis (MRVA) method for interpolationsource :MODIS_T-JPL, MODIS_A-JPL, AMSR2-REMSS, AVHRRMTA_G-NAVO, AVHRRMTB_G-NAVO, iQUAM-NOAA/NESDIS, Ice_Conc-OSISAF\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.41 GiB\n7.99 MiB\n\n\nShape\n(1, 17999, 36000)\n(1, 1023, 2047)\n\n\nCount\n325 Tasks\n324 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\n\nanalysis_error\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 1023, 2047), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nestimated error standard deviation of analysed_sst\n\nunits :\n\nkelvin\n\nvalid_min :\n\n0\n\nvalid_max :\n\n32767\n\ncomment :\n\nuncertainty in \\\"analysed_sst\\\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.41 GiB\n7.99 MiB\n\n\nShape\n(1, 17999, 36000)\n(1, 1023, 2047)\n\n\nCount\n325 Tasks\n324 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\n\n\ndt_1km_data\n\n\n(time, lat, lon)\n\n\ntimedelta64[ns]\n\n\ndask.array&lt;chunksize=(1, 1447, 2895), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\ntime to most recent 1km data\n\nvalid_min :\n\n-127\n\nvalid_max :\n\n127\n\nsource :\n\nMODIS and VIIRS pixels ingested by MUR\n\ncomment :\n\nThe grid value is hours between the analysis time and the most recent MODIS or VIIRS 1km L2P datum within 0.01 degrees from the grid point. \\\"Fill value\\\" indicates absence of such 1km data at the grid point.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n4.83 GiB\n31.96 MiB\n\n\nShape\n(1, 17999, 36000)\n(1, 1447, 2895)\n\n\nCount\n170 Tasks\n169 Chunks\n\n\nType\ntimedelta64[ns]\nnumpy.ndarray\n\n\n\n\n\n\n\n\n\nmask\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 1447, 2895), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nsea/land field composite mask\n\nvalid_min :\n\n1\n\nvalid_max :\n\n31\n\nflag_masks :\n\n[1, 2, 4, 8, 16]\n\nflag_meanings :\n\nopen_sea land open_lake open_sea_with_ice_in_the_grid open_lake_with_ice_in_the_grid\n\ncomment :\n\nmask can be used to further filter the data.\n\nsource :\n\nGMT \\\"grdlandmask\\\", ice flag from sea_ice_fraction data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.41 GiB\n15.98 MiB\n\n\nShape\n(1, 17999, 36000)\n(1, 1447, 2895)\n\n\nCount\n170 Tasks\n169 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\n\n\nsea_ice_fraction\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 1447, 2895), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nsea ice area fraction\n\nstandard_name :\n\nsea_ice_area_fraction\n\nvalid_min :\n\n0\n\nvalid_max :\n\n100\n\nsource :\n\nEUMETSAT OSI-SAF, copyright EUMETSAT\n\ncomment :\n\nice fraction is a dimensionless quantity between 0 and 1; it has been interpolated by a nearest neighbor approach.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.41 GiB\n15.98 MiB\n\n\nShape\n(1, 17999, 36000)\n(1, 1447, 2895)\n\n\nCount\n170 Tasks\n169 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\n\n\nsst_anomaly\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 1023, 2047), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nSST anomaly from a seasonal SST climatology based on the MUR data over 2003-2014 period\n\nunits :\n\nkelvin\n\nvalid_min :\n\n-32767\n\nvalid_max :\n\n32767\n\ncomment :\n\nanomaly reference to the day-of-year average between 2003 and 2014\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.41 GiB\n7.99 MiB\n\n\nShape\n(1, 17999, 36000)\n(1, 1023, 2047)\n\n\nCount\n325 Tasks\n324 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\n\nAttributes: (47)Conventions :CF-1.7title :Daily MUR SST, Final productsummary :A merged, multi-sensor L4 Foundation SST analysis product from JPL.references :http://podaac.jpl.nasa.gov/Multi-scale_Ultra-high_Resolution_MUR-SSTinstitution :Jet Propulsion Laboratoryhistory :created at nominal 4-day latency; replaced nrt (1-day latency) version.comment :MUR = \\\"Multi-scale Ultra-high Resolution\\\"license :These data are available free of charge under data policy of JPL PO.DAAC.id :MUR-JPL-L4-GLOB-v04.1naming_authority :org.ghrsstproduct_version :04.1uuid :27665bc0-d5fc-11e1-9b23-0800200c9a66gds_version_id :2.0netcdf_version_id :4.1date_created :20210910T072132Zstart_time :20210901T090000Zstop_time :20210901T090000Ztime_coverage_start :20210831T210000Ztime_coverage_end :20210901T210000Zfile_quality_level :3source :MODIS_T-JPL, MODIS_A-JPL, AMSR2-REMSS, AVHRRMTA_G-NAVO, AVHRRMTB_G-NAVO, iQUAM-NOAA/NESDIS, Ice_Conc-OSISAFplatform :Terra, Aqua, GCOM-W, MetOp-A, MetOp-B, Buoys/Shipssensor :MODIS, AMSR2, AVHRR, in-situMetadata_Conventions :Unidata Observation Dataset v1.0metadata_link :http://podaac.jpl.nasa.gov/ws/metadata/dataset/?format=iso&shortName=MUR-JPL-L4-GLOB-v04.1keywords :Oceans &gt; Ocean Temperature &gt; Sea Surface Temperaturekeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordsstandard_name_vocabulary :NetCDF Climate and Forecast (CF) Metadata Conventionsouthernmost_latitude :-90.0northernmost_latitude :90.0westernmost_longitude :-180.0easternmost_longitude :180.0spatial_resolution :0.01 degreesgeospatial_lat_units :degrees northgeospatial_lat_resolution :0.009999999776geospatial_lon_units :degrees eastgeospatial_lon_resolution :0.009999999776acknowledgment :Please acknowledge the use of these data with the following statement:  These data were provided by JPL under support by NASA MEaSUREs program.creator_name :JPL MUR SST projectcreator_email :ghrsst@podaac.jpl.nasa.govcreator_url :http://mur.jpl.nasa.govproject :NASA Making Earth Science Data Records for Use in Research Environments (MEaSUREs) Programpublisher_name :GHRSST Project Officepublisher_url :http://www.ghrsst.orgpublisher_email :ghrsst-po@nceo.ac.ukprocessing_level :L4cdm_data_type :grid\n\n\n\nds.analysed_sst\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'analysed_sst' (time: 1, lat: 17999, lon: 36000)&gt;\ndask.array&lt;open_dataset-4d5a9a1e1fda090e80524b67b2e413c6analysed_sst, shape=(1, 17999, 36000), dtype=float32, chunksize=(1, 1023, 2047), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * lat      (lat) float32 -89.99 -89.98 -89.97 -89.96 ... 89.97 89.98 89.99\n  * lon      (lon) float32 -180.0 -180.0 -180.0 -180.0 ... 180.0 180.0 180.0\n  * time     (time) datetime64[ns] 2021-09-01T09:00:00\nAttributes:\n    long_name:      analysed sea surface temperature\n    standard_name:  sea_surface_foundation_temperature\n    units:          kelvin\n    valid_min:      -32767\n    valid_max:      32767\n    comment:        \\\"Final\\\" version using Multi-Resolution Variational Anal...\n    source:         MODIS_T-JPL, MODIS_A-JPL, AMSR2-REMSS, AVHRRMTA_G-NAVO, A...xarray.DataArray'analysed_sst'time: 1lat: 17999lon: 36000dask.array&lt;chunksize=(1, 1023, 2047), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.41 GiB\n7.99 MiB\n\n\nShape\n(1, 17999, 36000)\n(1, 1023, 2047)\n\n\nCount\n325 Tasks\n324 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\nCoordinates: (3)lat(lat)float32-89.99 -89.98 ... 89.98 89.99long_name :latitudestandard_name :latitudeaxis :Yunits :degrees_northvalid_min :-90.0valid_max :90.0comment :geolocations inherited from the input data without correctionarray([-89.99, -89.98, -89.97, ...,  89.97,  89.98,  89.99], dtype=float32)lon(lon)float32-180.0 -180.0 ... 180.0 180.0long_name :longitudestandard_name :longitudeaxis :Xunits :degrees_eastvalid_min :-180.0valid_max :180.0comment :geolocations inherited from the input data without correctionarray([-179.99, -179.98, -179.97, ...,  179.98,  179.99,  180.  ],\n      dtype=float32)time(time)datetime64[ns]2021-09-01T09:00:00long_name :reference time of sst fieldstandard_name :timeaxis :Tcomment :Nominal time of analyzed fieldsarray(['2021-09-01T09:00:00.000000000'], dtype='datetime64[ns]')Attributes: (7)long_name :analysed sea surface temperaturestandard_name :sea_surface_foundation_temperatureunits :kelvinvalid_min :-32767valid_max :32767comment :\\\"Final\\\" version using Multi-Resolution Variational Analysis (MRVA) method for interpolationsource :MODIS_T-JPL, MODIS_A-JPL, AMSR2-REMSS, AVHRRMTA_G-NAVO, AVHRRMTB_G-NAVO, iQUAM-NOAA/NESDIS, Ice_Conc-OSISAF\n\n\n\nsst = ds.analysed_sst.sel(lat=lats, lon=lons)\nsst\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'analysed_sst' (time: 1, lat: 801, lon: 1701)&gt;\ndask.array&lt;getitem, shape=(1, 801, 1701), dtype=float32, chunksize=(1, 601, 1536), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * lat      (lat) float32 41.0 41.01 41.02 41.03 ... 48.97 48.98 48.99 49.0\n  * lon      (lon) float32 -93.0 -92.99 -92.98 -92.97 ... -76.02 -76.01 -76.0\n  * time     (time) datetime64[ns] 2021-09-01T09:00:00\nAttributes:\n    long_name:      analysed sea surface temperature\n    standard_name:  sea_surface_foundation_temperature\n    units:          kelvin\n    valid_min:      -32767\n    valid_max:      32767\n    comment:        \\\"Final\\\" version using Multi-Resolution Variational Anal...\n    source:         MODIS_T-JPL, MODIS_A-JPL, AMSR2-REMSS, AVHRRMTA_G-NAVO, A...xarray.DataArray'analysed_sst'time: 1lat: 801lon: 1701dask.array&lt;chunksize=(1, 200, 1536), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n5.20 MiB\n3.52 MiB\n\n\nShape\n(1, 801, 1701)\n(1, 601, 1536)\n\n\nCount\n329 Tasks\n4 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\nCoordinates: (3)lat(lat)float3241.0 41.01 41.02 ... 48.99 49.0long_name :latitudestandard_name :latitudeaxis :Yunits :degrees_northvalid_min :-90.0valid_max :90.0comment :geolocations inherited from the input data without correctionarray([41.  , 41.01, 41.02, ..., 48.98, 48.99, 49.  ], dtype=float32)lon(lon)float32-93.0 -92.99 ... -76.01 -76.0long_name :longitudestandard_name :longitudeaxis :Xunits :degrees_eastvalid_min :-180.0valid_max :180.0comment :geolocations inherited from the input data without correctionarray([-93.  , -92.99, -92.98, ..., -76.02, -76.01, -76.  ], dtype=float32)time(time)datetime64[ns]2021-09-01T09:00:00long_name :reference time of sst fieldstandard_name :timeaxis :Tcomment :Nominal time of analyzed fieldsarray(['2021-09-01T09:00:00.000000000'], dtype='datetime64[ns]')Attributes: (7)long_name :analysed sea surface temperaturestandard_name :sea_surface_foundation_temperatureunits :kelvinvalid_min :-32767valid_max :32767comment :\\\"Final\\\" version using Multi-Resolution Variational Analysis (MRVA) method for interpolationsource :MODIS_T-JPL, MODIS_A-JPL, AMSR2-REMSS, AVHRRMTA_G-NAVO, AVHRRMTB_G-NAVO, iQUAM-NOAA/NESDIS, Ice_Conc-OSISAF\n\n\n\nsst.plot()\n\n&lt;matplotlib.collections.QuadMesh at 0x7f2d9848d4c0&gt;\n\n\n\n\n\n\n\n\nAggregate and analyze 30 files\nSet up a function to open all of our URLs as XArrays in parallel\n\ndef open_as_zarr_xarray(url):\n    return xr.open_zarr(EosdisStore(url), consolidated=False)\n\ndatasets = pqdm(urls, open_as_zarr_xarray, n_jobs=30)\n\n\n\n\n\n\n\n\n\n\nCombine the individual file-based datasets into a single xarray dataset with a time axis\n\nds = xr.concat(datasets, 'time')\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:           (time: 30, lat: 17999, lon: 36000)\nCoordinates:\n  * lat               (lat) float32 -89.99 -89.98 -89.97 ... 89.97 89.98 89.99\n  * lon               (lon) float32 -180.0 -180.0 -180.0 ... 180.0 180.0 180.0\n  * time              (time) datetime64[ns] 2021-09-01T09:00:00 ... 2021-09-3...\nData variables:\n    analysed_sst      (time, lat, lon) float32 dask.array&lt;chunksize=(1, 1023, 2047), meta=np.ndarray&gt;\n    analysis_error    (time, lat, lon) float32 dask.array&lt;chunksize=(1, 1023, 2047), meta=np.ndarray&gt;\n    dt_1km_data       (time, lat, lon) timedelta64[ns] dask.array&lt;chunksize=(1, 1447, 2895), meta=np.ndarray&gt;\n    mask              (time, lat, lon) float32 dask.array&lt;chunksize=(1, 1447, 2895), meta=np.ndarray&gt;\n    sea_ice_fraction  (time, lat, lon) float32 dask.array&lt;chunksize=(1, 1447, 2895), meta=np.ndarray&gt;\n    sst_anomaly       (time, lat, lon) float32 dask.array&lt;chunksize=(1, 1023, 2047), meta=np.ndarray&gt;\nAttributes: (12/47)\n    Conventions:                CF-1.7\n    title:                      Daily MUR SST, Final product\n    summary:                    A merged, multi-sensor L4 Foundation SST anal...\n    references:                 http://podaac.jpl.nasa.gov/Multi-scale_Ultra-...\n    institution:                Jet Propulsion Laboratory\n    history:                    created at nominal 4-day latency; replaced nr...\n    ...                         ...\n    project:                    NASA Making Earth Science Data Records for Us...\n    publisher_name:             GHRSST Project Office\n    publisher_url:              http://www.ghrsst.org\n    publisher_email:            ghrsst-po@nceo.ac.uk\n    processing_level:           L4\n    cdm_data_type:              gridxarray.DatasetDimensions:time: 30lat: 17999lon: 36000Coordinates: (3)lat(lat)float32-89.99 -89.98 ... 89.98 89.99long_name :latitudestandard_name :latitudeaxis :Yunits :degrees_northvalid_min :-90.0valid_max :90.0comment :geolocations inherited from the input data without correctionarray([-89.99, -89.98, -89.97, ...,  89.97,  89.98,  89.99], dtype=float32)lon(lon)float32-180.0 -180.0 ... 180.0 180.0long_name :longitudestandard_name :longitudeaxis :Xunits :degrees_eastvalid_min :-180.0valid_max :180.0comment :geolocations inherited from the input data without correctionarray([-179.99, -179.98, -179.97, ...,  179.98,  179.99,  180.  ],\n      dtype=float32)time(time)datetime64[ns]2021-09-01T09:00:00 ... 2021-09-...long_name :reference time of sst fieldstandard_name :timeaxis :Tcomment :Nominal time of analyzed fieldsarray(['2021-09-01T09:00:00.000000000', '2021-09-02T09:00:00.000000000',\n       '2021-09-03T09:00:00.000000000', '2021-09-04T09:00:00.000000000',\n       '2021-09-05T09:00:00.000000000', '2021-09-06T09:00:00.000000000',\n       '2021-09-07T09:00:00.000000000', '2021-09-08T09:00:00.000000000',\n       '2021-09-09T09:00:00.000000000', '2021-09-10T09:00:00.000000000',\n       '2021-09-11T09:00:00.000000000', '2021-09-12T09:00:00.000000000',\n       '2021-09-13T09:00:00.000000000', '2021-09-14T09:00:00.000000000',\n       '2021-09-15T09:00:00.000000000', '2021-09-16T09:00:00.000000000',\n       '2021-09-17T09:00:00.000000000', '2021-09-18T09:00:00.000000000',\n       '2021-09-19T09:00:00.000000000', '2021-09-20T09:00:00.000000000',\n       '2021-09-21T09:00:00.000000000', '2021-09-22T09:00:00.000000000',\n       '2021-09-23T09:00:00.000000000', '2021-09-24T09:00:00.000000000',\n       '2021-09-25T09:00:00.000000000', '2021-09-26T09:00:00.000000000',\n       '2021-09-27T09:00:00.000000000', '2021-09-28T09:00:00.000000000',\n       '2021-09-29T09:00:00.000000000', '2021-09-30T09:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (6)analysed_sst(time, lat, lon)float32dask.array&lt;chunksize=(1, 1023, 2047), meta=np.ndarray&gt;long_name :analysed sea surface temperaturestandard_name :sea_surface_foundation_temperatureunits :kelvinvalid_min :-32767valid_max :32767comment :\\\"Final\\\" version using Multi-Resolution Variational Analysis (MRVA) method for interpolationsource :MODIS_T-JPL, MODIS_A-JPL, AMSR2-REMSS, AVHRRMTA_G-NAVO, AVHRRMTB_G-NAVO, iQUAM-NOAA/NESDIS, Ice_Conc-OSISAF\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n72.42 GiB\n7.99 MiB\n\n\nShape\n(30, 17999, 36000)\n(1, 1023, 2047)\n\n\nCount\n19470 Tasks\n9720 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\n\nanalysis_error\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 1023, 2047), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nestimated error standard deviation of analysed_sst\n\nunits :\n\nkelvin\n\nvalid_min :\n\n0\n\nvalid_max :\n\n32767\n\ncomment :\n\nuncertainty in \\\"analysed_sst\\\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n72.42 GiB\n7.99 MiB\n\n\nShape\n(30, 17999, 36000)\n(1, 1023, 2047)\n\n\nCount\n19470 Tasks\n9720 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\n\n\ndt_1km_data\n\n\n(time, lat, lon)\n\n\ntimedelta64[ns]\n\n\ndask.array&lt;chunksize=(1, 1447, 2895), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\ntime to most recent 1km data\n\nvalid_min :\n\n-127\n\nvalid_max :\n\n127\n\nsource :\n\nMODIS and VIIRS pixels ingested by MUR\n\ncomment :\n\nThe grid value is hours between the analysis time and the most recent MODIS or VIIRS 1km L2P datum within 0.01 degrees from the grid point. \\\"Fill value\\\" indicates absence of such 1km data at the grid point.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n144.83 GiB\n31.96 MiB\n\n\nShape\n(30, 17999, 36000)\n(1, 1447, 2895)\n\n\nCount\n10170 Tasks\n5070 Chunks\n\n\nType\ntimedelta64[ns]\nnumpy.ndarray\n\n\n\n\n\n\n\n\n\nmask\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 1447, 2895), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nsea/land field composite mask\n\nvalid_min :\n\n1\n\nvalid_max :\n\n31\n\nflag_masks :\n\n[1, 2, 4, 8, 16]\n\nflag_meanings :\n\nopen_sea land open_lake open_sea_with_ice_in_the_grid open_lake_with_ice_in_the_grid\n\ncomment :\n\nmask can be used to further filter the data.\n\nsource :\n\nGMT \\\"grdlandmask\\\", ice flag from sea_ice_fraction data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n72.42 GiB\n15.98 MiB\n\n\nShape\n(30, 17999, 36000)\n(1, 1447, 2895)\n\n\nCount\n10170 Tasks\n5070 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\n\n\nsea_ice_fraction\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 1447, 2895), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nsea ice area fraction\n\nstandard_name :\n\nsea_ice_area_fraction\n\nvalid_min :\n\n0\n\nvalid_max :\n\n100\n\nsource :\n\nEUMETSAT OSI-SAF, copyright EUMETSAT\n\ncomment :\n\nice fraction is a dimensionless quantity between 0 and 1; it has been interpolated by a nearest neighbor approach.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n72.42 GiB\n15.98 MiB\n\n\nShape\n(30, 17999, 36000)\n(1, 1447, 2895)\n\n\nCount\n10170 Tasks\n5070 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\n\n\nsst_anomaly\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 1023, 2047), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nSST anomaly from a seasonal SST climatology based on the MUR data over 2003-2014 period\n\nunits :\n\nkelvin\n\nvalid_min :\n\n-32767\n\nvalid_max :\n\n32767\n\ncomment :\n\nanomaly reference to the day-of-year average between 2003 and 2014\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n72.42 GiB\n7.99 MiB\n\n\nShape\n(30, 17999, 36000)\n(1, 1023, 2047)\n\n\nCount\n19470 Tasks\n9720 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\n\nAttributes: (47)Conventions :CF-1.7title :Daily MUR SST, Final productsummary :A merged, multi-sensor L4 Foundation SST analysis product from JPL.references :http://podaac.jpl.nasa.gov/Multi-scale_Ultra-high_Resolution_MUR-SSTinstitution :Jet Propulsion Laboratoryhistory :created at nominal 4-day latency; replaced nrt (1-day latency) version.comment :MUR = \\\"Multi-scale Ultra-high Resolution\\\"license :These data are available free of charge under data policy of JPL PO.DAAC.id :MUR-JPL-L4-GLOB-v04.1naming_authority :org.ghrsstproduct_version :04.1uuid :27665bc0-d5fc-11e1-9b23-0800200c9a66gds_version_id :2.0netcdf_version_id :4.1date_created :20210910T072132Zstart_time :20210901T090000Zstop_time :20210901T090000Ztime_coverage_start :20210831T210000Ztime_coverage_end :20210901T210000Zfile_quality_level :3source :MODIS_T-JPL, MODIS_A-JPL, AMSR2-REMSS, AVHRRMTA_G-NAVO, AVHRRMTB_G-NAVO, iQUAM-NOAA/NESDIS, Ice_Conc-OSISAFplatform :Terra, Aqua, GCOM-W, MetOp-A, MetOp-B, Buoys/Shipssensor :MODIS, AMSR2, AVHRR, in-situMetadata_Conventions :Unidata Observation Dataset v1.0metadata_link :http://podaac.jpl.nasa.gov/ws/metadata/dataset/?format=iso&shortName=MUR-JPL-L4-GLOB-v04.1keywords :Oceans &gt; Ocean Temperature &gt; Sea Surface Temperaturekeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordsstandard_name_vocabulary :NetCDF Climate and Forecast (CF) Metadata Conventionsouthernmost_latitude :-90.0northernmost_latitude :90.0westernmost_longitude :-180.0easternmost_longitude :180.0spatial_resolution :0.01 degreesgeospatial_lat_units :degrees northgeospatial_lat_resolution :0.009999999776geospatial_lon_units :degrees eastgeospatial_lon_resolution :0.009999999776acknowledgment :Please acknowledge the use of these data with the following statement:  These data were provided by JPL under support by NASA MEaSUREs program.creator_name :JPL MUR SST projectcreator_email :ghrsst@podaac.jpl.nasa.govcreator_url :http://mur.jpl.nasa.govproject :NASA Making Earth Science Data Records for Use in Research Environments (MEaSUREs) Programpublisher_name :GHRSST Project Officepublisher_url :http://www.ghrsst.orgpublisher_email :ghrsst-po@nceo.ac.ukprocessing_level :L4cdm_data_type :grid\n\n\nLook at the Analysed SST variable metadata\n\nall_sst = ds.analysed_sst\nall_sst\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'analysed_sst' (time: 30, lat: 17999, lon: 36000)&gt;\ndask.array&lt;concatenate, shape=(30, 17999, 36000), dtype=float32, chunksize=(1, 1023, 2047), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * lat      (lat) float32 -89.99 -89.98 -89.97 -89.96 ... 89.97 89.98 89.99\n  * lon      (lon) float32 -180.0 -180.0 -180.0 -180.0 ... 180.0 180.0 180.0\n  * time     (time) datetime64[ns] 2021-09-01T09:00:00 ... 2021-09-30T09:00:00\nAttributes:\n    long_name:      analysed sea surface temperature\n    standard_name:  sea_surface_foundation_temperature\n    units:          kelvin\n    valid_min:      -32767\n    valid_max:      32767\n    comment:        \\\"Final\\\" version using Multi-Resolution Variational Anal...\n    source:         MODIS_T-JPL, MODIS_A-JPL, AMSR2-REMSS, AVHRRMTA_G-NAVO, A...xarray.DataArray'analysed_sst'time: 30lat: 17999lon: 36000dask.array&lt;chunksize=(1, 1023, 2047), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n72.42 GiB\n7.99 MiB\n\n\nShape\n(30, 17999, 36000)\n(1, 1023, 2047)\n\n\nCount\n19470 Tasks\n9720 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\nCoordinates: (3)lat(lat)float32-89.99 -89.98 ... 89.98 89.99long_name :latitudestandard_name :latitudeaxis :Yunits :degrees_northvalid_min :-90.0valid_max :90.0comment :geolocations inherited from the input data without correctionarray([-89.99, -89.98, -89.97, ...,  89.97,  89.98,  89.99], dtype=float32)lon(lon)float32-180.0 -180.0 ... 180.0 180.0long_name :longitudestandard_name :longitudeaxis :Xunits :degrees_eastvalid_min :-180.0valid_max :180.0comment :geolocations inherited from the input data without correctionarray([-179.99, -179.98, -179.97, ...,  179.98,  179.99,  180.  ],\n      dtype=float32)time(time)datetime64[ns]2021-09-01T09:00:00 ... 2021-09-...long_name :reference time of sst fieldstandard_name :timeaxis :Tcomment :Nominal time of analyzed fieldsarray(['2021-09-01T09:00:00.000000000', '2021-09-02T09:00:00.000000000',\n       '2021-09-03T09:00:00.000000000', '2021-09-04T09:00:00.000000000',\n       '2021-09-05T09:00:00.000000000', '2021-09-06T09:00:00.000000000',\n       '2021-09-07T09:00:00.000000000', '2021-09-08T09:00:00.000000000',\n       '2021-09-09T09:00:00.000000000', '2021-09-10T09:00:00.000000000',\n       '2021-09-11T09:00:00.000000000', '2021-09-12T09:00:00.000000000',\n       '2021-09-13T09:00:00.000000000', '2021-09-14T09:00:00.000000000',\n       '2021-09-15T09:00:00.000000000', '2021-09-16T09:00:00.000000000',\n       '2021-09-17T09:00:00.000000000', '2021-09-18T09:00:00.000000000',\n       '2021-09-19T09:00:00.000000000', '2021-09-20T09:00:00.000000000',\n       '2021-09-21T09:00:00.000000000', '2021-09-22T09:00:00.000000000',\n       '2021-09-23T09:00:00.000000000', '2021-09-24T09:00:00.000000000',\n       '2021-09-25T09:00:00.000000000', '2021-09-26T09:00:00.000000000',\n       '2021-09-27T09:00:00.000000000', '2021-09-28T09:00:00.000000000',\n       '2021-09-29T09:00:00.000000000', '2021-09-30T09:00:00.000000000'],\n      dtype='datetime64[ns]')Attributes: (7)long_name :analysed sea surface temperaturestandard_name :sea_surface_foundation_temperatureunits :kelvinvalid_min :-32767valid_max :32767comment :\\\"Final\\\" version using Multi-Resolution Variational Analysis (MRVA) method for interpolationsource :MODIS_T-JPL, MODIS_A-JPL, AMSR2-REMSS, AVHRRMTA_G-NAVO, AVHRRMTB_G-NAVO, iQUAM-NOAA/NESDIS, Ice_Conc-OSISAF\n\n\nCreate a dataset / variable that is only our area of interest and view its metadata\n\nsst = ds.analysed_sst.sel(lat=lats, lon=lons)\nsst\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'analysed_sst' (time: 30, lat: 801, lon: 1701)&gt;\ndask.array&lt;getitem, shape=(30, 801, 1701), dtype=float32, chunksize=(1, 601, 1536), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * lat      (lat) float32 41.0 41.01 41.02 41.03 ... 48.97 48.98 48.99 49.0\n  * lon      (lon) float32 -93.0 -92.99 -92.98 -92.97 ... -76.02 -76.01 -76.0\n  * time     (time) datetime64[ns] 2021-09-01T09:00:00 ... 2021-09-30T09:00:00\nAttributes:\n    long_name:      analysed sea surface temperature\n    standard_name:  sea_surface_foundation_temperature\n    units:          kelvin\n    valid_min:      -32767\n    valid_max:      32767\n    comment:        \\\"Final\\\" version using Multi-Resolution Variational Anal...\n    source:         MODIS_T-JPL, MODIS_A-JPL, AMSR2-REMSS, AVHRRMTA_G-NAVO, A...xarray.DataArray'analysed_sst'time: 30lat: 801lon: 1701dask.array&lt;chunksize=(1, 200, 1536), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n155.93 MiB\n3.52 MiB\n\n\nShape\n(30, 801, 1701)\n(1, 601, 1536)\n\n\nCount\n19590 Tasks\n120 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\nCoordinates: (3)lat(lat)float3241.0 41.01 41.02 ... 48.99 49.0long_name :latitudestandard_name :latitudeaxis :Yunits :degrees_northvalid_min :-90.0valid_max :90.0comment :geolocations inherited from the input data without correctionarray([41.  , 41.01, 41.02, ..., 48.98, 48.99, 49.  ], dtype=float32)lon(lon)float32-93.0 -92.99 ... -76.01 -76.0long_name :longitudestandard_name :longitudeaxis :Xunits :degrees_eastvalid_min :-180.0valid_max :180.0comment :geolocations inherited from the input data without correctionarray([-93.  , -92.99, -92.98, ..., -76.02, -76.01, -76.  ], dtype=float32)time(time)datetime64[ns]2021-09-01T09:00:00 ... 2021-09-...long_name :reference time of sst fieldstandard_name :timeaxis :Tcomment :Nominal time of analyzed fieldsarray(['2021-09-01T09:00:00.000000000', '2021-09-02T09:00:00.000000000',\n       '2021-09-03T09:00:00.000000000', '2021-09-04T09:00:00.000000000',\n       '2021-09-05T09:00:00.000000000', '2021-09-06T09:00:00.000000000',\n       '2021-09-07T09:00:00.000000000', '2021-09-08T09:00:00.000000000',\n       '2021-09-09T09:00:00.000000000', '2021-09-10T09:00:00.000000000',\n       '2021-09-11T09:00:00.000000000', '2021-09-12T09:00:00.000000000',\n       '2021-09-13T09:00:00.000000000', '2021-09-14T09:00:00.000000000',\n       '2021-09-15T09:00:00.000000000', '2021-09-16T09:00:00.000000000',\n       '2021-09-17T09:00:00.000000000', '2021-09-18T09:00:00.000000000',\n       '2021-09-19T09:00:00.000000000', '2021-09-20T09:00:00.000000000',\n       '2021-09-21T09:00:00.000000000', '2021-09-22T09:00:00.000000000',\n       '2021-09-23T09:00:00.000000000', '2021-09-24T09:00:00.000000000',\n       '2021-09-25T09:00:00.000000000', '2021-09-26T09:00:00.000000000',\n       '2021-09-27T09:00:00.000000000', '2021-09-28T09:00:00.000000000',\n       '2021-09-29T09:00:00.000000000', '2021-09-30T09:00:00.000000000'],\n      dtype='datetime64[ns]')Attributes: (7)long_name :analysed sea surface temperaturestandard_name :sea_surface_foundation_temperatureunits :kelvinvalid_min :-32767valid_max :32767comment :\\\"Final\\\" version using Multi-Resolution Variational Analysis (MRVA) method for interpolationsource :MODIS_T-JPL, MODIS_A-JPL, AMSR2-REMSS, AVHRRMTA_G-NAVO, AVHRRMTB_G-NAVO, iQUAM-NOAA/NESDIS, Ice_Conc-OSISAF\n\n\nXArray reads data lazily, i.e.¬†only when our code actually needs it. Up to this point, we haven‚Äôt read any data values, only metadata. The next line will force XArray to read the portions of the source files containing our area of interest. Behind the scenes, the eosdis-zarr-store library is ensuring data is fetched as efficiently as possible.\nNote: This line isn‚Äôt strictly necessary, since XArray will automatically read the data we need the first time our code tries to use it, but calling this will make sure that we can read the data multiple times later on without re-fetching anything from the source files.\nThis line will take several seconds to complete, but since it is retrieving only about 50 MB of data from 22 GB of source files, several seconds constitutes a significant time, bandwidth, and disk space savings.\n\nsst.load();\n\nNow we can start looking at aggregations across the time dimension. In this case, plot the standard deviation of the temperature at each point to get a visual sense of how much temperatures fluctuate over the course of the month.\n\n# We expect a warning here, from finding the standard deviation of arrays that contain all N/A values.\n# numpy produces N/A for these points, though, which is exactly what we want.\nstdev_sst = sst.std('time')\nstdev_sst.name = 'stdev of analysed_sst [Kelvin]'\nstdev_sst.plot();\n\n/srv/conda/envs/notebook/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1670: RuntimeWarning: Degrees of freedom &lt;= 0 for slice.\n  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n\n\n\n\n\n\nInteractive animation of a month of data\nThis section isn‚Äôt as important to fully understand. It shows us a way to get an interactive animation to see what we have retrieved so far\nDefine an animation function to plot the ith time step. We need to make sure each plot is using the same color scale, set by vmin and vmax so the animation is consistent\n\nsst_min = sst.min()\nsst_max = sst.max()\n\ndef show_time_step(i):\n    plt.clf()\n    res = sst[i].plot.imshow(vmin=sst_min, vmax=sst_max)\n    return (res,)\n\nRender each time slice once and show it as an HTML animation with interactive controls\n\n#anim = animation.FuncAnimation(plt.gcf(), func=show_time_step, frames=len(sst))\n#display(HTML(anim.to_jshtml()))\n#plt.close()\n\n\n\n\nSupplemental: What‚Äôs happening here?\nFor EOSDIS data in the cloud, we have begun producing a metadata sidecar file in a format called DMR++ that extracts all of the information about arrays, variables, and dimensions from data files, as well as the byte offsets in the NetCDF4 file where data can be found. This information is sufficient to let the Zarr library read data from our NetCDF4 files, but it‚Äôs in the wrong format. zarr-eosdis-store knows how to fetch the sidecar file and transform it into something the Zarr library understands. Passing it when reading Zarr using XArray or the Zarr library lets these libraries interact with EOSDIS data exactly as if they were Zarr stores in a way that‚Äôs more optimal for reading data in the cloud. Beyond this, the zarr-eosdis-store library makes some optimizations in the way it reads data to help make up for situations where the NetCDF4 file is not internally arranged well for cloud-based access patterns."
  },
  {
    "objectID": "python-notebooks/04_Appending_Data.html#load-in-data",
    "href": "python-notebooks/04_Appending_Data.html#load-in-data",
    "title": "Adding new data",
    "section": "Load in data",
    "text": "Load in data\n\nog_ds = xr.open_zarr('demonstrated data/final-sample-appending.zarr/')\nnew_ds = xr.open_zarr('demonstrated data/new-data-sample.zarr/')\n\nNote that our new dataset does not have any metadata. As shown in the previous notebooks, metadata is added at the last step, so now we are going to copy all metadata from the original dataset to our new one."
  },
  {
    "objectID": "python-notebooks/04_Appending_Data.html#add-metadata",
    "href": "python-notebooks/04_Appending_Data.html#add-metadata",
    "title": "Adding new data",
    "section": "Add metadata",
    "text": "Add metadata\n\n# copy dataset metadata\nnew_ds.attrs = og_ds.attrs\n\n# copy variables/dimensions metadata\n# make sure that all vars in new_ds exist in og_ds\nfor var in new_ds.variables:\n    new_ds[var].attrs = og_ds[var].attrs\n\n\n# double-check\nnew_ds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:          (time: 2556, lat: 81, lon: 81)\nCoordinates:\n  * lat              (lat) float32 25.0 24.75 24.5 24.25 ... 5.75 5.5 5.25 5.0\n  * lon              (lon) float32 60.0 60.25 60.5 60.75 ... 79.5 79.75 80.0\n  * time             (time) datetime64[ns] 1993-01-01 1993-01-02 ... 1999-12-31\nData variables: (12/14)\n    CHL              (time, lat, lon) float32 dask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n    CHL_uncertainty  (time, lat, lon) float32 dask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n    adt              (time, lat, lon) float32 dask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n    air_temp         (time, lat, lon) float32 dask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n    direction        (time, lat, lon) float32 dask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n    sla              (time, lat, lon) float32 dask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n    ...               ...\n    u_curr           (time, lat, lon) float32 dask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n    u_wind           (time, lat, lon) float32 dask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n    ug_curr          (time, lat, lon) float32 dask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n    v_curr           (time, lat, lon) float32 dask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n    v_wind           (time, lat, lon) float32 dask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n    vg_curr          (time, lat, lon) float32 dask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\nAttributes: (12/17)\n    creator_email:              minhphan@uw.edu\n    creator_name:               Minh Phan\n    creator_type:               person\n    date_created:               2023-11-11\n    geospatial_lat_max:         25.0\n    geospatial_lat_min:         5.0\n    ...                         ...\n    geospatial_lon_units:       degrees_east\n    source:                     OSCAR, ERA5 Reanalysis, Copernicus Climate Ch...\n    summary:                    Daily mean of 0.25 x 0.25 degrees gridded dat...\n    time_coverage_end:          2002-12-31T23:59:59\n    time_coverage_start:        2000-01-01T00:00:00\n    title:                      Sample of Climate Data for Coastal Upwelling ...xarray.DatasetDimensions:time: 2556lat: 81lon: 81Coordinates: (3)lat(lat)float3225.0 24.75 24.5 ... 5.5 5.25 5.0long_name :latitudestandard_name :latitudeunits :degrees_northarray([25.  , 24.75, 24.5 , 24.25, 24.  , 23.75, 23.5 , 23.25, 23.  , 22.75,\n       22.5 , 22.25, 22.  , 21.75, 21.5 , 21.25, 21.  , 20.75, 20.5 , 20.25,\n       20.  , 19.75, 19.5 , 19.25, 19.  , 18.75, 18.5 , 18.25, 18.  , 17.75,\n       17.5 , 17.25, 17.  , 16.75, 16.5 , 16.25, 16.  , 15.75, 15.5 , 15.25,\n       15.  , 14.75, 14.5 , 14.25, 14.  , 13.75, 13.5 , 13.25, 13.  , 12.75,\n       12.5 , 12.25, 12.  , 11.75, 11.5 , 11.25, 11.  , 10.75, 10.5 , 10.25,\n       10.  ,  9.75,  9.5 ,  9.25,  9.  ,  8.75,  8.5 ,  8.25,  8.  ,  7.75,\n        7.5 ,  7.25,  7.  ,  6.75,  6.5 ,  6.25,  6.  ,  5.75,  5.5 ,  5.25,\n        5.  ], dtype=float32)lon(lon)float3260.0 60.25 60.5 ... 79.5 79.75 80.0long_name :longitudestandard_name :longitudeunits :degrees_eastarray([60.  , 60.25, 60.5 , 60.75, 61.  , 61.25, 61.5 , 61.75, 62.  , 62.25,\n       62.5 , 62.75, 63.  , 63.25, 63.5 , 63.75, 64.  , 64.25, 64.5 , 64.75,\n       65.  , 65.25, 65.5 , 65.75, 66.  , 66.25, 66.5 , 66.75, 67.  , 67.25,\n       67.5 , 67.75, 68.  , 68.25, 68.5 , 68.75, 69.  , 69.25, 69.5 , 69.75,\n       70.  , 70.25, 70.5 , 70.75, 71.  , 71.25, 71.5 , 71.75, 72.  , 72.25,\n       72.5 , 72.75, 73.  , 73.25, 73.5 , 73.75, 74.  , 74.25, 74.5 , 74.75,\n       75.  , 75.25, 75.5 , 75.75, 76.  , 76.25, 76.5 , 76.75, 77.  , 77.25,\n       77.5 , 77.75, 78.  , 78.25, 78.5 , 78.75, 79.  , 79.25, 79.5 , 79.75,\n       80.  ], dtype=float32)time(time)datetime64[ns]1993-01-01 ... 1999-12-31_ChunkSizes :1_CoordinateAxisType :Timeaxis :Tcoverage_content_type :coordinatelong_name :reference timestandard_name :timevalid_max :401767.0valid_min :0.0array(['1993-01-01T00:00:00.000000000', '1993-01-02T00:00:00.000000000',\n       '1993-01-03T00:00:00.000000000', ..., '1999-12-29T00:00:00.000000000',\n       '1999-12-30T00:00:00.000000000', '1999-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (14)CHL(time, lat, lon)float32dask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;_ChunkSizes :[1, 256, 256]ancillary_variables :flags CHL_uncertaintycoverage_content_type :modelResultinput_files_reprocessings :Processors versions: MODIS R2022.0NRT/VIIRSN R2022.0NRT/OLCIA 07.02/VIIRSJ1 R2022.0NRT/OLCIB 07.02long_name :Chlorophyll-a concentration - Mean of the binned pixelsstandard_name :mass_concentration_of_chlorophyll_a_in_sea_watertype :surfaceunits :milligram m-3valid_max :1000.0valid_min :0.0\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n63.97 MiB\n2.50 MiB\n\n\nShape\n(2556, 81, 81)\n(100, 81, 81)\n\n\nDask graph\n26 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nCHL_uncertainty\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n\n\n\n\n_ChunkSizes :\n\n[1, 256, 256]\n\ncoverage_content_type :\n\nqualityInformation\n\nlong_name :\n\nChlorophyll-a concentration - Uncertainty estimation\n\nunits :\n\n%\n\nvalid_max :\n\n32767\n\nvalid_min :\n\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n63.97 MiB\n2.50 MiB\n\n\nShape\n(2556, 81, 81)\n(100, 81, 81)\n\n\nDask graph\n26 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nadt\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n\n\n\n\n_ChunkSizes :\n\n[1, 50, 50]\n\ncomment :\n\nThe absolute dynamic topography is the sea surface height above geoid; the adt is obtained as follows: adt=sla+mdt where mdt is the mean dynamic topography; see the product user manual for details\n\ngrid_mapping :\n\ncrs\n\nlong_name :\n\nAbsolute dynamic topography\n\nstandard_name :\n\nsea_surface_height_above_geoid\n\nunits :\n\nm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n63.97 MiB\n2.50 MiB\n\n\nShape\n(2556, 81, 81)\n(100, 81, 81)\n\n\nDask graph\n26 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nair_temp\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n2 metre temperature\n\nnameCDM :\n\n2_metre_temperature_surface\n\nnameECMWF :\n\n2 metre temperature\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n2t\n\nstandard_name :\n\nair_temperature\n\nunits :\n\nK\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n63.97 MiB\n2.50 MiB\n\n\nShape\n(2556, 81, 81)\n(100, 81, 81)\n\n\nDask graph\n26 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\ndirection\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre wind direction\n\nunits :\n\n¬∞C\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n63.97 MiB\n2.50 MiB\n\n\nShape\n(2556, 81, 81)\n(100, 81, 81)\n\n\nDask graph\n26 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nsla\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n\n\n\n\n_ChunkSizes :\n\n[1, 50, 50]\n\nancillary_variables :\n\nerr_sla\n\ncomment :\n\nThe sea level anomaly is the sea surface height above mean sea surface; it is referenced to the [1993, 2012] period; see the product user manual for details\n\ngrid_mapping :\n\ncrs\n\nlong_name :\n\nSea level anomaly\n\nstandard_name :\n\nsea_surface_height_above_sea_level\n\nunits :\n\nm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n63.97 MiB\n2.50 MiB\n\n\nShape\n(2556, 81, 81)\n(100, 81, 81)\n\n\nDask graph\n26 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nspeed\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre absolute speed\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n63.97 MiB\n2.50 MiB\n\n\nShape\n(2556, 81, 81)\n(100, 81, 81)\n\n\nDask graph\n26 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nsst\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nSea surface temperature\n\nnameCDM :\n\nSea_surface_temperature_surface\n\nnameECMWF :\n\nSea surface temperature\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\nsst\n\nstandard_name :\n\nsea_surface_temperature\n\nunits :\n\nK\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n63.97 MiB\n2.50 MiB\n\n\nShape\n(2556, 81, 81)\n(100, 81, 81)\n\n\nDask graph\n26 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nu_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nVelocities are an average over the top 30m of the mixed layer\n\ncoverage_content_type :\n\nmodelResult\n\ndepth :\n\n15m\n\nlong_name :\n\nzonal total surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148 ; WIND source: ECMWF ERA5 10m wind DOI: 10.24381/cds.adbb2d47 ; SST source: CMC 0.2 deg SST V2.0 DOI: 10.5067/GHCMC-4FM02\n\nstandard_name :\n\neastward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n63.97 MiB\n2.50 MiB\n\n\nShape\n(2556, 81, 81)\n(100, 81, 81)\n\n\nDask graph\n26 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nu_wind\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre U wind component\n\nnameCDM :\n\n10_metre_U_wind_component_surface\n\nnameECMWF :\n\n10 metre U wind component\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n10u\n\nstandard_name :\n\neastward_wind\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n63.97 MiB\n2.50 MiB\n\n\nShape\n(2556, 81, 81)\n(100, 81, 81)\n\n\nDask graph\n26 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nug_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nGeostrophic velocities calculated from absolute dynamic topography\n\ndepth :\n\n15m\n\nlong_name :\n\nzonal geostrophic surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148\n\nstandard_name :\n\ngeostrophic_eastward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n63.97 MiB\n2.50 MiB\n\n\nShape\n(2556, 81, 81)\n(100, 81, 81)\n\n\nDask graph\n26 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nv_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nVelocities are an average over the top 30m of the mixed layer\n\ncoverage_content_type :\n\nmodelResult\n\ndepth :\n\n15m\n\nlong_name :\n\nmeridional total surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148 ; WIND source: ECMWF ERA5 10m wind DOI: 10.24381/cds.adbb2d47 ; SST source: CMC 0.2 deg SST V2.0 DOI: 10.5067/GHCMC-4FM02\n\nstandard_name :\n\nnorthward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n63.97 MiB\n2.50 MiB\n\n\nShape\n(2556, 81, 81)\n(100, 81, 81)\n\n\nDask graph\n26 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nv_wind\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre V wind component\n\nnameCDM :\n\n10_metre_V_wind_component_surface\n\nnameECMWF :\n\n10 metre V wind component\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n10v\n\nstandard_name :\n\nnorthward_wind\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n63.97 MiB\n2.50 MiB\n\n\nShape\n(2556, 81, 81)\n(100, 81, 81)\n\n\nDask graph\n26 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nvg_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nGeostrophic velocities calculated from absolute dynamic topography\n\ndepth :\n\n15m\n\nlong_name :\n\nmeridional geostrophic surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148\n\nstandard_name :\n\ngeostrophic_northward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n63.97 MiB\n2.50 MiB\n\n\nShape\n(2556, 81, 81)\n(100, 81, 81)\n\n\nDask graph\n26 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nAttributes: (17)creator_email :minhphan@uw.educreator_name :Minh Phancreator_type :persondate_created :2023-11-11geospatial_lat_max :25.0geospatial_lat_min :5.0geospatial_lat_resolution :0.25geospatial_lat_units :degrees_northgeospatial_lon_max :80.0geospatial_lon_min :60.0geospatial_lon_resolution :0.25geospatial_lon_units :degrees_eastsource :OSCAR, ERA5 Reanalysis, Copernicus Climate Change Service (C3S), Copernicus Marine Environment Monitoring Service (CMEMS)summary :Daily mean of 0.25 x 0.25 degrees gridded data from multiple climate variables that may influence the patterns of coastal upwelling in the focused areatime_coverage_end :2002-12-31T23:59:59time_coverage_start :2000-01-01T00:00:00title :Sample of Climate Data for Coastal Upwelling Machine Learning Project in Indian Ocean"
  },
  {
    "objectID": "python-notebooks/04_Appending_Data.html#appending-data",
    "href": "python-notebooks/04_Appending_Data.html#appending-data",
    "title": "Adding new data",
    "section": "Appending data",
    "text": "Appending data\n\nnew_ds.to_zarr('demonstrated data/final-sample-appending.zarr/', append_dim='time', mode='a')\n\n&lt;xarray.backends.zarr.ZarrStore at 0x7f1455c51dd0&gt;"
  },
  {
    "objectID": "python-notebooks/04_Appending_Data.html#final-result",
    "href": "python-notebooks/04_Appending_Data.html#final-result",
    "title": "Adding new data",
    "section": "Final result",
    "text": "Final result\n\nxr.open_zarr('demonstrated data/final-sample-appending.zarr/')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:          (time: 3287, lat: 81, lon: 81)\nCoordinates:\n  * lat              (lat) float32 25.0 24.75 24.5 24.25 ... 5.75 5.5 5.25 5.0\n  * lon              (lon) float32 60.0 60.25 60.5 60.75 ... 79.5 79.75 80.0\n  * time             (time) datetime64[ns] 2000-01-01 2000-01-02 ... 1999-12-31\nData variables: (12/14)\n    CHL              (time, lat, lon) float32 dask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n    CHL_uncertainty  (time, lat, lon) float32 dask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n    adt              (time, lat, lon) float32 dask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n    air_temp         (time, lat, lon) float32 dask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n    direction        (time, lat, lon) float32 dask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n    sla              (time, lat, lon) float32 dask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n    ...               ...\n    u_curr           (time, lat, lon) float32 dask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n    u_wind           (time, lat, lon) float32 dask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n    ug_curr          (time, lat, lon) float32 dask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n    v_curr           (time, lat, lon) float32 dask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n    v_wind           (time, lat, lon) float32 dask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n    vg_curr          (time, lat, lon) float32 dask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\nAttributes: (12/17)\n    creator_email:              minhphan@uw.edu\n    creator_name:               Minh Phan\n    creator_type:               person\n    date_created:               2023-11-11\n    geospatial_lat_max:         25.0\n    geospatial_lat_min:         5.0\n    ...                         ...\n    geospatial_lon_units:       degrees_east\n    source:                     OSCAR, ERA5 Reanalysis, Copernicus Climate Ch...\n    summary:                    Daily mean of 0.25 x 0.25 degrees gridded dat...\n    time_coverage_end:          2002-12-31T23:59:59\n    time_coverage_start:        2000-01-01T00:00:00\n    title:                      Sample of Climate Data for Coastal Upwelling ...xarray.DatasetDimensions:time: 3287lat: 81lon: 81Coordinates: (3)lat(lat)float3225.0 24.75 24.5 ... 5.5 5.25 5.0long_name :latitudestandard_name :latitudeunits :degrees_northarray([25.  , 24.75, 24.5 , 24.25, 24.  , 23.75, 23.5 , 23.25, 23.  , 22.75,\n       22.5 , 22.25, 22.  , 21.75, 21.5 , 21.25, 21.  , 20.75, 20.5 , 20.25,\n       20.  , 19.75, 19.5 , 19.25, 19.  , 18.75, 18.5 , 18.25, 18.  , 17.75,\n       17.5 , 17.25, 17.  , 16.75, 16.5 , 16.25, 16.  , 15.75, 15.5 , 15.25,\n       15.  , 14.75, 14.5 , 14.25, 14.  , 13.75, 13.5 , 13.25, 13.  , 12.75,\n       12.5 , 12.25, 12.  , 11.75, 11.5 , 11.25, 11.  , 10.75, 10.5 , 10.25,\n       10.  ,  9.75,  9.5 ,  9.25,  9.  ,  8.75,  8.5 ,  8.25,  8.  ,  7.75,\n        7.5 ,  7.25,  7.  ,  6.75,  6.5 ,  6.25,  6.  ,  5.75,  5.5 ,  5.25,\n        5.  ], dtype=float32)lon(lon)float3260.0 60.25 60.5 ... 79.5 79.75 80.0long_name :longitudestandard_name :longitudeunits :degrees_eastarray([60.  , 60.25, 60.5 , 60.75, 61.  , 61.25, 61.5 , 61.75, 62.  , 62.25,\n       62.5 , 62.75, 63.  , 63.25, 63.5 , 63.75, 64.  , 64.25, 64.5 , 64.75,\n       65.  , 65.25, 65.5 , 65.75, 66.  , 66.25, 66.5 , 66.75, 67.  , 67.25,\n       67.5 , 67.75, 68.  , 68.25, 68.5 , 68.75, 69.  , 69.25, 69.5 , 69.75,\n       70.  , 70.25, 70.5 , 70.75, 71.  , 71.25, 71.5 , 71.75, 72.  , 72.25,\n       72.5 , 72.75, 73.  , 73.25, 73.5 , 73.75, 74.  , 74.25, 74.5 , 74.75,\n       75.  , 75.25, 75.5 , 75.75, 76.  , 76.25, 76.5 , 76.75, 77.  , 77.25,\n       77.5 , 77.75, 78.  , 78.25, 78.5 , 78.75, 79.  , 79.25, 79.5 , 79.75,\n       80.  ], dtype=float32)time(time)datetime64[ns]2000-01-01 ... 1999-12-31_ChunkSizes :1_CoordinateAxisType :Timeaxis :Tcoverage_content_type :coordinatelong_name :reference timestandard_name :timevalid_max :401767.0valid_min :0.0array(['2000-01-01T00:00:00.000000000', '2000-01-02T00:00:00.000000000',\n       '2000-01-03T00:00:00.000000000', ..., '1999-12-29T00:00:00.000000000',\n       '1999-12-30T00:00:00.000000000', '1999-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (14)CHL(time, lat, lon)float32dask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;_ChunkSizes :[1, 256, 256]ancillary_variables :flags CHL_uncertaintycoverage_content_type :modelResultinput_files_reprocessings :Processors versions: MODIS R2022.0NRT/VIIRSN R2022.0NRT/OLCIA 07.02/VIIRSJ1 R2022.0NRT/OLCIB 07.02long_name :Chlorophyll-a concentration - Mean of the binned pixelsstandard_name :mass_concentration_of_chlorophyll_a_in_sea_watertype :surfaceunits :milligram m-3valid_max :1000.0valid_min :0.0\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n82.27 MiB\n2.50 MiB\n\n\nShape\n(3287, 81, 81)\n(100, 81, 81)\n\n\nDask graph\n33 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nCHL_uncertainty\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n\n\n\n\n_ChunkSizes :\n\n[1, 256, 256]\n\ncoverage_content_type :\n\nqualityInformation\n\nlong_name :\n\nChlorophyll-a concentration - Uncertainty estimation\n\nunits :\n\n%\n\nvalid_max :\n\n32767\n\nvalid_min :\n\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n82.27 MiB\n2.50 MiB\n\n\nShape\n(3287, 81, 81)\n(100, 81, 81)\n\n\nDask graph\n33 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nadt\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n\n\n\n\n_ChunkSizes :\n\n[1, 50, 50]\n\ncomment :\n\nThe absolute dynamic topography is the sea surface height above geoid; the adt is obtained as follows: adt=sla+mdt where mdt is the mean dynamic topography; see the product user manual for details\n\ngrid_mapping :\n\ncrs\n\nlong_name :\n\nAbsolute dynamic topography\n\nstandard_name :\n\nsea_surface_height_above_geoid\n\nunits :\n\nm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n82.27 MiB\n2.50 MiB\n\n\nShape\n(3287, 81, 81)\n(100, 81, 81)\n\n\nDask graph\n33 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nair_temp\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n2 metre temperature\n\nnameCDM :\n\n2_metre_temperature_surface\n\nnameECMWF :\n\n2 metre temperature\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n2t\n\nstandard_name :\n\nair_temperature\n\nunits :\n\nK\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n82.27 MiB\n2.50 MiB\n\n\nShape\n(3287, 81, 81)\n(100, 81, 81)\n\n\nDask graph\n33 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\ndirection\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre wind direction\n\nunits :\n\n¬∞C\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n82.27 MiB\n2.50 MiB\n\n\nShape\n(3287, 81, 81)\n(100, 81, 81)\n\n\nDask graph\n33 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nsla\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n\n\n\n\n_ChunkSizes :\n\n[1, 50, 50]\n\nancillary_variables :\n\nerr_sla\n\ncomment :\n\nThe sea level anomaly is the sea surface height above mean sea surface; it is referenced to the [1993, 2012] period; see the product user manual for details\n\ngrid_mapping :\n\ncrs\n\nlong_name :\n\nSea level anomaly\n\nstandard_name :\n\nsea_surface_height_above_sea_level\n\nunits :\n\nm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n82.27 MiB\n2.50 MiB\n\n\nShape\n(3287, 81, 81)\n(100, 81, 81)\n\n\nDask graph\n33 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nspeed\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre absolute speed\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n82.27 MiB\n2.50 MiB\n\n\nShape\n(3287, 81, 81)\n(100, 81, 81)\n\n\nDask graph\n33 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nsst\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nSea surface temperature\n\nnameCDM :\n\nSea_surface_temperature_surface\n\nnameECMWF :\n\nSea surface temperature\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\nsst\n\nstandard_name :\n\nsea_surface_temperature\n\nunits :\n\nK\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n82.27 MiB\n2.50 MiB\n\n\nShape\n(3287, 81, 81)\n(100, 81, 81)\n\n\nDask graph\n33 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nu_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nVelocities are an average over the top 30m of the mixed layer\n\ncoverage_content_type :\n\nmodelResult\n\ndepth :\n\n15m\n\nlong_name :\n\nzonal total surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148 ; WIND source: ECMWF ERA5 10m wind DOI: 10.24381/cds.adbb2d47 ; SST source: CMC 0.2 deg SST V2.0 DOI: 10.5067/GHCMC-4FM02\n\nstandard_name :\n\neastward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n82.27 MiB\n2.50 MiB\n\n\nShape\n(3287, 81, 81)\n(100, 81, 81)\n\n\nDask graph\n33 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nu_wind\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre U wind component\n\nnameCDM :\n\n10_metre_U_wind_component_surface\n\nnameECMWF :\n\n10 metre U wind component\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n10u\n\nstandard_name :\n\neastward_wind\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n82.27 MiB\n2.50 MiB\n\n\nShape\n(3287, 81, 81)\n(100, 81, 81)\n\n\nDask graph\n33 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nug_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nGeostrophic velocities calculated from absolute dynamic topography\n\ndepth :\n\n15m\n\nlong_name :\n\nzonal geostrophic surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148\n\nstandard_name :\n\ngeostrophic_eastward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n82.27 MiB\n2.50 MiB\n\n\nShape\n(3287, 81, 81)\n(100, 81, 81)\n\n\nDask graph\n33 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nv_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nVelocities are an average over the top 30m of the mixed layer\n\ncoverage_content_type :\n\nmodelResult\n\ndepth :\n\n15m\n\nlong_name :\n\nmeridional total surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148 ; WIND source: ECMWF ERA5 10m wind DOI: 10.24381/cds.adbb2d47 ; SST source: CMC 0.2 deg SST V2.0 DOI: 10.5067/GHCMC-4FM02\n\nstandard_name :\n\nnorthward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n82.27 MiB\n2.50 MiB\n\n\nShape\n(3287, 81, 81)\n(100, 81, 81)\n\n\nDask graph\n33 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nv_wind\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\n10 metre V wind component\n\nnameCDM :\n\n10_metre_V_wind_component_surface\n\nnameECMWF :\n\n10 metre V wind component\n\nproduct_type :\n\nanalysis\n\nshortNameECMWF :\n\n10v\n\nstandard_name :\n\nnorthward_wind\n\nunits :\n\nm s**-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n82.27 MiB\n2.50 MiB\n\n\nShape\n(3287, 81, 81)\n(100, 81, 81)\n\n\nDask graph\n33 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nvg_curr\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 81, 81), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nGeostrophic velocities calculated from absolute dynamic topography\n\ndepth :\n\n15m\n\nlong_name :\n\nmeridional geostrophic surface current\n\nsource :\n\nSSH source: CMEMS SSALTO/DUACS SEALEVEL_GLO_PHY_L4_MY_008_047 DOI: 10.48670/moi-00148\n\nstandard_name :\n\ngeostrophic_northward_sea_water_velocity\n\nunits :\n\nm s-1\n\nvalid_max :\n\n3.0\n\nvalid_min :\n\n-3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n82.27 MiB\n2.50 MiB\n\n\nShape\n(3287, 81, 81)\n(100, 81, 81)\n\n\nDask graph\n33 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nAttributes: (17)creator_email :minhphan@uw.educreator_name :Minh Phancreator_type :persondate_created :2023-11-11geospatial_lat_max :25.0geospatial_lat_min :5.0geospatial_lat_resolution :0.25geospatial_lat_units :degrees_northgeospatial_lon_max :80.0geospatial_lon_min :60.0geospatial_lon_resolution :0.25geospatial_lon_units :degrees_eastsource :OSCAR, ERA5 Reanalysis, Copernicus Climate Change Service (C3S), Copernicus Marine Environment Monitoring Service (CMEMS)summary :Daily mean of 0.25 x 0.25 degrees gridded data from multiple climate variables that may influence the patterns of coastal upwelling in the focused areatime_coverage_end :2002-12-31T23:59:59time_coverage_start :2000-01-01T00:00:00title :Sample of Climate Data for Coastal Upwelling Machine Learning Project in Indian Ocean"
  },
  {
    "objectID": "python-notebooks/04_Combining_Data.html#loading-data",
    "href": "python-notebooks/04_Combining_Data.html#loading-data",
    "title": "Combine, clean, and export the data",
    "section": "Loading data",
    "text": "Loading data\nBegin by loading all the data into disk\n\nimport xarray as xr\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nimport matplotlib.pyplot as plt\n\n\n# era5 = xr.combine_by_coords([xr.open_mfdataset('demonstrated data/era5/eastward_wind_at_10_metres/*.nc'),\n#                              xr.open_mfdataset('demonstrated data/era5/northward_wind_at_10_metres/*.nc')])\n\nera5 = xr.open_mfdataset('demonstrated data/era5/*/*.nc')\nsalinity = xr.open_dataset('demonstrated data/salinity_at_0_49m.nc')\nsst = xr.open_mfdataset('demonstrated data/earth_data/*/*.nc')['analysed_sst'] # we only have one collection in earth_data directory currently"
  },
  {
    "objectID": "python-notebooks/04_Combining_Data.html#process-era5-data-and-calculate-speeddirection",
    "href": "python-notebooks/04_Combining_Data.html#process-era5-data-and-calculate-speeddirection",
    "title": "Combine, clean, and export the data",
    "section": "Process ERA5 data and calculate speed/direction",
    "text": "Process ERA5 data and calculate speed/direction\n\n# adding calculated variables (direction and speed)\nera5 = era5.assign(speed = np.sqrt(era5.eastward_wind_at_10_metres**2 + era5.northward_wind_at_10_metres**2))\nera5 = era5.assign(direction = np.rad2deg(np.arctan2(era5.northward_wind_at_10_metres, era5.eastward_wind_at_10_metres)))\n\n# coarsen ()\nera5 = era5.coarsen(time0=24).mean(keep_attrs=True).compute()\n\n\nera5 = era5.rename({'time0': 'time', \n                    'eastward_wind_at_10_metres': 'u_wind', \n                    'northward_wind_at_10_metres': 'v_wind'})\n\n\n# fix time coordinate by resetting to 12AM\nera5 = era5.assign_coords(time=np.arange('2003-01-01', '2003-03-01', timedelta(days=1), dtype='datetime64[ns]')) # again, right-exclusive\n\n\nera5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:    (time: 59, lat: 81, lon: 81)\nCoordinates:\n  * lon        (lon) float32 60.0 60.25 60.5 60.75 ... 79.25 79.5 79.75 80.0\n  * lat        (lat) float32 25.0 24.75 24.5 24.25 24.0 ... 5.75 5.5 5.25 5.0\n  * time       (time) datetime64[ns] 2003-01-01 2003-01-02 ... 2003-02-28\nData variables:\n    u_wind     (time, lat, lon) float32 3.128 2.865 2.555 ... -3.737 -4.479\n    v_wind     (time, lat, lon) float32 0.6016 0.6589 0.6068 ... -1.914 -2.474\n    speed      (time, lat, lon) float32 3.945 3.645 3.368 ... 3.216 4.227 5.153\n    direction  (time, lat, lon) float32 22.31 17.58 13.01 ... -153.4 -151.6\nAttributes:\n    source:       Reanalysis\n    institution:  ECMWF\n    tilte:        ERA5 forecastsxarray.DatasetDimensions:time: 59lat: 81lon: 81Coordinates: (3)lon(lon)float3260.0 60.25 60.5 ... 79.5 79.75 80.0standard_name :longitudelong_name :longitudeunits :degrees_eastarray([60.  , 60.25, 60.5 , 60.75, 61.  , 61.25, 61.5 , 61.75, 62.  , 62.25,\n       62.5 , 62.75, 63.  , 63.25, 63.5 , 63.75, 64.  , 64.25, 64.5 , 64.75,\n       65.  , 65.25, 65.5 , 65.75, 66.  , 66.25, 66.5 , 66.75, 67.  , 67.25,\n       67.5 , 67.75, 68.  , 68.25, 68.5 , 68.75, 69.  , 69.25, 69.5 , 69.75,\n       70.  , 70.25, 70.5 , 70.75, 71.  , 71.25, 71.5 , 71.75, 72.  , 72.25,\n       72.5 , 72.75, 73.  , 73.25, 73.5 , 73.75, 74.  , 74.25, 74.5 , 74.75,\n       75.  , 75.25, 75.5 , 75.75, 76.  , 76.25, 76.5 , 76.75, 77.  , 77.25,\n       77.5 , 77.75, 78.  , 78.25, 78.5 , 78.75, 79.  , 79.25, 79.5 , 79.75,\n       80.  ], dtype=float32)lat(lat)float3225.0 24.75 24.5 ... 5.5 5.25 5.0standard_name :latitudelong_name :latitudeunits :degrees_northarray([25.  , 24.75, 24.5 , 24.25, 24.  , 23.75, 23.5 , 23.25, 23.  , 22.75,\n       22.5 , 22.25, 22.  , 21.75, 21.5 , 21.25, 21.  , 20.75, 20.5 , 20.25,\n       20.  , 19.75, 19.5 , 19.25, 19.  , 18.75, 18.5 , 18.25, 18.  , 17.75,\n       17.5 , 17.25, 17.  , 16.75, 16.5 , 16.25, 16.  , 15.75, 15.5 , 15.25,\n       15.  , 14.75, 14.5 , 14.25, 14.  , 13.75, 13.5 , 13.25, 13.  , 12.75,\n       12.5 , 12.25, 12.  , 11.75, 11.5 , 11.25, 11.  , 10.75, 10.5 , 10.25,\n       10.  ,  9.75,  9.5 ,  9.25,  9.  ,  8.75,  8.5 ,  8.25,  8.  ,  7.75,\n        7.5 ,  7.25,  7.  ,  6.75,  6.5 ,  6.25,  6.  ,  5.75,  5.5 ,  5.25,\n        5.  ], dtype=float32)time(time)datetime64[ns]2003-01-01 ... 2003-02-28array(['2003-01-01T00:00:00.000000000', '2003-01-02T00:00:00.000000000',\n       '2003-01-03T00:00:00.000000000', '2003-01-04T00:00:00.000000000',\n       '2003-01-05T00:00:00.000000000', '2003-01-06T00:00:00.000000000',\n       '2003-01-07T00:00:00.000000000', '2003-01-08T00:00:00.000000000',\n       '2003-01-09T00:00:00.000000000', '2003-01-10T00:00:00.000000000',\n       '2003-01-11T00:00:00.000000000', '2003-01-12T00:00:00.000000000',\n       '2003-01-13T00:00:00.000000000', '2003-01-14T00:00:00.000000000',\n       '2003-01-15T00:00:00.000000000', '2003-01-16T00:00:00.000000000',\n       '2003-01-17T00:00:00.000000000', '2003-01-18T00:00:00.000000000',\n       '2003-01-19T00:00:00.000000000', '2003-01-20T00:00:00.000000000',\n       '2003-01-21T00:00:00.000000000', '2003-01-22T00:00:00.000000000',\n       '2003-01-23T00:00:00.000000000', '2003-01-24T00:00:00.000000000',\n       '2003-01-25T00:00:00.000000000', '2003-01-26T00:00:00.000000000',\n       '2003-01-27T00:00:00.000000000', '2003-01-28T00:00:00.000000000',\n       '2003-01-29T00:00:00.000000000', '2003-01-30T00:00:00.000000000',\n       '2003-01-31T00:00:00.000000000', '2003-02-01T00:00:00.000000000',\n       '2003-02-02T00:00:00.000000000', '2003-02-03T00:00:00.000000000',\n       '2003-02-04T00:00:00.000000000', '2003-02-05T00:00:00.000000000',\n       '2003-02-06T00:00:00.000000000', '2003-02-07T00:00:00.000000000',\n       '2003-02-08T00:00:00.000000000', '2003-02-09T00:00:00.000000000',\n       '2003-02-10T00:00:00.000000000', '2003-02-11T00:00:00.000000000',\n       '2003-02-12T00:00:00.000000000', '2003-02-13T00:00:00.000000000',\n       '2003-02-14T00:00:00.000000000', '2003-02-15T00:00:00.000000000',\n       '2003-02-16T00:00:00.000000000', '2003-02-17T00:00:00.000000000',\n       '2003-02-18T00:00:00.000000000', '2003-02-19T00:00:00.000000000',\n       '2003-02-20T00:00:00.000000000', '2003-02-21T00:00:00.000000000',\n       '2003-02-22T00:00:00.000000000', '2003-02-23T00:00:00.000000000',\n       '2003-02-24T00:00:00.000000000', '2003-02-25T00:00:00.000000000',\n       '2003-02-26T00:00:00.000000000', '2003-02-27T00:00:00.000000000',\n       '2003-02-28T00:00:00.000000000'], dtype='datetime64[ns]')Data variables: (4)u_wind(time, lat, lon)float323.128 2.865 2.555 ... -3.737 -4.479standard_name :eastward_windunits :m s**-1long_name :10 metre U wind componentnameECMWF :10 metre U wind componentshortNameECMWF :10unameCDM :10_metre_U_wind_component_surfaceproduct_type :analysisarray([[[ 3.1276042 ,  2.8645833 ,  2.5546875 , ...,  0.8776042 ,\n          0.9036458 ,  0.8072917 ],\n        [ 3.1223958 ,  2.8854167 ,  2.5703125 , ...,  0.6015625 ,\n          0.6380208 ,  0.6848958 ],\n        [ 2.5026042 ,  2.2864583 ,  2.0598958 , ...,  0.3828125 ,\n          0.390625  ,  0.45833334],\n        ...,\n        [-2.7916667 , -2.8932292 , -3.0182292 , ...,  0.04947917,\n          0.21614583,  0.23177083],\n        [-2.6380208 , -2.7317708 , -2.8567708 , ..., -0.02604167,\n          0.06510416,  0.05729167],\n        [-2.53125   , -2.6354167 , -2.7682292 , ..., -0.19270833,\n         -0.09114584, -0.07291666]],\n\n       [[-0.41666666, -0.7369792 , -1.0208334 , ..., -1.4401041 ,\n         -1.5677084 , -1.5677084 ],\n        [ 0.12239584, -0.13541667, -0.39583334, ..., -1.4348959 ,\n         -1.453125  , -1.3541666 ],\n        [ 0.6953125 ,  0.5234375 ,  0.44791666, ..., -1.3515625 ,\n         -1.2317709 , -1.140625  ],\n...\n        [-3.0833333 , -3.0286458 , -2.9739583 , ..., -0.8567708 ,\n         -1.21875   , -1.7447916 ],\n        [-3.0364583 , -2.9973958 , -2.9401042 , ..., -0.9322917 ,\n         -1.3854166 , -1.9401041 ],\n        [-3.0026042 , -2.9739583 , -2.9322917 , ..., -1.0442709 ,\n         -1.4401041 , -1.8255209 ]],\n\n       [[ 8.1119795 ,  8.020833  ,  7.950521  , ..., -1.0338541 ,\n         -1.0494791 , -1.0703125 ],\n        [ 7.78125   ,  7.7890625 ,  7.734375  , ..., -0.7942708 ,\n         -0.7135417 , -0.6015625 ],\n        [ 7.205729  ,  7.294271  ,  7.325521  , ..., -0.5026042 ,\n         -0.38020834, -0.29166666],\n        ...,\n        [-1.4270834 , -1.3619791 , -1.3125    , ..., -1.6119791 ,\n         -2.5520833 , -3.8203125 ],\n        [-1.2604166 , -1.2005209 , -1.1328125 , ..., -2.3958333 ,\n         -3.390625  , -4.5390625 ],\n        [-1.0625    , -1.0208334 , -0.9739583 , ..., -2.8697917 ,\n         -3.7369792 , -4.4791665 ]]], dtype=float32)v_wind(time, lat, lon)float320.6016 0.6589 ... -1.914 -2.474standard_name :northward_windunits :m s**-1long_name :10 metre V wind componentnameECMWF :10 metre V wind componentshortNameECMWF :10vnameCDM :10_metre_V_wind_component_surfaceproduct_type :analysisarray([[[ 6.0156250e-01,  6.5885419e-01,  6.0677081e-01, ...,\n         -1.3671875e+00, -1.1848959e+00, -8.8020831e-01],\n        [ 1.0729166e+00,  1.1536459e+00,  1.0807291e+00, ...,\n         -1.4791666e+00, -1.3463541e+00, -1.1796875e+00],\n        [ 1.1510416e+00,  1.2838541e+00,  1.3229166e+00, ...,\n         -1.5963541e+00, -1.4869791e+00, -1.3489584e+00],\n        ...,\n        [-3.6770833e+00, -3.8229167e+00, -3.9713542e+00, ...,\n         -2.3437500e+00, -1.3020834e+00, -3.7760416e-01],\n        [-3.7083333e+00, -3.8255208e+00, -3.9088542e+00, ...,\n         -2.3593750e+00, -1.5390625e+00, -7.4739581e-01],\n        [-3.7395833e+00, -3.7526042e+00, -3.7812500e+00, ...,\n         -2.4817708e+00, -1.6953125e+00, -1.1484375e+00]],\n\n       [[-1.5208334e+00, -1.7005209e+00, -1.9192709e+00, ...,\n         -3.9062500e-01, -4.2708334e-01, -4.9218750e-01],\n        [-1.9010416e+00, -1.9583334e+00, -2.1458333e+00, ...,\n         -5.0260419e-01, -5.8593750e-01, -5.0260419e-01],\n        [-2.2838542e+00, -2.2473958e+00, -2.2109375e+00, ...,\n         -4.9479166e-01, -6.6145831e-01, -6.0937500e-01],\n...\n        [-4.0729165e+00, -4.1276040e+00, -4.1770835e+00, ...,\n         -2.1171875e+00, -1.6562500e+00, -1.3151041e+00],\n        [-4.0885415e+00, -4.1484375e+00, -4.2005210e+00, ...,\n         -2.4244792e+00, -2.1119792e+00, -1.8515625e+00],\n        [-4.0625000e+00, -4.1197915e+00, -4.1614585e+00, ...,\n         -2.7031250e+00, -2.4166667e+00, -2.2708333e+00]],\n\n       [[-1.9791667e-01,  5.2083335e-03,  1.5625000e-01, ...,\n         -1.2760417e-01, -1.6145833e-01, -1.6927083e-01],\n        [-7.9687500e-01, -5.9895831e-01, -4.3750000e-01, ...,\n          5.4687500e-02,  4.6875000e-02,  1.5364583e-01],\n        [-1.2994791e+00, -1.0937500e+00, -9.4010419e-01, ...,\n          2.2395833e-01,  2.9166666e-01,  3.8020834e-01],\n        ...,\n        [-3.0625000e+00, -3.0781250e+00, -3.1197917e+00, ...,\n          5.4687500e-01,  1.2500000e-01, -8.0729169e-01],\n        [-3.1614583e+00, -3.2005208e+00, -3.2291667e+00, ...,\n         -4.1666666e-01, -1.0598959e+00, -1.8958334e+00],\n        [-3.2526042e+00, -3.3255208e+00, -3.3750000e+00, ...,\n         -1.3333334e+00, -1.9140625e+00, -2.4739583e+00]]], dtype=float32)speed(time, lat, lon)float323.945 3.645 3.368 ... 4.227 5.153array([[[3.9451437, 3.6454449, 3.36847  , ..., 1.9201735, 1.8865218,\n         1.7468839],\n        [4.1419544, 3.8504055, 3.444007 , ..., 1.9789158, 1.9153827,\n         1.8310242],\n        [3.6950111, 3.4435928, 3.1019478, ..., 1.9867468, 1.9142274,\n         1.8317447],\n        ...,\n        [4.7469373, 4.912411 , 5.092304 , ..., 2.5135314, 1.6478343,\n         1.3388939],\n        [4.676735 , 4.8191776, 4.95795  , ..., 2.5241547, 1.7888355,\n         1.2424096],\n        [4.636933 , 4.7195234, 4.826424 , ..., 2.6338756, 1.8901653,\n         1.4170161]],\n\n       [[2.3318148, 2.4910347, 2.7374954, ..., 1.7411982, 1.8597933,\n         1.8728681],\n        [2.414438 , 2.5525272, 2.7947886, ..., 1.7888421, 1.8435212,\n         1.7328191],\n        [2.7723866, 2.8588107, 2.8829317, ..., 1.7411242, 1.7196132,\n         1.6104554],\n...\n        [5.146253 , 5.1557384, 5.1632495, ..., 2.8367107, 2.591439 ,\n         2.534239 ],\n        [5.1336794, 5.1560197, 5.166705 , ..., 3.0493386, 2.972432 ,\n         3.014616 ],\n        [5.0962915, 5.1267576, 5.1389093, ..., 3.263656 , 3.196616 ,\n         3.23878  ]],\n\n       [[8.189649 , 8.106084 , 8.037755 , ..., 2.1850507, 2.2752473,\n         2.2316103],\n        [7.892883 , 7.890354 , 7.823316 , ..., 2.3158002, 2.3716338,\n         2.317237 ],\n        [7.391702 , 7.4489727, 7.4578147, ..., 2.337489 , 2.231067 ,\n         2.291389 ],\n        ...,\n        [3.4753053, 3.467406 , 3.4899578, ..., 1.9244485, 2.6942132,\n         4.00572  ],\n        [3.5153275, 3.533966 , 3.542756 , ..., 2.549701 , 3.6089706,\n         4.9639993],\n        [3.5520408, 3.6114044, 3.6468   , ..., 3.2160883, 4.226849 ,\n         5.1525497]]], dtype=float32)direction(time, lat, lon)float3222.31 17.58 13.01 ... -153.4 -151.6standard_name :northward_windunits :m s**-1long_name :10 metre V wind componentnameECMWF :10 metre V wind componentshortNameECMWF :10vnameCDM :10_metre_V_wind_component_surfaceproduct_type :analysisarray([[[  22.31256  ,   17.577126 ,   13.005119 , ...,  -67.38259  ,\n          -47.755325 ,  -33.74233  ],\n        [  30.220772 ,   29.43544  ,   26.357203 , ...,  -78.10914  ,\n          -64.685646 ,  -49.11764  ],\n        [  35.783054 ,   37.048702 ,   35.60695  , ...,  -85.95185  ,\n          -86.35613  ,  -70.15734  ],\n        ...,\n        [-126.93241  , -126.620476 , -126.65524  , ...,  -89.24673  ,\n          -80.60198  ,   -4.201843 ],\n        [-125.02897  , -124.928856 , -125.43902  , ...,  -90.54617  ,\n          -84.9731   ,  -69.18275  ],\n        [-123.4584   , -124.29117  , -125.278015 , ...,  -93.996056 ,\n          -90.138794 ,  -88.29269  ]],\n\n       [[ -72.070206 ,  -85.1071   ,  -93.38763  , ...,  -58.74728  ,\n          -60.06917  ,  -58.698284 ],\n        [ -80.372795 ,  -90.8068   ,  -96.67983  , ...,  -58.435246 ,\n          -56.67389  ,  -58.751236 ],\n        [ -80.680115 ,  -86.12191  ,  -88.40796  , ...,  -59.58118  ,\n          -53.925976 ,  -70.68571  ],\n...\n        [-127.28055  , -126.49218  , -125.74938  , ...,  -71.24183  ,\n          -79.70399  , -133.6834   ],\n        [-126.68684  , -125.9416   , -125.15563  , ..., -114.46243  ,\n         -107.83289  , -131.2433   ],\n        [-126.467865 , -125.800575 , -125.19381  , ..., -113.44105  ,\n         -120.908356 , -126.25626  ]],\n\n       [[  -0.9108308,    0.405311 ,    1.362003 , ...,  -42.883907 ,\n          -38.677364 ,  -36.70583  ],\n        [  -5.2671247,   -3.8980722,   -2.9317954, ...,  -26.818964 ,\n          -36.38267  ,   -2.3517277],\n        [  -9.45855  ,   -7.810347 ,   -6.7994595, ...,    9.280067 ,\n            2.028537 ,   22.812065 ],\n        ...,\n        [-115.06766  , -113.99908  , -113.05724  , ...,   26.856642 ,\n           10.176036 ,  -94.912285 ],\n        [-111.756325 , -110.610634 , -109.46108  , ...,  -81.22135  ,\n         -133.58519  , -158.10306  ],\n        [-108.30612  , -107.21555  , -106.26769  , ..., -140.94438  ,\n         -153.4189   , -151.60954  ]]], dtype=float32)Indexes: (3)lonPandasIndexPandasIndex(Index([ 60.0, 60.25,  60.5, 60.75,  61.0, 61.25,  61.5, 61.75,  62.0, 62.25,\n        62.5, 62.75,  63.0, 63.25,  63.5, 63.75,  64.0, 64.25,  64.5, 64.75,\n        65.0, 65.25,  65.5, 65.75,  66.0, 66.25,  66.5, 66.75,  67.0, 67.25,\n        67.5, 67.75,  68.0, 68.25,  68.5, 68.75,  69.0, 69.25,  69.5, 69.75,\n        70.0, 70.25,  70.5, 70.75,  71.0, 71.25,  71.5, 71.75,  72.0, 72.25,\n        72.5, 72.75,  73.0, 73.25,  73.5, 73.75,  74.0, 74.25,  74.5, 74.75,\n        75.0, 75.25,  75.5, 75.75,  76.0, 76.25,  76.5, 76.75,  77.0, 77.25,\n        77.5, 77.75,  78.0, 78.25,  78.5, 78.75,  79.0, 79.25,  79.5, 79.75,\n        80.0],\n      dtype='float32', name='lon'))latPandasIndexPandasIndex(Index([ 25.0, 24.75,  24.5, 24.25,  24.0, 23.75,  23.5, 23.25,  23.0, 22.75,\n        22.5, 22.25,  22.0, 21.75,  21.5, 21.25,  21.0, 20.75,  20.5, 20.25,\n        20.0, 19.75,  19.5, 19.25,  19.0, 18.75,  18.5, 18.25,  18.0, 17.75,\n        17.5, 17.25,  17.0, 16.75,  16.5, 16.25,  16.0, 15.75,  15.5, 15.25,\n        15.0, 14.75,  14.5, 14.25,  14.0, 13.75,  13.5, 13.25,  13.0, 12.75,\n        12.5, 12.25,  12.0, 11.75,  11.5, 11.25,  11.0, 10.75,  10.5, 10.25,\n        10.0,  9.75,   9.5,  9.25,   9.0,  8.75,   8.5,  8.25,   8.0,  7.75,\n         7.5,  7.25,   7.0,  6.75,   6.5,  6.25,   6.0,  5.75,   5.5,  5.25,\n         5.0],\n      dtype='float32', name='lat'))timePandasIndexPandasIndex(DatetimeIndex(['2003-01-01', '2003-01-02', '2003-01-03', '2003-01-04',\n               '2003-01-05', '2003-01-06', '2003-01-07', '2003-01-08',\n               '2003-01-09', '2003-01-10', '2003-01-11', '2003-01-12',\n               '2003-01-13', '2003-01-14', '2003-01-15', '2003-01-16',\n               '2003-01-17', '2003-01-18', '2003-01-19', '2003-01-20',\n               '2003-01-21', '2003-01-22', '2003-01-23', '2003-01-24',\n               '2003-01-25', '2003-01-26', '2003-01-27', '2003-01-28',\n               '2003-01-29', '2003-01-30', '2003-01-31', '2003-02-01',\n               '2003-02-02', '2003-02-03', '2003-02-04', '2003-02-05',\n               '2003-02-06', '2003-02-07', '2003-02-08', '2003-02-09',\n               '2003-02-10', '2003-02-11', '2003-02-12', '2003-02-13',\n               '2003-02-14', '2003-02-15', '2003-02-16', '2003-02-17',\n               '2003-02-18', '2003-02-19', '2003-02-20', '2003-02-21',\n               '2003-02-22', '2003-02-23', '2003-02-24', '2003-02-25',\n               '2003-02-26', '2003-02-27', '2003-02-28'],\n              dtype='datetime64[ns]', name='time', freq=None))Attributes: (3)source :Reanalysisinstitution :ECMWFtilte :ERA5 forecasts"
  },
  {
    "objectID": "python-notebooks/04_Combining_Data.html#process-mur-data",
    "href": "python-notebooks/04_Combining_Data.html#process-mur-data",
    "title": "Combine, clean, and export the data",
    "section": "Process MUR data",
    "text": "Process MUR data\n\nsst\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'analysed_sst' (time: 59, lat: 80, lon: 80)&gt;\ndask.array&lt;concatenate, shape=(59, 80, 80), dtype=float32, chunksize=(31, 80, 80), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * time     (time) datetime64[ns] 2003-01-01T09:00:00 ... 2003-02-28T09:00:00\n  * lat      (lat) float32 5.125 5.375 5.625 5.875 ... 24.12 24.38 24.62 24.88\n  * lon      (lon) float32 60.12 60.38 60.62 60.88 ... 79.12 79.38 79.62 79.88\nAttributes:\n    long_name:              analysed sea surface temperature\n    standard_name:          sea_surface_foundation_temperature\n    coverage_content_type:  physicalMeasurement\n    units:                  kelvin\n    valid_min:              -32767\n    valid_max:              32767\n    comment:                \"Final\" version using Multi-Resolution Variationa...\n    source:                 MODIS_T-JPL, MODIS_A-JPL, AMSRE-REMSS, AVHRR17_G-...xarray.DataArray'analysed_sst'time: 59lat: 80lon: 80dask.array&lt;chunksize=(31, 80, 80), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.44 MiB\n775.00 kiB\n\n\nShape\n(59, 80, 80)\n(31, 80, 80)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\nCoordinates: (3)time(time)datetime64[ns]2003-01-01T09:00:00 ... 2003-02-...long_name :reference time of sst fieldstandard_name :timecoverage_content_type :coordinateaxis :Tcomment :Nominal time of analyzed fieldsarray(['2003-01-01T09:00:00.000000000', '2003-01-02T09:00:00.000000000',\n       '2003-01-03T09:00:00.000000000', '2003-01-04T09:00:00.000000000',\n       '2003-01-05T09:00:00.000000000', '2003-01-06T09:00:00.000000000',\n       '2003-01-07T09:00:00.000000000', '2003-01-08T09:00:00.000000000',\n       '2003-01-09T09:00:00.000000000', '2003-01-10T09:00:00.000000000',\n       '2003-01-11T09:00:00.000000000', '2003-01-12T09:00:00.000000000',\n       '2003-01-13T09:00:00.000000000', '2003-01-14T09:00:00.000000000',\n       '2003-01-15T09:00:00.000000000', '2003-01-16T09:00:00.000000000',\n       '2003-01-17T09:00:00.000000000', '2003-01-18T09:00:00.000000000',\n       '2003-01-19T09:00:00.000000000', '2003-01-20T09:00:00.000000000',\n       '2003-01-21T09:00:00.000000000', '2003-01-22T09:00:00.000000000',\n       '2003-01-23T09:00:00.000000000', '2003-01-24T09:00:00.000000000',\n       '2003-01-25T09:00:00.000000000', '2003-01-26T09:00:00.000000000',\n       '2003-01-27T09:00:00.000000000', '2003-01-28T09:00:00.000000000',\n       '2003-01-29T09:00:00.000000000', '2003-01-30T09:00:00.000000000',\n       '2003-01-31T09:00:00.000000000', '2003-02-01T09:00:00.000000000',\n       '2003-02-02T09:00:00.000000000', '2003-02-03T09:00:00.000000000',\n       '2003-02-04T09:00:00.000000000', '2003-02-05T09:00:00.000000000',\n       '2003-02-06T09:00:00.000000000', '2003-02-07T09:00:00.000000000',\n       '2003-02-08T09:00:00.000000000', '2003-02-09T09:00:00.000000000',\n       '2003-02-10T09:00:00.000000000', '2003-02-11T09:00:00.000000000',\n       '2003-02-12T09:00:00.000000000', '2003-02-13T09:00:00.000000000',\n       '2003-02-14T09:00:00.000000000', '2003-02-15T09:00:00.000000000',\n       '2003-02-16T09:00:00.000000000', '2003-02-17T09:00:00.000000000',\n       '2003-02-18T09:00:00.000000000', '2003-02-19T09:00:00.000000000',\n       '2003-02-20T09:00:00.000000000', '2003-02-21T09:00:00.000000000',\n       '2003-02-22T09:00:00.000000000', '2003-02-23T09:00:00.000000000',\n       '2003-02-24T09:00:00.000000000', '2003-02-25T09:00:00.000000000',\n       '2003-02-26T09:00:00.000000000', '2003-02-27T09:00:00.000000000',\n       '2003-02-28T09:00:00.000000000'], dtype='datetime64[ns]')lat(lat)float325.125 5.375 5.625 ... 24.62 24.88long_name :latitudestandard_name :latitudecoverage_content_type :coordinateaxis :Yunits :degrees_northvalid_min :-90.0valid_max :90.0comment :geolocations inherited from the input data without correctionarray([ 5.125,  5.375,  5.625,  5.875,  6.125,  6.375,  6.625,  6.875,  7.125,\n        7.375,  7.625,  7.875,  8.125,  8.375,  8.625,  8.875,  9.125,  9.375,\n        9.625,  9.875, 10.125, 10.375, 10.625, 10.875, 11.125, 11.375, 11.625,\n       11.875, 12.125, 12.375, 12.625, 12.875, 13.125, 13.375, 13.625, 13.875,\n       14.125, 14.375, 14.625, 14.875, 15.125, 15.375, 15.625, 15.875, 16.125,\n       16.375, 16.625, 16.875, 17.125, 17.375, 17.625, 17.875, 18.125, 18.375,\n       18.625, 18.875, 19.125, 19.375, 19.625, 19.875, 20.125, 20.375, 20.625,\n       20.875, 21.125, 21.375, 21.625, 21.875, 22.125, 22.375, 22.625, 22.875,\n       23.125, 23.375, 23.625, 23.875, 24.125, 24.375, 24.625, 24.875],\n      dtype=float32)lon(lon)float3260.12 60.38 60.62 ... 79.62 79.88long_name :longitudestandard_name :longitudecoverage_content_type :coordinateaxis :Xunits :degrees_eastvalid_min :-180.0valid_max :180.0comment :geolocations inherited from the input data without correctionarray([60.125, 60.375, 60.625, 60.875, 61.125, 61.375, 61.625, 61.875, 62.125,\n       62.375, 62.625, 62.875, 63.125, 63.375, 63.625, 63.875, 64.125, 64.375,\n       64.625, 64.875, 65.125, 65.375, 65.625, 65.875, 66.125, 66.375, 66.625,\n       66.875, 67.125, 67.375, 67.625, 67.875, 68.125, 68.375, 68.625, 68.875,\n       69.125, 69.375, 69.625, 69.875, 70.125, 70.375, 70.625, 70.875, 71.125,\n       71.375, 71.625, 71.875, 72.125, 72.375, 72.625, 72.875, 73.125, 73.375,\n       73.625, 73.875, 74.125, 74.375, 74.625, 74.875, 75.125, 75.375, 75.625,\n       75.875, 76.125, 76.375, 76.625, 76.875, 77.125, 77.375, 77.625, 77.875,\n       78.125, 78.375, 78.625, 78.875, 79.125, 79.375, 79.625, 79.875],\n      dtype=float32)Indexes: (3)timePandasIndexPandasIndex(DatetimeIndex(['2003-01-01 09:00:00', '2003-01-02 09:00:00',\n               '2003-01-03 09:00:00', '2003-01-04 09:00:00',\n               '2003-01-05 09:00:00', '2003-01-06 09:00:00',\n               '2003-01-07 09:00:00', '2003-01-08 09:00:00',\n               '2003-01-09 09:00:00', '2003-01-10 09:00:00',\n               '2003-01-11 09:00:00', '2003-01-12 09:00:00',\n               '2003-01-13 09:00:00', '2003-01-14 09:00:00',\n               '2003-01-15 09:00:00', '2003-01-16 09:00:00',\n               '2003-01-17 09:00:00', '2003-01-18 09:00:00',\n               '2003-01-19 09:00:00', '2003-01-20 09:00:00',\n               '2003-01-21 09:00:00', '2003-01-22 09:00:00',\n               '2003-01-23 09:00:00', '2003-01-24 09:00:00',\n               '2003-01-25 09:00:00', '2003-01-26 09:00:00',\n               '2003-01-27 09:00:00', '2003-01-28 09:00:00',\n               '2003-01-29 09:00:00', '2003-01-30 09:00:00',\n               '2003-01-31 09:00:00', '2003-02-01 09:00:00',\n               '2003-02-02 09:00:00', '2003-02-03 09:00:00',\n               '2003-02-04 09:00:00', '2003-02-05 09:00:00',\n               '2003-02-06 09:00:00', '2003-02-07 09:00:00',\n               '2003-02-08 09:00:00', '2003-02-09 09:00:00',\n               '2003-02-10 09:00:00', '2003-02-11 09:00:00',\n               '2003-02-12 09:00:00', '2003-02-13 09:00:00',\n               '2003-02-14 09:00:00', '2003-02-15 09:00:00',\n               '2003-02-16 09:00:00', '2003-02-17 09:00:00',\n               '2003-02-18 09:00:00', '2003-02-19 09:00:00',\n               '2003-02-20 09:00:00', '2003-02-21 09:00:00',\n               '2003-02-22 09:00:00', '2003-02-23 09:00:00',\n               '2003-02-24 09:00:00', '2003-02-25 09:00:00',\n               '2003-02-26 09:00:00', '2003-02-27 09:00:00',\n               '2003-02-28 09:00:00'],\n              dtype='datetime64[ns]', name='time', freq=None))latPandasIndexPandasIndex(Index([ 5.125,  5.375,  5.625,  5.875,  6.125,  6.375,  6.625,  6.875,  7.125,\n        7.375,  7.625,  7.875,  8.125,  8.375,  8.625,  8.875,  9.125,  9.375,\n        9.625,  9.875, 10.125, 10.375, 10.625, 10.875, 11.125, 11.375, 11.625,\n       11.875, 12.125, 12.375, 12.625, 12.875, 13.125, 13.375, 13.625, 13.875,\n       14.125, 14.375, 14.625, 14.875, 15.125, 15.375, 15.625, 15.875, 16.125,\n       16.375, 16.625, 16.875, 17.125, 17.375, 17.625, 17.875, 18.125, 18.375,\n       18.625, 18.875, 19.125, 19.375, 19.625, 19.875, 20.125, 20.375, 20.625,\n       20.875, 21.125, 21.375, 21.625, 21.875, 22.125, 22.375, 22.625, 22.875,\n       23.125, 23.375, 23.625, 23.875, 24.125, 24.375, 24.625, 24.875],\n      dtype='float32', name='lat'))lonPandasIndexPandasIndex(Index([60.125, 60.375, 60.625, 60.875, 61.125, 61.375, 61.625, 61.875, 62.125,\n       62.375, 62.625, 62.875, 63.125, 63.375, 63.625, 63.875, 64.125, 64.375,\n       64.625, 64.875, 65.125, 65.375, 65.625, 65.875, 66.125, 66.375, 66.625,\n       66.875, 67.125, 67.375, 67.625, 67.875, 68.125, 68.375, 68.625, 68.875,\n       69.125, 69.375, 69.625, 69.875, 70.125, 70.375, 70.625, 70.875, 71.125,\n       71.375, 71.625, 71.875, 72.125, 72.375, 72.625, 72.875, 73.125, 73.375,\n       73.625, 73.875, 74.125, 74.375, 74.625, 74.875, 75.125, 75.375, 75.625,\n       75.875, 76.125, 76.375, 76.625, 76.875, 77.125, 77.375, 77.625, 77.875,\n       78.125, 78.375, 78.625, 78.875, 79.125, 79.375, 79.625, 79.875],\n      dtype='float32', name='lon'))Attributes: (8)long_name :analysed sea surface temperaturestandard_name :sea_surface_foundation_temperaturecoverage_content_type :physicalMeasurementunits :kelvinvalid_min :-32767valid_max :32767comment :\"Final\" version using Multi-Resolution Variational Analysis (MRVA) method for interpolationsource :MODIS_T-JPL, MODIS_A-JPL, AMSRE-REMSS, AVHRR17_G-ACSPO, iQUAM-NOAA/NESDIS, Ice_Conc-OSISAF\n\n\n\nInterpolation\nYou can notice that the MUR sea surface temperature data, while having the same 0.25x0.25 deg spatial resolution as the ERA5 data, has a different point offset and we cannot combine them together without some interpolation. Xarray‚Äôs interp_like() function is designed to help us achieve this feat easily!\nDisclaimer: Interpolation may remove important outlier data (as you can see in the map below). Make sure to choose interpolation type wisely.\nfix time\nSince we don‚Äôt want to interpolate time (as time is already averaged over the day), let‚Äôs fix it to center at 12AM\n\nsst = sst.assign_coords(time=np.arange('2003-01-01', '2003-03-01', timedelta(days=1), dtype='datetime64[ns]'))\n\n\nsst_interp = sst.interp_like(era5)\n\nDISCLAIMER: make sure that the three coordinate names are identical on both datasets for xarray to infer correctly\n\n# verify before and after interpolation\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 8)) # 2 columns & 1 row, width, height \nsst.isel(time=0).plot.imshow(ax=ax1)\nax1.set_title('before interpolation')\nax1.set_aspect('equal')\n\nsst_interp.isel(time=0).plot.imshow(ax=ax2)\nax2.set_title('after interpolation')\nax2.set_aspect('equal')"
  },
  {
    "objectID": "python-notebooks/04_Combining_Data.html#process-copernicus-salinity-data",
    "href": "python-notebooks/04_Combining_Data.html#process-copernicus-salinity-data",
    "title": "Combine, clean, and export the data",
    "section": "Process Copernicus Salinity data",
    "text": "Process Copernicus Salinity data\n\nsalinity # also need interpolation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:    (longitude: 241, latitude: 241, time: 59)\nCoordinates:\n  * longitude  (longitude) float32 60.0 60.08 60.17 60.25 ... 79.83 79.92 80.0\n  * latitude   (latitude) float32 5.0 5.083 5.167 5.25 ... 24.83 24.92 25.0\n  * time       (time) datetime64[ns] 2003-01-01T12:00:00 ... 2003-02-28T12:00:00\nData variables:\n    so         (time, latitude, longitude) float32 ...xarray.DatasetDimensions:longitude: 241latitude: 241time: 59Coordinates: (3)longitude(longitude)float3260.0 60.08 60.17 ... 79.92 80.0valid_min :-180.0valid_max :179.91667step :0.08332825units :degrees_eastunit_long :Degrees Eastlong_name :Longitudestandard_name :longitudeaxis :X_ChunkSizes :4320array([60.      , 60.083332, 60.166668, ..., 79.833336, 79.916664, 80.      ],\n      dtype=float32)latitude(latitude)float325.0 5.083 5.167 ... 24.92 25.0valid_min :-80.0valid_max :90.0step :0.08333588units :degrees_northunit_long :Degrees Northlong_name :Latitudestandard_name :latitudeaxis :Y_ChunkSizes :2041array([ 5.      ,  5.083333,  5.166667, ..., 24.833334, 24.916666, 25.      ],\n      dtype=float32)time(time)datetime64[ns]2003-01-01T12:00:00 ... 2003-02-...long_name :Time (hours since 1950-01-01)standard_name :timeaxis :T_ChunkSizes :1024array(['2003-01-01T12:00:00.000000000', '2003-01-02T12:00:00.000000000',\n       '2003-01-03T12:00:00.000000000', '2003-01-04T12:00:00.000000000',\n       '2003-01-05T12:00:00.000000000', '2003-01-06T12:00:00.000000000',\n       '2003-01-07T12:00:00.000000000', '2003-01-08T12:00:00.000000000',\n       '2003-01-09T12:00:00.000000000', '2003-01-10T12:00:00.000000000',\n       '2003-01-11T12:00:00.000000000', '2003-01-12T12:00:00.000000000',\n       '2003-01-13T12:00:00.000000000', '2003-01-14T12:00:00.000000000',\n       '2003-01-15T12:00:00.000000000', '2003-01-16T12:00:00.000000000',\n       '2003-01-17T12:00:00.000000000', '2003-01-18T12:00:00.000000000',\n       '2003-01-19T12:00:00.000000000', '2003-01-20T12:00:00.000000000',\n       '2003-01-21T12:00:00.000000000', '2003-01-22T12:00:00.000000000',\n       '2003-01-23T12:00:00.000000000', '2003-01-24T12:00:00.000000000',\n       '2003-01-25T12:00:00.000000000', '2003-01-26T12:00:00.000000000',\n       '2003-01-27T12:00:00.000000000', '2003-01-28T12:00:00.000000000',\n       '2003-01-29T12:00:00.000000000', '2003-01-30T12:00:00.000000000',\n       '2003-01-31T12:00:00.000000000', '2003-02-01T12:00:00.000000000',\n       '2003-02-02T12:00:00.000000000', '2003-02-03T12:00:00.000000000',\n       '2003-02-04T12:00:00.000000000', '2003-02-05T12:00:00.000000000',\n       '2003-02-06T12:00:00.000000000', '2003-02-07T12:00:00.000000000',\n       '2003-02-08T12:00:00.000000000', '2003-02-09T12:00:00.000000000',\n       '2003-02-10T12:00:00.000000000', '2003-02-11T12:00:00.000000000',\n       '2003-02-12T12:00:00.000000000', '2003-02-13T12:00:00.000000000',\n       '2003-02-14T12:00:00.000000000', '2003-02-15T12:00:00.000000000',\n       '2003-02-16T12:00:00.000000000', '2003-02-17T12:00:00.000000000',\n       '2003-02-18T12:00:00.000000000', '2003-02-19T12:00:00.000000000',\n       '2003-02-20T12:00:00.000000000', '2003-02-21T12:00:00.000000000',\n       '2003-02-22T12:00:00.000000000', '2003-02-23T12:00:00.000000000',\n       '2003-02-24T12:00:00.000000000', '2003-02-25T12:00:00.000000000',\n       '2003-02-26T12:00:00.000000000', '2003-02-27T12:00:00.000000000',\n       '2003-02-28T12:00:00.000000000'], dtype='datetime64[ns]')Data variables: (1)so(time, latitude, longitude)float32...long_name :Salinitystandard_name :sea_water_salinityunits :1e-3unit_long :Practical Salinity Unitvalid_min :1valid_max :28336cell_methods :area: mean_ChunkSizes :[  1   7 341 720][3426779 values with dtype=float32]Indexes: (3)longitudePandasIndexPandasIndex(Index([             60.0, 60.08333206176758, 60.16666793823242,\n                   60.25, 60.33333206176758, 60.41666793823242,\n                    60.5, 60.58333206176758, 60.66666793823242,\n                   60.75,\n       ...\n                   79.25, 79.33333587646484, 79.41666412353516,\n                    79.5, 79.58333587646484, 79.66666412353516,\n                   79.75, 79.83333587646484, 79.91666412353516,\n                    80.0],\n      dtype='float32', name='longitude', length=241))latitudePandasIndexPandasIndex(Index([              5.0, 5.083333492279053, 5.166666507720947,\n                    5.25, 5.333333492279053, 5.416666507720947,\n                     5.5, 5.583333492279053, 5.666666507720947,\n                    5.75,\n       ...\n                   24.25, 24.33333396911621, 24.41666603088379,\n                    24.5, 24.58333396911621, 24.66666603088379,\n                   24.75, 24.83333396911621, 24.91666603088379,\n                    25.0],\n      dtype='float32', name='latitude', length=241))timePandasIndexPandasIndex(DatetimeIndex(['2003-01-01 12:00:00', '2003-01-02 12:00:00',\n               '2003-01-03 12:00:00', '2003-01-04 12:00:00',\n               '2003-01-05 12:00:00', '2003-01-06 12:00:00',\n               '2003-01-07 12:00:00', '2003-01-08 12:00:00',\n               '2003-01-09 12:00:00', '2003-01-10 12:00:00',\n               '2003-01-11 12:00:00', '2003-01-12 12:00:00',\n               '2003-01-13 12:00:00', '2003-01-14 12:00:00',\n               '2003-01-15 12:00:00', '2003-01-16 12:00:00',\n               '2003-01-17 12:00:00', '2003-01-18 12:00:00',\n               '2003-01-19 12:00:00', '2003-01-20 12:00:00',\n               '2003-01-21 12:00:00', '2003-01-22 12:00:00',\n               '2003-01-23 12:00:00', '2003-01-24 12:00:00',\n               '2003-01-25 12:00:00', '2003-01-26 12:00:00',\n               '2003-01-27 12:00:00', '2003-01-28 12:00:00',\n               '2003-01-29 12:00:00', '2003-01-30 12:00:00',\n               '2003-01-31 12:00:00', '2003-02-01 12:00:00',\n               '2003-02-02 12:00:00', '2003-02-03 12:00:00',\n               '2003-02-04 12:00:00', '2003-02-05 12:00:00',\n               '2003-02-06 12:00:00', '2003-02-07 12:00:00',\n               '2003-02-08 12:00:00', '2003-02-09 12:00:00',\n               '2003-02-10 12:00:00', '2003-02-11 12:00:00',\n               '2003-02-12 12:00:00', '2003-02-13 12:00:00',\n               '2003-02-14 12:00:00', '2003-02-15 12:00:00',\n               '2003-02-16 12:00:00', '2003-02-17 12:00:00',\n               '2003-02-18 12:00:00', '2003-02-19 12:00:00',\n               '2003-02-20 12:00:00', '2003-02-21 12:00:00',\n               '2003-02-22 12:00:00', '2003-02-23 12:00:00',\n               '2003-02-24 12:00:00', '2003-02-25 12:00:00',\n               '2003-02-26 12:00:00', '2003-02-27 12:00:00',\n               '2003-02-28 12:00:00'],\n              dtype='datetime64[ns]', name='time', freq=None))Attributes: (0)\n\n\n\nInterpolate\nYou can notice that the coordinates are not the same to era5 dataset, so let‚Äôs change it\n\nsalinity = salinity.rename({'longitude': 'lon', 'latitude': 'lat'})\n\n\n# fix time frame, again right-exclusive\nsalinity = salinity.assign_coords(time=np.arange('2003-01-01', '2003-03-01', timedelta(days=1), dtype='datetime64[ns]'))\n\n\nsalinity\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (lon: 241, lat: 241, time: 59)\nCoordinates:\n  * lon      (lon) float32 60.0 60.08 60.17 60.25 ... 79.75 79.83 79.92 80.0\n  * lat      (lat) float32 5.0 5.083 5.167 5.25 5.333 ... 24.75 24.83 24.92 25.0\n  * time     (time) datetime64[ns] 2003-01-01 2003-01-02 ... 2003-02-28\nData variables:\n    so       (time, lat, lon) float32 ...xarray.DatasetDimensions:lon: 241lat: 241time: 59Coordinates: (3)lon(lon)float3260.0 60.08 60.17 ... 79.92 80.0valid_min :-180.0valid_max :179.91667step :0.08332825units :degrees_eastunit_long :Degrees Eastlong_name :Longitudestandard_name :longitudeaxis :X_ChunkSizes :4320array([60.      , 60.083332, 60.166668, ..., 79.833336, 79.916664, 80.      ],\n      dtype=float32)lat(lat)float325.0 5.083 5.167 ... 24.92 25.0valid_min :-80.0valid_max :90.0step :0.08333588units :degrees_northunit_long :Degrees Northlong_name :Latitudestandard_name :latitudeaxis :Y_ChunkSizes :2041array([ 5.      ,  5.083333,  5.166667, ..., 24.833334, 24.916666, 25.      ],\n      dtype=float32)time(time)datetime64[ns]2003-01-01 ... 2003-02-28array(['2003-01-01T00:00:00.000000000', '2003-01-02T00:00:00.000000000',\n       '2003-01-03T00:00:00.000000000', '2003-01-04T00:00:00.000000000',\n       '2003-01-05T00:00:00.000000000', '2003-01-06T00:00:00.000000000',\n       '2003-01-07T00:00:00.000000000', '2003-01-08T00:00:00.000000000',\n       '2003-01-09T00:00:00.000000000', '2003-01-10T00:00:00.000000000',\n       '2003-01-11T00:00:00.000000000', '2003-01-12T00:00:00.000000000',\n       '2003-01-13T00:00:00.000000000', '2003-01-14T00:00:00.000000000',\n       '2003-01-15T00:00:00.000000000', '2003-01-16T00:00:00.000000000',\n       '2003-01-17T00:00:00.000000000', '2003-01-18T00:00:00.000000000',\n       '2003-01-19T00:00:00.000000000', '2003-01-20T00:00:00.000000000',\n       '2003-01-21T00:00:00.000000000', '2003-01-22T00:00:00.000000000',\n       '2003-01-23T00:00:00.000000000', '2003-01-24T00:00:00.000000000',\n       '2003-01-25T00:00:00.000000000', '2003-01-26T00:00:00.000000000',\n       '2003-01-27T00:00:00.000000000', '2003-01-28T00:00:00.000000000',\n       '2003-01-29T00:00:00.000000000', '2003-01-30T00:00:00.000000000',\n       '2003-01-31T00:00:00.000000000', '2003-02-01T00:00:00.000000000',\n       '2003-02-02T00:00:00.000000000', '2003-02-03T00:00:00.000000000',\n       '2003-02-04T00:00:00.000000000', '2003-02-05T00:00:00.000000000',\n       '2003-02-06T00:00:00.000000000', '2003-02-07T00:00:00.000000000',\n       '2003-02-08T00:00:00.000000000', '2003-02-09T00:00:00.000000000',\n       '2003-02-10T00:00:00.000000000', '2003-02-11T00:00:00.000000000',\n       '2003-02-12T00:00:00.000000000', '2003-02-13T00:00:00.000000000',\n       '2003-02-14T00:00:00.000000000', '2003-02-15T00:00:00.000000000',\n       '2003-02-16T00:00:00.000000000', '2003-02-17T00:00:00.000000000',\n       '2003-02-18T00:00:00.000000000', '2003-02-19T00:00:00.000000000',\n       '2003-02-20T00:00:00.000000000', '2003-02-21T00:00:00.000000000',\n       '2003-02-22T00:00:00.000000000', '2003-02-23T00:00:00.000000000',\n       '2003-02-24T00:00:00.000000000', '2003-02-25T00:00:00.000000000',\n       '2003-02-26T00:00:00.000000000', '2003-02-27T00:00:00.000000000',\n       '2003-02-28T00:00:00.000000000'], dtype='datetime64[ns]')Data variables: (1)so(time, lat, lon)float32...long_name :Salinitystandard_name :sea_water_salinityunits :1e-3unit_long :Practical Salinity Unitvalid_min :1valid_max :28336cell_methods :area: mean_ChunkSizes :[  1   7 341 720][3426779 values with dtype=float32]Indexes: (3)lonPandasIndexPandasIndex(Index([             60.0, 60.08333206176758, 60.16666793823242,\n                   60.25, 60.33333206176758, 60.41666793823242,\n                    60.5, 60.58333206176758, 60.66666793823242,\n                   60.75,\n       ...\n                   79.25, 79.33333587646484, 79.41666412353516,\n                    79.5, 79.58333587646484, 79.66666412353516,\n                   79.75, 79.83333587646484, 79.91666412353516,\n                    80.0],\n      dtype='float32', name='lon', length=241))latPandasIndexPandasIndex(Index([              5.0, 5.083333492279053, 5.166666507720947,\n                    5.25, 5.333333492279053, 5.416666507720947,\n                     5.5, 5.583333492279053, 5.666666507720947,\n                    5.75,\n       ...\n                   24.25, 24.33333396911621, 24.41666603088379,\n                    24.5, 24.58333396911621, 24.66666603088379,\n                   24.75, 24.83333396911621, 24.91666603088379,\n                    25.0],\n      dtype='float32', name='lat', length=241))timePandasIndexPandasIndex(DatetimeIndex(['2003-01-01', '2003-01-02', '2003-01-03', '2003-01-04',\n               '2003-01-05', '2003-01-06', '2003-01-07', '2003-01-08',\n               '2003-01-09', '2003-01-10', '2003-01-11', '2003-01-12',\n               '2003-01-13', '2003-01-14', '2003-01-15', '2003-01-16',\n               '2003-01-17', '2003-01-18', '2003-01-19', '2003-01-20',\n               '2003-01-21', '2003-01-22', '2003-01-23', '2003-01-24',\n               '2003-01-25', '2003-01-26', '2003-01-27', '2003-01-28',\n               '2003-01-29', '2003-01-30', '2003-01-31', '2003-02-01',\n               '2003-02-02', '2003-02-03', '2003-02-04', '2003-02-05',\n               '2003-02-06', '2003-02-07', '2003-02-08', '2003-02-09',\n               '2003-02-10', '2003-02-11', '2003-02-12', '2003-02-13',\n               '2003-02-14', '2003-02-15', '2003-02-16', '2003-02-17',\n               '2003-02-18', '2003-02-19', '2003-02-20', '2003-02-21',\n               '2003-02-22', '2003-02-23', '2003-02-24', '2003-02-25',\n               '2003-02-26', '2003-02-27', '2003-02-28'],\n              dtype='datetime64[ns]', name='time', freq=None))Attributes: (0)\n\n\n\nsalinity_interp = salinity.interp_like(era5)\n\n\n# verify before and after interpolation\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 8)) # 2 columns & 1 row, width, height \nsalinity['so'].isel(time=0).plot.imshow(ax=ax1)\nax1.set_title('before interpolation')\nax1.set_aspect('equal')\n\nsalinity_interp['so'].isel(time=0).plot.imshow(ax=ax2)\nax2.set_title('after interpolation')\nax2.set_aspect('equal')"
  },
  {
    "objectID": "python-notebooks/04_Combining_Data.html#combine-files-together",
    "href": "python-notebooks/04_Combining_Data.html#combine-files-together",
    "title": "Combine, clean, and export the data",
    "section": "Combine files together",
    "text": "Combine files together\nDue to different attributes in each dataset coordinates, we cannot combine all datasets together by coords by simply using the combine_by_coords() function in xarray. Issue is replicated below:\n\nfinal_ds = xr.combine_by_coords([era5, salinity_interp, sst_interp])\n\n\nfinal_ds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:       (time: 59, lat: 81, lon: 81)\nCoordinates:\n  * time          (time) datetime64[ns] 2003-01-01 2003-01-02 ... 2003-02-28\n  * lat           (lat) float32 25.0 24.75 24.5 24.25 24.0 ... 5.75 5.5 5.25 5.0\n  * lon           (lon) float32 60.0 60.25 60.5 60.75 ... 79.25 79.5 79.75 80.0\nData variables:\n    analysed_sst  (time, lat, lon) float32 dask.array&lt;chunksize=(59, 81, 81), meta=np.ndarray&gt;\n    u_wind        (time, lat, lon) float32 3.128 2.865 2.555 ... -3.737 -4.479\n    v_wind        (time, lat, lon) float32 0.6016 0.6589 ... -1.914 -2.474\n    speed         (time, lat, lon) float32 3.945 3.645 3.368 ... 4.227 5.153\n    direction     (time, lat, lon) float32 22.31 17.58 13.01 ... -153.4 -151.6\n    so            (time, lat, lon) float64 36.55 36.54 36.54 ... 34.0 34.08\nAttributes:\n    source:       Reanalysis\n    institution:  ECMWF\n    tilte:        ERA5 forecastsxarray.DatasetDimensions:time: 59lat: 81lon: 81Coordinates: (3)time(time)datetime64[ns]2003-01-01 ... 2003-02-28array(['2003-01-01T00:00:00.000000000', '2003-01-02T00:00:00.000000000',\n       '2003-01-03T00:00:00.000000000', '2003-01-04T00:00:00.000000000',\n       '2003-01-05T00:00:00.000000000', '2003-01-06T00:00:00.000000000',\n       '2003-01-07T00:00:00.000000000', '2003-01-08T00:00:00.000000000',\n       '2003-01-09T00:00:00.000000000', '2003-01-10T00:00:00.000000000',\n       '2003-01-11T00:00:00.000000000', '2003-01-12T00:00:00.000000000',\n       '2003-01-13T00:00:00.000000000', '2003-01-14T00:00:00.000000000',\n       '2003-01-15T00:00:00.000000000', '2003-01-16T00:00:00.000000000',\n       '2003-01-17T00:00:00.000000000', '2003-01-18T00:00:00.000000000',\n       '2003-01-19T00:00:00.000000000', '2003-01-20T00:00:00.000000000',\n       '2003-01-21T00:00:00.000000000', '2003-01-22T00:00:00.000000000',\n       '2003-01-23T00:00:00.000000000', '2003-01-24T00:00:00.000000000',\n       '2003-01-25T00:00:00.000000000', '2003-01-26T00:00:00.000000000',\n       '2003-01-27T00:00:00.000000000', '2003-01-28T00:00:00.000000000',\n       '2003-01-29T00:00:00.000000000', '2003-01-30T00:00:00.000000000',\n       '2003-01-31T00:00:00.000000000', '2003-02-01T00:00:00.000000000',\n       '2003-02-02T00:00:00.000000000', '2003-02-03T00:00:00.000000000',\n       '2003-02-04T00:00:00.000000000', '2003-02-05T00:00:00.000000000',\n       '2003-02-06T00:00:00.000000000', '2003-02-07T00:00:00.000000000',\n       '2003-02-08T00:00:00.000000000', '2003-02-09T00:00:00.000000000',\n       '2003-02-10T00:00:00.000000000', '2003-02-11T00:00:00.000000000',\n       '2003-02-12T00:00:00.000000000', '2003-02-13T00:00:00.000000000',\n       '2003-02-14T00:00:00.000000000', '2003-02-15T00:00:00.000000000',\n       '2003-02-16T00:00:00.000000000', '2003-02-17T00:00:00.000000000',\n       '2003-02-18T00:00:00.000000000', '2003-02-19T00:00:00.000000000',\n       '2003-02-20T00:00:00.000000000', '2003-02-21T00:00:00.000000000',\n       '2003-02-22T00:00:00.000000000', '2003-02-23T00:00:00.000000000',\n       '2003-02-24T00:00:00.000000000', '2003-02-25T00:00:00.000000000',\n       '2003-02-26T00:00:00.000000000', '2003-02-27T00:00:00.000000000',\n       '2003-02-28T00:00:00.000000000'], dtype='datetime64[ns]')lat(lat)float3225.0 24.75 24.5 ... 5.5 5.25 5.0standard_name :latitudelong_name :latitudeunits :degrees_northarray([25.  , 24.75, 24.5 , 24.25, 24.  , 23.75, 23.5 , 23.25, 23.  , 22.75,\n       22.5 , 22.25, 22.  , 21.75, 21.5 , 21.25, 21.  , 20.75, 20.5 , 20.25,\n       20.  , 19.75, 19.5 , 19.25, 19.  , 18.75, 18.5 , 18.25, 18.  , 17.75,\n       17.5 , 17.25, 17.  , 16.75, 16.5 , 16.25, 16.  , 15.75, 15.5 , 15.25,\n       15.  , 14.75, 14.5 , 14.25, 14.  , 13.75, 13.5 , 13.25, 13.  , 12.75,\n       12.5 , 12.25, 12.  , 11.75, 11.5 , 11.25, 11.  , 10.75, 10.5 , 10.25,\n       10.  ,  9.75,  9.5 ,  9.25,  9.  ,  8.75,  8.5 ,  8.25,  8.  ,  7.75,\n        7.5 ,  7.25,  7.  ,  6.75,  6.5 ,  6.25,  6.  ,  5.75,  5.5 ,  5.25,\n        5.  ], dtype=float32)lon(lon)float3260.0 60.25 60.5 ... 79.5 79.75 80.0standard_name :longitudelong_name :longitudeunits :degrees_eastarray([60.  , 60.25, 60.5 , 60.75, 61.  , 61.25, 61.5 , 61.75, 62.  , 62.25,\n       62.5 , 62.75, 63.  , 63.25, 63.5 , 63.75, 64.  , 64.25, 64.5 , 64.75,\n       65.  , 65.25, 65.5 , 65.75, 66.  , 66.25, 66.5 , 66.75, 67.  , 67.25,\n       67.5 , 67.75, 68.  , 68.25, 68.5 , 68.75, 69.  , 69.25, 69.5 , 69.75,\n       70.  , 70.25, 70.5 , 70.75, 71.  , 71.25, 71.5 , 71.75, 72.  , 72.25,\n       72.5 , 72.75, 73.  , 73.25, 73.5 , 73.75, 74.  , 74.25, 74.5 , 74.75,\n       75.  , 75.25, 75.5 , 75.75, 76.  , 76.25, 76.5 , 76.75, 77.  , 77.25,\n       77.5 , 77.75, 78.  , 78.25, 78.5 , 78.75, 79.  , 79.25, 79.5 , 79.75,\n       80.  ], dtype=float32)Data variables: (6)analysed_sst(time, lat, lon)float32dask.array&lt;chunksize=(59, 81, 81), meta=np.ndarray&gt;long_name :analysed sea surface temperaturestandard_name :sea_surface_foundation_temperaturecoverage_content_type :physicalMeasurementunits :kelvinvalid_min :-32767valid_max :32767comment :\"Final\" version using Multi-Resolution Variational Analysis (MRVA) method for interpolationsource :MODIS_T-JPL, MODIS_A-JPL, AMSRE-REMSS, AVHRR17_G-ACSPO, iQUAM-NOAA/NESDIS, Ice_Conc-OSISAF\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.48 MiB\n1.48 MiB\n\n\nShape\n(59, 81, 81)\n(59, 81, 81)\n\n\nDask graph\n1 chunks in 25 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\nu_wind(time, lat, lon)float323.128 2.865 2.555 ... -3.737 -4.479standard_name :eastward_windunits :m s**-1long_name :10 metre U wind componentnameECMWF :10 metre U wind componentshortNameECMWF :10unameCDM :10_metre_U_wind_component_surfaceproduct_type :analysisarray([[[ 3.1276042 ,  2.8645833 ,  2.5546875 , ...,  0.8776042 ,\n          0.9036458 ,  0.8072917 ],\n        [ 3.1223958 ,  2.8854167 ,  2.5703125 , ...,  0.6015625 ,\n          0.6380208 ,  0.6848958 ],\n        [ 2.5026042 ,  2.2864583 ,  2.0598958 , ...,  0.3828125 ,\n          0.390625  ,  0.45833334],\n        ...,\n        [-2.7916667 , -2.8932292 , -3.0182292 , ...,  0.04947917,\n          0.21614583,  0.23177083],\n        [-2.6380208 , -2.7317708 , -2.8567708 , ..., -0.02604167,\n          0.06510416,  0.05729167],\n        [-2.53125   , -2.6354167 , -2.7682292 , ..., -0.19270833,\n         -0.09114584, -0.07291666]],\n\n       [[-0.41666666, -0.7369792 , -1.0208334 , ..., -1.4401041 ,\n         -1.5677084 , -1.5677084 ],\n        [ 0.12239584, -0.13541667, -0.39583334, ..., -1.4348959 ,\n         -1.453125  , -1.3541666 ],\n        [ 0.6953125 ,  0.5234375 ,  0.44791666, ..., -1.3515625 ,\n         -1.2317709 , -1.140625  ],\n...\n        [-3.0833333 , -3.0286458 , -2.9739583 , ..., -0.8567708 ,\n         -1.21875   , -1.7447916 ],\n        [-3.0364583 , -2.9973958 , -2.9401042 , ..., -0.9322917 ,\n         -1.3854166 , -1.9401041 ],\n        [-3.0026042 , -2.9739583 , -2.9322917 , ..., -1.0442709 ,\n         -1.4401041 , -1.8255209 ]],\n\n       [[ 8.1119795 ,  8.020833  ,  7.950521  , ..., -1.0338541 ,\n         -1.0494791 , -1.0703125 ],\n        [ 7.78125   ,  7.7890625 ,  7.734375  , ..., -0.7942708 ,\n         -0.7135417 , -0.6015625 ],\n        [ 7.205729  ,  7.294271  ,  7.325521  , ..., -0.5026042 ,\n         -0.38020834, -0.29166666],\n        ...,\n        [-1.4270834 , -1.3619791 , -1.3125    , ..., -1.6119791 ,\n         -2.5520833 , -3.8203125 ],\n        [-1.2604166 , -1.2005209 , -1.1328125 , ..., -2.3958333 ,\n         -3.390625  , -4.5390625 ],\n        [-1.0625    , -1.0208334 , -0.9739583 , ..., -2.8697917 ,\n         -3.7369792 , -4.4791665 ]]], dtype=float32)v_wind(time, lat, lon)float320.6016 0.6589 ... -1.914 -2.474standard_name :northward_windunits :m s**-1long_name :10 metre V wind componentnameECMWF :10 metre V wind componentshortNameECMWF :10vnameCDM :10_metre_V_wind_component_surfaceproduct_type :analysisarray([[[ 6.0156250e-01,  6.5885419e-01,  6.0677081e-01, ...,\n         -1.3671875e+00, -1.1848959e+00, -8.8020831e-01],\n        [ 1.0729166e+00,  1.1536459e+00,  1.0807291e+00, ...,\n         -1.4791666e+00, -1.3463541e+00, -1.1796875e+00],\n        [ 1.1510416e+00,  1.2838541e+00,  1.3229166e+00, ...,\n         -1.5963541e+00, -1.4869791e+00, -1.3489584e+00],\n        ...,\n        [-3.6770833e+00, -3.8229167e+00, -3.9713542e+00, ...,\n         -2.3437500e+00, -1.3020834e+00, -3.7760416e-01],\n        [-3.7083333e+00, -3.8255208e+00, -3.9088542e+00, ...,\n         -2.3593750e+00, -1.5390625e+00, -7.4739581e-01],\n        [-3.7395833e+00, -3.7526042e+00, -3.7812500e+00, ...,\n         -2.4817708e+00, -1.6953125e+00, -1.1484375e+00]],\n\n       [[-1.5208334e+00, -1.7005209e+00, -1.9192709e+00, ...,\n         -3.9062500e-01, -4.2708334e-01, -4.9218750e-01],\n        [-1.9010416e+00, -1.9583334e+00, -2.1458333e+00, ...,\n         -5.0260419e-01, -5.8593750e-01, -5.0260419e-01],\n        [-2.2838542e+00, -2.2473958e+00, -2.2109375e+00, ...,\n         -4.9479166e-01, -6.6145831e-01, -6.0937500e-01],\n...\n        [-4.0729165e+00, -4.1276040e+00, -4.1770835e+00, ...,\n         -2.1171875e+00, -1.6562500e+00, -1.3151041e+00],\n        [-4.0885415e+00, -4.1484375e+00, -4.2005210e+00, ...,\n         -2.4244792e+00, -2.1119792e+00, -1.8515625e+00],\n        [-4.0625000e+00, -4.1197915e+00, -4.1614585e+00, ...,\n         -2.7031250e+00, -2.4166667e+00, -2.2708333e+00]],\n\n       [[-1.9791667e-01,  5.2083335e-03,  1.5625000e-01, ...,\n         -1.2760417e-01, -1.6145833e-01, -1.6927083e-01],\n        [-7.9687500e-01, -5.9895831e-01, -4.3750000e-01, ...,\n          5.4687500e-02,  4.6875000e-02,  1.5364583e-01],\n        [-1.2994791e+00, -1.0937500e+00, -9.4010419e-01, ...,\n          2.2395833e-01,  2.9166666e-01,  3.8020834e-01],\n        ...,\n        [-3.0625000e+00, -3.0781250e+00, -3.1197917e+00, ...,\n          5.4687500e-01,  1.2500000e-01, -8.0729169e-01],\n        [-3.1614583e+00, -3.2005208e+00, -3.2291667e+00, ...,\n         -4.1666666e-01, -1.0598959e+00, -1.8958334e+00],\n        [-3.2526042e+00, -3.3255208e+00, -3.3750000e+00, ...,\n         -1.3333334e+00, -1.9140625e+00, -2.4739583e+00]]], dtype=float32)speed(time, lat, lon)float323.945 3.645 3.368 ... 4.227 5.153array([[[3.9451437, 3.6454449, 3.36847  , ..., 1.9201735, 1.8865218,\n         1.7468839],\n        [4.1419544, 3.8504055, 3.444007 , ..., 1.9789158, 1.9153827,\n         1.8310242],\n        [3.6950111, 3.4435928, 3.1019478, ..., 1.9867468, 1.9142274,\n         1.8317447],\n        ...,\n        [4.7469373, 4.912411 , 5.092304 , ..., 2.5135314, 1.6478343,\n         1.3388939],\n        [4.676735 , 4.8191776, 4.95795  , ..., 2.5241547, 1.7888355,\n         1.2424096],\n        [4.636933 , 4.7195234, 4.826424 , ..., 2.6338756, 1.8901653,\n         1.4170161]],\n\n       [[2.3318148, 2.4910347, 2.7374954, ..., 1.7411982, 1.8597933,\n         1.8728681],\n        [2.414438 , 2.5525272, 2.7947886, ..., 1.7888421, 1.8435212,\n         1.7328191],\n        [2.7723866, 2.8588107, 2.8829317, ..., 1.7411242, 1.7196132,\n         1.6104554],\n...\n        [5.146253 , 5.1557384, 5.1632495, ..., 2.8367107, 2.591439 ,\n         2.534239 ],\n        [5.1336794, 5.1560197, 5.166705 , ..., 3.0493386, 2.972432 ,\n         3.014616 ],\n        [5.0962915, 5.1267576, 5.1389093, ..., 3.263656 , 3.196616 ,\n         3.23878  ]],\n\n       [[8.189649 , 8.106084 , 8.037755 , ..., 2.1850507, 2.2752473,\n         2.2316103],\n        [7.892883 , 7.890354 , 7.823316 , ..., 2.3158002, 2.3716338,\n         2.317237 ],\n        [7.391702 , 7.4489727, 7.4578147, ..., 2.337489 , 2.231067 ,\n         2.291389 ],\n        ...,\n        [3.4753053, 3.467406 , 3.4899578, ..., 1.9244485, 2.6942132,\n         4.00572  ],\n        [3.5153275, 3.533966 , 3.542756 , ..., 2.549701 , 3.6089706,\n         4.9639993],\n        [3.5520408, 3.6114044, 3.6468   , ..., 3.2160883, 4.226849 ,\n         5.1525497]]], dtype=float32)direction(time, lat, lon)float3222.31 17.58 13.01 ... -153.4 -151.6standard_name :northward_windunits :m s**-1long_name :10 metre V wind componentnameECMWF :10 metre V wind componentshortNameECMWF :10vnameCDM :10_metre_V_wind_component_surfaceproduct_type :analysisarray([[[  22.31256  ,   17.577126 ,   13.005119 , ...,  -67.38259  ,\n          -47.755325 ,  -33.74233  ],\n        [  30.220772 ,   29.43544  ,   26.357203 , ...,  -78.10914  ,\n          -64.685646 ,  -49.11764  ],\n        [  35.783054 ,   37.048702 ,   35.60695  , ...,  -85.95185  ,\n          -86.35613  ,  -70.15734  ],\n        ...,\n        [-126.93241  , -126.620476 , -126.65524  , ...,  -89.24673  ,\n          -80.60198  ,   -4.201843 ],\n        [-125.02897  , -124.928856 , -125.43902  , ...,  -90.54617  ,\n          -84.9731   ,  -69.18275  ],\n        [-123.4584   , -124.29117  , -125.278015 , ...,  -93.996056 ,\n          -90.138794 ,  -88.29269  ]],\n\n       [[ -72.070206 ,  -85.1071   ,  -93.38763  , ...,  -58.74728  ,\n          -60.06917  ,  -58.698284 ],\n        [ -80.372795 ,  -90.8068   ,  -96.67983  , ...,  -58.435246 ,\n          -56.67389  ,  -58.751236 ],\n        [ -80.680115 ,  -86.12191  ,  -88.40796  , ...,  -59.58118  ,\n          -53.925976 ,  -70.68571  ],\n...\n        [-127.28055  , -126.49218  , -125.74938  , ...,  -71.24183  ,\n          -79.70399  , -133.6834   ],\n        [-126.68684  , -125.9416   , -125.15563  , ..., -114.46243  ,\n         -107.83289  , -131.2433   ],\n        [-126.467865 , -125.800575 , -125.19381  , ..., -113.44105  ,\n         -120.908356 , -126.25626  ]],\n\n       [[  -0.9108308,    0.405311 ,    1.362003 , ...,  -42.883907 ,\n          -38.677364 ,  -36.70583  ],\n        [  -5.2671247,   -3.8980722,   -2.9317954, ...,  -26.818964 ,\n          -36.38267  ,   -2.3517277],\n        [  -9.45855  ,   -7.810347 ,   -6.7994595, ...,    9.280067 ,\n            2.028537 ,   22.812065 ],\n        ...,\n        [-115.06766  , -113.99908  , -113.05724  , ...,   26.856642 ,\n           10.176036 ,  -94.912285 ],\n        [-111.756325 , -110.610634 , -109.46108  , ...,  -81.22135  ,\n         -133.58519  , -158.10306  ],\n        [-108.30612  , -107.21555  , -106.26769  , ..., -140.94438  ,\n         -153.4189   , -151.60954  ]]], dtype=float32)so(time, lat, lon)float6436.55 36.54 36.54 ... 34.0 34.08long_name :Salinitystandard_name :sea_water_salinityunits :1e-3unit_long :Practical Salinity Unitvalid_min :1valid_max :28336cell_methods :area: mean_ChunkSizes :[  1   7 341 720]array([[[36.55201721, 36.54133606, 36.53675842, ...,         nan,\n                 nan,         nan],\n        [36.5459137 , 36.57948685, 36.59321976, ...,         nan,\n                 nan,         nan],\n        [36.62831497, 36.71834564, 36.75344086, ...,         nan,\n                 nan,         nan],\n        ...,\n        [35.49760437, 35.48692322, 35.45030212, ..., 32.78908539,\n         32.854702  , 32.9493103 ],\n        [35.4884491 , 35.43046188, 35.37553024, ..., 34.19446564,\n         34.28754425, 34.29059601],\n        [35.39994431, 35.37400436, 35.36179733, ..., 33.71685028,\n         33.60850906, 33.68022537]],\n\n       [[36.5459137 , 36.53370667, 36.51997375, ...,         nan,\n                 nan,         nan],\n        [36.55659485, 36.57033157, 36.57338333, ...,         nan,\n                 nan,         nan],\n        [36.62373734, 36.71987152, 36.76717377, ...,         nan,\n                 nan,         nan],\n...\n        [35.06271744, 35.02761841, 34.97268677, ..., 33.4818573 ,\n         33.47270203, 33.48796082],\n        [35.18326187, 35.16342545, 35.1084938 , ..., 33.60698318,\n         33.68175125, 33.7733078 ],\n        [35.24582672, 35.19089508, 35.13595963, ..., 33.75041962,\n         33.87859726, 33.99151611]],\n\n       [[36.590168  , 36.59474564, 36.58864212, ...,         nan,\n                 nan,         nan],\n        [36.56117249, 36.57643509, 36.59321976, ...,         nan,\n                 nan,         nan],\n        [36.7000351 , 36.61000443, 36.57796097, ...,         nan,\n                 nan,         nan],\n        ...,\n        [35.08407974, 35.06729507, 35.01541138, ..., 33.4299736 ,\n         33.41471481, 33.44981384],\n        [35.21378326, 35.19547272, 35.14206314, ..., 33.6420784 ,\n         33.72447968, 33.8465538 ],\n        [35.24277496, 35.18326187, 35.13290787, ..., 33.8831749 ,\n         34.00067139, 34.08307266]]])Indexes: (3)timePandasIndexPandasIndex(DatetimeIndex(['2003-01-01', '2003-01-02', '2003-01-03', '2003-01-04',\n               '2003-01-05', '2003-01-06', '2003-01-07', '2003-01-08',\n               '2003-01-09', '2003-01-10', '2003-01-11', '2003-01-12',\n               '2003-01-13', '2003-01-14', '2003-01-15', '2003-01-16',\n               '2003-01-17', '2003-01-18', '2003-01-19', '2003-01-20',\n               '2003-01-21', '2003-01-22', '2003-01-23', '2003-01-24',\n               '2003-01-25', '2003-01-26', '2003-01-27', '2003-01-28',\n               '2003-01-29', '2003-01-30', '2003-01-31', '2003-02-01',\n               '2003-02-02', '2003-02-03', '2003-02-04', '2003-02-05',\n               '2003-02-06', '2003-02-07', '2003-02-08', '2003-02-09',\n               '2003-02-10', '2003-02-11', '2003-02-12', '2003-02-13',\n               '2003-02-14', '2003-02-15', '2003-02-16', '2003-02-17',\n               '2003-02-18', '2003-02-19', '2003-02-20', '2003-02-21',\n               '2003-02-22', '2003-02-23', '2003-02-24', '2003-02-25',\n               '2003-02-26', '2003-02-27', '2003-02-28'],\n              dtype='datetime64[ns]', name='time', freq=None))latPandasIndexPandasIndex(Index([ 25.0, 24.75,  24.5, 24.25,  24.0, 23.75,  23.5, 23.25,  23.0, 22.75,\n        22.5, 22.25,  22.0, 21.75,  21.5, 21.25,  21.0, 20.75,  20.5, 20.25,\n        20.0, 19.75,  19.5, 19.25,  19.0, 18.75,  18.5, 18.25,  18.0, 17.75,\n        17.5, 17.25,  17.0, 16.75,  16.5, 16.25,  16.0, 15.75,  15.5, 15.25,\n        15.0, 14.75,  14.5, 14.25,  14.0, 13.75,  13.5, 13.25,  13.0, 12.75,\n        12.5, 12.25,  12.0, 11.75,  11.5, 11.25,  11.0, 10.75,  10.5, 10.25,\n        10.0,  9.75,   9.5,  9.25,   9.0,  8.75,   8.5,  8.25,   8.0,  7.75,\n         7.5,  7.25,   7.0,  6.75,   6.5,  6.25,   6.0,  5.75,   5.5,  5.25,\n         5.0],\n      dtype='float32', name='lat'))lonPandasIndexPandasIndex(Index([ 60.0, 60.25,  60.5, 60.75,  61.0, 61.25,  61.5, 61.75,  62.0, 62.25,\n        62.5, 62.75,  63.0, 63.25,  63.5, 63.75,  64.0, 64.25,  64.5, 64.75,\n        65.0, 65.25,  65.5, 65.75,  66.0, 66.25,  66.5, 66.75,  67.0, 67.25,\n        67.5, 67.75,  68.0, 68.25,  68.5, 68.75,  69.0, 69.25,  69.5, 69.75,\n        70.0, 70.25,  70.5, 70.75,  71.0, 71.25,  71.5, 71.75,  72.0, 72.25,\n        72.5, 72.75,  73.0, 73.25,  73.5, 73.75,  74.0, 74.25,  74.5, 74.75,\n        75.0, 75.25,  75.5, 75.75,  76.0, 76.25,  76.5, 76.75,  77.0, 77.25,\n        77.5, 77.75,  78.0, 78.25,  78.5, 78.75,  79.0, 79.25,  79.5, 79.75,\n        80.0],\n      dtype='float32', name='lon'))Attributes: (3)source :Reanalysisinstitution :ECMWFtilte :ERA5 forecasts"
  },
  {
    "objectID": "python-notebooks/04_Combining_Data.html#adding-metadata-and-rename-variables",
    "href": "python-notebooks/04_Combining_Data.html#adding-metadata-and-rename-variables",
    "title": "Combine, clean, and export the data",
    "section": "Adding metadata and rename variables",
    "text": "Adding metadata and rename variables\nMetadata is one aspect we also need to address. Correct metadata is vital especially when we want to share our final dataset for others to use, or when we want to graph and feed data for other tools to work on. As we combine and process variables into the final dataset, some of the metadata may be lost, so we need to make sure that their metadata is reserved and resolve any conflict for the export process to proceed smoothly.\n\nfinal_ds['speed'].attrs = {\n    'units': 'm s**-1',\n    'long_name': '10 metre absolute speed'\n}\n\nfinal_ds['direction'].attrs = {\n    'units': '¬∞C',\n    'long_name': '10 metre wind direction'\n}\n\n\nfinal_ds.time.attrs = salinity.time.attrs\n\n\n# you do not need to add everything in here, but it's an example template\n\nfinal_ds.attrs = {\n    'title': 'Sample of Climate Data for Coastal Upwelling Machine Learning Project in Indian Ocean',\n    'summary': \"Daily mean of 0.25 x 0.25 degrees gridded data from multiple climate variables that may influence the patterns of coastal upwelling in the focused area\",\n    'creator_name': 'Minh Phan',\n    'creator_email': 'minhphan@uw.edu',\n    'creator_type': 'person',\n    'source': 'OSCAR, ERA5 Reanalysis, Copernicus Climate Change Service (C3S), Copernicus Marine Environment Monitoring Service (CMEMS)',\n    'geospatial_lat_min': float(final_ds.lat.min().values),\n    'geospatial_lat_max': float(final_ds.lat.max().values),\n    'geospatial_lat_units': 'degrees_north',\n    'geospatial_lat_resolution': 0.25,\n    'geospatial_lon_min': float(final_ds.lon.min().values),\n    'geospatial_lon_max': float(final_ds.lon.max().values),\n    'geospatial_lon_units': 'degrees_east',\n    'geospatial_lon_resolution': 0.25,\n    'time_coverage_start': '2000-01-01T00:00:00',\n    'time_coverage_end': '2002-12-31T23:59:59',\n    'date_created': datetime.today().strftime('%Y-%d-%d')\n}\n\n\nfinal_ds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:       (time: 59, lat: 81, lon: 81)\nCoordinates:\n  * time          (time) datetime64[ns] 2003-01-01 2003-01-02 ... 2003-02-28\n  * lat           (lat) float32 25.0 24.75 24.5 24.25 24.0 ... 5.75 5.5 5.25 5.0\n  * lon           (lon) float32 60.0 60.25 60.5 60.75 ... 79.25 79.5 79.75 80.0\nData variables:\n    analysed_sst  (time, lat, lon) float32 dask.array&lt;chunksize=(59, 81, 81), meta=np.ndarray&gt;\n    u_wind        (time, lat, lon) float32 3.128 2.865 2.555 ... -3.737 -4.479\n    v_wind        (time, lat, lon) float32 0.6016 0.6589 ... -1.914 -2.474\n    speed         (time, lat, lon) float32 3.945 3.645 3.368 ... 4.227 5.153\n    direction     (time, lat, lon) float32 22.31 17.58 13.01 ... -153.4 -151.6\n    so            (time, lat, lon) float64 36.55 36.54 36.54 ... 34.0 34.08\nAttributes: (12/17)\n    title:                      Sample of Climate Data for Coastal Upwelling ...\n    summary:                    Daily mean of 0.25 x 0.25 degrees gridded dat...\n    creator_name:               Minh Phan\n    creator_email:              minhphan@uw.edu\n    creator_type:               person\n    source:                     OSCAR, ERA5 Reanalysis, Copernicus Climate Ch...\n    ...                         ...\n    geospatial_lon_max:         80.0\n    geospatial_lon_units:       degrees_east\n    geospatial_lon_resolution:  0.25\n    time_coverage_start:        2000-01-01T00:00:00\n    time_coverage_end:          2002-12-31T23:59:59\n    date_created:               2023-14-14xarray.DatasetDimensions:time: 59lat: 81lon: 81Coordinates: (3)time(time)datetime64[ns]2003-01-01 ... 2003-02-28array(['2003-01-01T00:00:00.000000000', '2003-01-02T00:00:00.000000000',\n       '2003-01-03T00:00:00.000000000', '2003-01-04T00:00:00.000000000',\n       '2003-01-05T00:00:00.000000000', '2003-01-06T00:00:00.000000000',\n       '2003-01-07T00:00:00.000000000', '2003-01-08T00:00:00.000000000',\n       '2003-01-09T00:00:00.000000000', '2003-01-10T00:00:00.000000000',\n       '2003-01-11T00:00:00.000000000', '2003-01-12T00:00:00.000000000',\n       '2003-01-13T00:00:00.000000000', '2003-01-14T00:00:00.000000000',\n       '2003-01-15T00:00:00.000000000', '2003-01-16T00:00:00.000000000',\n       '2003-01-17T00:00:00.000000000', '2003-01-18T00:00:00.000000000',\n       '2003-01-19T00:00:00.000000000', '2003-01-20T00:00:00.000000000',\n       '2003-01-21T00:00:00.000000000', '2003-01-22T00:00:00.000000000',\n       '2003-01-23T00:00:00.000000000', '2003-01-24T00:00:00.000000000',\n       '2003-01-25T00:00:00.000000000', '2003-01-26T00:00:00.000000000',\n       '2003-01-27T00:00:00.000000000', '2003-01-28T00:00:00.000000000',\n       '2003-01-29T00:00:00.000000000', '2003-01-30T00:00:00.000000000',\n       '2003-01-31T00:00:00.000000000', '2003-02-01T00:00:00.000000000',\n       '2003-02-02T00:00:00.000000000', '2003-02-03T00:00:00.000000000',\n       '2003-02-04T00:00:00.000000000', '2003-02-05T00:00:00.000000000',\n       '2003-02-06T00:00:00.000000000', '2003-02-07T00:00:00.000000000',\n       '2003-02-08T00:00:00.000000000', '2003-02-09T00:00:00.000000000',\n       '2003-02-10T00:00:00.000000000', '2003-02-11T00:00:00.000000000',\n       '2003-02-12T00:00:00.000000000', '2003-02-13T00:00:00.000000000',\n       '2003-02-14T00:00:00.000000000', '2003-02-15T00:00:00.000000000',\n       '2003-02-16T00:00:00.000000000', '2003-02-17T00:00:00.000000000',\n       '2003-02-18T00:00:00.000000000', '2003-02-19T00:00:00.000000000',\n       '2003-02-20T00:00:00.000000000', '2003-02-21T00:00:00.000000000',\n       '2003-02-22T00:00:00.000000000', '2003-02-23T00:00:00.000000000',\n       '2003-02-24T00:00:00.000000000', '2003-02-25T00:00:00.000000000',\n       '2003-02-26T00:00:00.000000000', '2003-02-27T00:00:00.000000000',\n       '2003-02-28T00:00:00.000000000'], dtype='datetime64[ns]')lat(lat)float3225.0 24.75 24.5 ... 5.5 5.25 5.0standard_name :latitudelong_name :latitudeunits :degrees_northarray([25.  , 24.75, 24.5 , 24.25, 24.  , 23.75, 23.5 , 23.25, 23.  , 22.75,\n       22.5 , 22.25, 22.  , 21.75, 21.5 , 21.25, 21.  , 20.75, 20.5 , 20.25,\n       20.  , 19.75, 19.5 , 19.25, 19.  , 18.75, 18.5 , 18.25, 18.  , 17.75,\n       17.5 , 17.25, 17.  , 16.75, 16.5 , 16.25, 16.  , 15.75, 15.5 , 15.25,\n       15.  , 14.75, 14.5 , 14.25, 14.  , 13.75, 13.5 , 13.25, 13.  , 12.75,\n       12.5 , 12.25, 12.  , 11.75, 11.5 , 11.25, 11.  , 10.75, 10.5 , 10.25,\n       10.  ,  9.75,  9.5 ,  9.25,  9.  ,  8.75,  8.5 ,  8.25,  8.  ,  7.75,\n        7.5 ,  7.25,  7.  ,  6.75,  6.5 ,  6.25,  6.  ,  5.75,  5.5 ,  5.25,\n        5.  ], dtype=float32)lon(lon)float3260.0 60.25 60.5 ... 79.5 79.75 80.0standard_name :longitudelong_name :longitudeunits :degrees_eastarray([60.  , 60.25, 60.5 , 60.75, 61.  , 61.25, 61.5 , 61.75, 62.  , 62.25,\n       62.5 , 62.75, 63.  , 63.25, 63.5 , 63.75, 64.  , 64.25, 64.5 , 64.75,\n       65.  , 65.25, 65.5 , 65.75, 66.  , 66.25, 66.5 , 66.75, 67.  , 67.25,\n       67.5 , 67.75, 68.  , 68.25, 68.5 , 68.75, 69.  , 69.25, 69.5 , 69.75,\n       70.  , 70.25, 70.5 , 70.75, 71.  , 71.25, 71.5 , 71.75, 72.  , 72.25,\n       72.5 , 72.75, 73.  , 73.25, 73.5 , 73.75, 74.  , 74.25, 74.5 , 74.75,\n       75.  , 75.25, 75.5 , 75.75, 76.  , 76.25, 76.5 , 76.75, 77.  , 77.25,\n       77.5 , 77.75, 78.  , 78.25, 78.5 , 78.75, 79.  , 79.25, 79.5 , 79.75,\n       80.  ], dtype=float32)Data variables: (6)analysed_sst(time, lat, lon)float32dask.array&lt;chunksize=(59, 81, 81), meta=np.ndarray&gt;long_name :analysed sea surface temperaturestandard_name :sea_surface_foundation_temperaturecoverage_content_type :physicalMeasurementunits :kelvinvalid_min :-32767valid_max :32767comment :\"Final\" version using Multi-Resolution Variational Analysis (MRVA) method for interpolationsource :MODIS_T-JPL, MODIS_A-JPL, AMSRE-REMSS, AVHRR17_G-ACSPO, iQUAM-NOAA/NESDIS, Ice_Conc-OSISAF\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.48 MiB\n1.48 MiB\n\n\nShape\n(59, 81, 81)\n(59, 81, 81)\n\n\nDask graph\n1 chunks in 25 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\nu_wind(time, lat, lon)float323.128 2.865 2.555 ... -3.737 -4.479standard_name :eastward_windunits :m s**-1long_name :10 metre U wind componentnameECMWF :10 metre U wind componentshortNameECMWF :10unameCDM :10_metre_U_wind_component_surfaceproduct_type :analysisarray([[[ 3.1276042 ,  2.8645833 ,  2.5546875 , ...,  0.8776042 ,\n          0.9036458 ,  0.8072917 ],\n        [ 3.1223958 ,  2.8854167 ,  2.5703125 , ...,  0.6015625 ,\n          0.6380208 ,  0.6848958 ],\n        [ 2.5026042 ,  2.2864583 ,  2.0598958 , ...,  0.3828125 ,\n          0.390625  ,  0.45833334],\n        ...,\n        [-2.7916667 , -2.8932292 , -3.0182292 , ...,  0.04947917,\n          0.21614583,  0.23177083],\n        [-2.6380208 , -2.7317708 , -2.8567708 , ..., -0.02604167,\n          0.06510416,  0.05729167],\n        [-2.53125   , -2.6354167 , -2.7682292 , ..., -0.19270833,\n         -0.09114584, -0.07291666]],\n\n       [[-0.41666666, -0.7369792 , -1.0208334 , ..., -1.4401041 ,\n         -1.5677084 , -1.5677084 ],\n        [ 0.12239584, -0.13541667, -0.39583334, ..., -1.4348959 ,\n         -1.453125  , -1.3541666 ],\n        [ 0.6953125 ,  0.5234375 ,  0.44791666, ..., -1.3515625 ,\n         -1.2317709 , -1.140625  ],\n...\n        [-3.0833333 , -3.0286458 , -2.9739583 , ..., -0.8567708 ,\n         -1.21875   , -1.7447916 ],\n        [-3.0364583 , -2.9973958 , -2.9401042 , ..., -0.9322917 ,\n         -1.3854166 , -1.9401041 ],\n        [-3.0026042 , -2.9739583 , -2.9322917 , ..., -1.0442709 ,\n         -1.4401041 , -1.8255209 ]],\n\n       [[ 8.1119795 ,  8.020833  ,  7.950521  , ..., -1.0338541 ,\n         -1.0494791 , -1.0703125 ],\n        [ 7.78125   ,  7.7890625 ,  7.734375  , ..., -0.7942708 ,\n         -0.7135417 , -0.6015625 ],\n        [ 7.205729  ,  7.294271  ,  7.325521  , ..., -0.5026042 ,\n         -0.38020834, -0.29166666],\n        ...,\n        [-1.4270834 , -1.3619791 , -1.3125    , ..., -1.6119791 ,\n         -2.5520833 , -3.8203125 ],\n        [-1.2604166 , -1.2005209 , -1.1328125 , ..., -2.3958333 ,\n         -3.390625  , -4.5390625 ],\n        [-1.0625    , -1.0208334 , -0.9739583 , ..., -2.8697917 ,\n         -3.7369792 , -4.4791665 ]]], dtype=float32)v_wind(time, lat, lon)float320.6016 0.6589 ... -1.914 -2.474standard_name :northward_windunits :m s**-1long_name :10 metre V wind componentnameECMWF :10 metre V wind componentshortNameECMWF :10vnameCDM :10_metre_V_wind_component_surfaceproduct_type :analysisarray([[[ 6.0156250e-01,  6.5885419e-01,  6.0677081e-01, ...,\n         -1.3671875e+00, -1.1848959e+00, -8.8020831e-01],\n        [ 1.0729166e+00,  1.1536459e+00,  1.0807291e+00, ...,\n         -1.4791666e+00, -1.3463541e+00, -1.1796875e+00],\n        [ 1.1510416e+00,  1.2838541e+00,  1.3229166e+00, ...,\n         -1.5963541e+00, -1.4869791e+00, -1.3489584e+00],\n        ...,\n        [-3.6770833e+00, -3.8229167e+00, -3.9713542e+00, ...,\n         -2.3437500e+00, -1.3020834e+00, -3.7760416e-01],\n        [-3.7083333e+00, -3.8255208e+00, -3.9088542e+00, ...,\n         -2.3593750e+00, -1.5390625e+00, -7.4739581e-01],\n        [-3.7395833e+00, -3.7526042e+00, -3.7812500e+00, ...,\n         -2.4817708e+00, -1.6953125e+00, -1.1484375e+00]],\n\n       [[-1.5208334e+00, -1.7005209e+00, -1.9192709e+00, ...,\n         -3.9062500e-01, -4.2708334e-01, -4.9218750e-01],\n        [-1.9010416e+00, -1.9583334e+00, -2.1458333e+00, ...,\n         -5.0260419e-01, -5.8593750e-01, -5.0260419e-01],\n        [-2.2838542e+00, -2.2473958e+00, -2.2109375e+00, ...,\n         -4.9479166e-01, -6.6145831e-01, -6.0937500e-01],\n...\n        [-4.0729165e+00, -4.1276040e+00, -4.1770835e+00, ...,\n         -2.1171875e+00, -1.6562500e+00, -1.3151041e+00],\n        [-4.0885415e+00, -4.1484375e+00, -4.2005210e+00, ...,\n         -2.4244792e+00, -2.1119792e+00, -1.8515625e+00],\n        [-4.0625000e+00, -4.1197915e+00, -4.1614585e+00, ...,\n         -2.7031250e+00, -2.4166667e+00, -2.2708333e+00]],\n\n       [[-1.9791667e-01,  5.2083335e-03,  1.5625000e-01, ...,\n         -1.2760417e-01, -1.6145833e-01, -1.6927083e-01],\n        [-7.9687500e-01, -5.9895831e-01, -4.3750000e-01, ...,\n          5.4687500e-02,  4.6875000e-02,  1.5364583e-01],\n        [-1.2994791e+00, -1.0937500e+00, -9.4010419e-01, ...,\n          2.2395833e-01,  2.9166666e-01,  3.8020834e-01],\n        ...,\n        [-3.0625000e+00, -3.0781250e+00, -3.1197917e+00, ...,\n          5.4687500e-01,  1.2500000e-01, -8.0729169e-01],\n        [-3.1614583e+00, -3.2005208e+00, -3.2291667e+00, ...,\n         -4.1666666e-01, -1.0598959e+00, -1.8958334e+00],\n        [-3.2526042e+00, -3.3255208e+00, -3.3750000e+00, ...,\n         -1.3333334e+00, -1.9140625e+00, -2.4739583e+00]]], dtype=float32)speed(time, lat, lon)float323.945 3.645 3.368 ... 4.227 5.153units :m s**-1long_name :10 metre absolute speedarray([[[3.9451437, 3.6454449, 3.36847  , ..., 1.9201735, 1.8865218,\n         1.7468839],\n        [4.1419544, 3.8504055, 3.444007 , ..., 1.9789158, 1.9153827,\n         1.8310242],\n        [3.6950111, 3.4435928, 3.1019478, ..., 1.9867468, 1.9142274,\n         1.8317447],\n        ...,\n        [4.7469373, 4.912411 , 5.092304 , ..., 2.5135314, 1.6478343,\n         1.3388939],\n        [4.676735 , 4.8191776, 4.95795  , ..., 2.5241547, 1.7888355,\n         1.2424096],\n        [4.636933 , 4.7195234, 4.826424 , ..., 2.6338756, 1.8901653,\n         1.4170161]],\n\n       [[2.3318148, 2.4910347, 2.7374954, ..., 1.7411982, 1.8597933,\n         1.8728681],\n        [2.414438 , 2.5525272, 2.7947886, ..., 1.7888421, 1.8435212,\n         1.7328191],\n        [2.7723866, 2.8588107, 2.8829317, ..., 1.7411242, 1.7196132,\n         1.6104554],\n...\n        [5.146253 , 5.1557384, 5.1632495, ..., 2.8367107, 2.591439 ,\n         2.534239 ],\n        [5.1336794, 5.1560197, 5.166705 , ..., 3.0493386, 2.972432 ,\n         3.014616 ],\n        [5.0962915, 5.1267576, 5.1389093, ..., 3.263656 , 3.196616 ,\n         3.23878  ]],\n\n       [[8.189649 , 8.106084 , 8.037755 , ..., 2.1850507, 2.2752473,\n         2.2316103],\n        [7.892883 , 7.890354 , 7.823316 , ..., 2.3158002, 2.3716338,\n         2.317237 ],\n        [7.391702 , 7.4489727, 7.4578147, ..., 2.337489 , 2.231067 ,\n         2.291389 ],\n        ...,\n        [3.4753053, 3.467406 , 3.4899578, ..., 1.9244485, 2.6942132,\n         4.00572  ],\n        [3.5153275, 3.533966 , 3.542756 , ..., 2.549701 , 3.6089706,\n         4.9639993],\n        [3.5520408, 3.6114044, 3.6468   , ..., 3.2160883, 4.226849 ,\n         5.1525497]]], dtype=float32)direction(time, lat, lon)float3222.31 17.58 13.01 ... -153.4 -151.6units :¬∞Clong_name :10 metre wind directionarray([[[  22.31256  ,   17.577126 ,   13.005119 , ...,  -67.38259  ,\n          -47.755325 ,  -33.74233  ],\n        [  30.220772 ,   29.43544  ,   26.357203 , ...,  -78.10914  ,\n          -64.685646 ,  -49.11764  ],\n        [  35.783054 ,   37.048702 ,   35.60695  , ...,  -85.95185  ,\n          -86.35613  ,  -70.15734  ],\n        ...,\n        [-126.93241  , -126.620476 , -126.65524  , ...,  -89.24673  ,\n          -80.60198  ,   -4.201843 ],\n        [-125.02897  , -124.928856 , -125.43902  , ...,  -90.54617  ,\n          -84.9731   ,  -69.18275  ],\n        [-123.4584   , -124.29117  , -125.278015 , ...,  -93.996056 ,\n          -90.138794 ,  -88.29269  ]],\n\n       [[ -72.070206 ,  -85.1071   ,  -93.38763  , ...,  -58.74728  ,\n          -60.06917  ,  -58.698284 ],\n        [ -80.372795 ,  -90.8068   ,  -96.67983  , ...,  -58.435246 ,\n          -56.67389  ,  -58.751236 ],\n        [ -80.680115 ,  -86.12191  ,  -88.40796  , ...,  -59.58118  ,\n          -53.925976 ,  -70.68571  ],\n...\n        [-127.28055  , -126.49218  , -125.74938  , ...,  -71.24183  ,\n          -79.70399  , -133.6834   ],\n        [-126.68684  , -125.9416   , -125.15563  , ..., -114.46243  ,\n         -107.83289  , -131.2433   ],\n        [-126.467865 , -125.800575 , -125.19381  , ..., -113.44105  ,\n         -120.908356 , -126.25626  ]],\n\n       [[  -0.9108308,    0.405311 ,    1.362003 , ...,  -42.883907 ,\n          -38.677364 ,  -36.70583  ],\n        [  -5.2671247,   -3.8980722,   -2.9317954, ...,  -26.818964 ,\n          -36.38267  ,   -2.3517277],\n        [  -9.45855  ,   -7.810347 ,   -6.7994595, ...,    9.280067 ,\n            2.028537 ,   22.812065 ],\n        ...,\n        [-115.06766  , -113.99908  , -113.05724  , ...,   26.856642 ,\n           10.176036 ,  -94.912285 ],\n        [-111.756325 , -110.610634 , -109.46108  , ...,  -81.22135  ,\n         -133.58519  , -158.10306  ],\n        [-108.30612  , -107.21555  , -106.26769  , ..., -140.94438  ,\n         -153.4189   , -151.60954  ]]], dtype=float32)so(time, lat, lon)float6436.55 36.54 36.54 ... 34.0 34.08long_name :Salinitystandard_name :sea_water_salinityunits :1e-3unit_long :Practical Salinity Unitvalid_min :1valid_max :28336cell_methods :area: mean_ChunkSizes :[  1   7 341 720]array([[[36.55201721, 36.54133606, 36.53675842, ...,         nan,\n                 nan,         nan],\n        [36.5459137 , 36.57948685, 36.59321976, ...,         nan,\n                 nan,         nan],\n        [36.62831497, 36.71834564, 36.75344086, ...,         nan,\n                 nan,         nan],\n        ...,\n        [35.49760437, 35.48692322, 35.45030212, ..., 32.78908539,\n         32.854702  , 32.9493103 ],\n        [35.4884491 , 35.43046188, 35.37553024, ..., 34.19446564,\n         34.28754425, 34.29059601],\n        [35.39994431, 35.37400436, 35.36179733, ..., 33.71685028,\n         33.60850906, 33.68022537]],\n\n       [[36.5459137 , 36.53370667, 36.51997375, ...,         nan,\n                 nan,         nan],\n        [36.55659485, 36.57033157, 36.57338333, ...,         nan,\n                 nan,         nan],\n        [36.62373734, 36.71987152, 36.76717377, ...,         nan,\n                 nan,         nan],\n...\n        [35.06271744, 35.02761841, 34.97268677, ..., 33.4818573 ,\n         33.47270203, 33.48796082],\n        [35.18326187, 35.16342545, 35.1084938 , ..., 33.60698318,\n         33.68175125, 33.7733078 ],\n        [35.24582672, 35.19089508, 35.13595963, ..., 33.75041962,\n         33.87859726, 33.99151611]],\n\n       [[36.590168  , 36.59474564, 36.58864212, ...,         nan,\n                 nan,         nan],\n        [36.56117249, 36.57643509, 36.59321976, ...,         nan,\n                 nan,         nan],\n        [36.7000351 , 36.61000443, 36.57796097, ...,         nan,\n                 nan,         nan],\n        ...,\n        [35.08407974, 35.06729507, 35.01541138, ..., 33.4299736 ,\n         33.41471481, 33.44981384],\n        [35.21378326, 35.19547272, 35.14206314, ..., 33.6420784 ,\n         33.72447968, 33.8465538 ],\n        [35.24277496, 35.18326187, 35.13290787, ..., 33.8831749 ,\n         34.00067139, 34.08307266]]])Indexes: (3)timePandasIndexPandasIndex(DatetimeIndex(['2003-01-01', '2003-01-02', '2003-01-03', '2003-01-04',\n               '2003-01-05', '2003-01-06', '2003-01-07', '2003-01-08',\n               '2003-01-09', '2003-01-10', '2003-01-11', '2003-01-12',\n               '2003-01-13', '2003-01-14', '2003-01-15', '2003-01-16',\n               '2003-01-17', '2003-01-18', '2003-01-19', '2003-01-20',\n               '2003-01-21', '2003-01-22', '2003-01-23', '2003-01-24',\n               '2003-01-25', '2003-01-26', '2003-01-27', '2003-01-28',\n               '2003-01-29', '2003-01-30', '2003-01-31', '2003-02-01',\n               '2003-02-02', '2003-02-03', '2003-02-04', '2003-02-05',\n               '2003-02-06', '2003-02-07', '2003-02-08', '2003-02-09',\n               '2003-02-10', '2003-02-11', '2003-02-12', '2003-02-13',\n               '2003-02-14', '2003-02-15', '2003-02-16', '2003-02-17',\n               '2003-02-18', '2003-02-19', '2003-02-20', '2003-02-21',\n               '2003-02-22', '2003-02-23', '2003-02-24', '2003-02-25',\n               '2003-02-26', '2003-02-27', '2003-02-28'],\n              dtype='datetime64[ns]', name='time', freq=None))latPandasIndexPandasIndex(Index([ 25.0, 24.75,  24.5, 24.25,  24.0, 23.75,  23.5, 23.25,  23.0, 22.75,\n        22.5, 22.25,  22.0, 21.75,  21.5, 21.25,  21.0, 20.75,  20.5, 20.25,\n        20.0, 19.75,  19.5, 19.25,  19.0, 18.75,  18.5, 18.25,  18.0, 17.75,\n        17.5, 17.25,  17.0, 16.75,  16.5, 16.25,  16.0, 15.75,  15.5, 15.25,\n        15.0, 14.75,  14.5, 14.25,  14.0, 13.75,  13.5, 13.25,  13.0, 12.75,\n        12.5, 12.25,  12.0, 11.75,  11.5, 11.25,  11.0, 10.75,  10.5, 10.25,\n        10.0,  9.75,   9.5,  9.25,   9.0,  8.75,   8.5,  8.25,   8.0,  7.75,\n         7.5,  7.25,   7.0,  6.75,   6.5,  6.25,   6.0,  5.75,   5.5,  5.25,\n         5.0],\n      dtype='float32', name='lat'))lonPandasIndexPandasIndex(Index([ 60.0, 60.25,  60.5, 60.75,  61.0, 61.25,  61.5, 61.75,  62.0, 62.25,\n        62.5, 62.75,  63.0, 63.25,  63.5, 63.75,  64.0, 64.25,  64.5, 64.75,\n        65.0, 65.25,  65.5, 65.75,  66.0, 66.25,  66.5, 66.75,  67.0, 67.25,\n        67.5, 67.75,  68.0, 68.25,  68.5, 68.75,  69.0, 69.25,  69.5, 69.75,\n        70.0, 70.25,  70.5, 70.75,  71.0, 71.25,  71.5, 71.75,  72.0, 72.25,\n        72.5, 72.75,  73.0, 73.25,  73.5, 73.75,  74.0, 74.25,  74.5, 74.75,\n        75.0, 75.25,  75.5, 75.75,  76.0, 76.25,  76.5, 76.75,  77.0, 77.25,\n        77.5, 77.75,  78.0, 78.25,  78.5, 78.75,  79.0, 79.25,  79.5, 79.75,\n        80.0],\n      dtype='float32', name='lon'))Attributes: (17)title :Sample of Climate Data for Coastal Upwelling Machine Learning Project in Indian Oceansummary :Daily mean of 0.25 x 0.25 degrees gridded data from multiple climate variables that may influence the patterns of coastal upwelling in the focused areacreator_name :Minh Phancreator_email :minhphan@uw.educreator_type :personsource :OSCAR, ERA5 Reanalysis, Copernicus Climate Change Service (C3S), Copernicus Marine Environment Monitoring Service (CMEMS)geospatial_lat_min :5.0geospatial_lat_max :25.0geospatial_lat_units :degrees_northgeospatial_lat_resolution :0.25geospatial_lon_min :60.0geospatial_lon_max :80.0geospatial_lon_units :degrees_eastgeospatial_lon_resolution :0.25time_coverage_start :2000-01-01T00:00:00time_coverage_end :2002-12-31T23:59:59date_created :2023-14-14"
  },
  {
    "objectID": "python-notebooks/04_Combining_Data.html#adding-consistency-across-variables-convert-data-types-rechunking",
    "href": "python-notebooks/04_Combining_Data.html#adding-consistency-across-variables-convert-data-types-rechunking",
    "title": "Combine, clean, and export the data",
    "section": "Adding consistency across variables: convert data types, rechunking",
    "text": "Adding consistency across variables: convert data types, rechunking\nNote that some variables has float64 dtype, while others have float32. We want to add consistency by converting all of them to float32 to save some disk space. The precision from float64 is most likely not going to be lost after the conversion, as our figures do not have enough precision to exceed the limit.\n\nfor var in final_ds.data_vars:\n    if str(final_ds[var].dtype) == 'float64':\n        final_ds[var].values = final_ds[var].astype('float32')\n\nWe also need to rechunk the dataset before we can export the data. Some of the variables have original chunk sizes, which is specified in their encodings, and we need to reset these values so that zarr would not reconvert the encoding when export is happening. This is a known issue.\n\nfor var in final_ds.data_vars:\n    if 'chunks' in list(final_ds[var].encoding.keys()):\n        del final_ds[var].encoding['chunks']\n\nFinally, reset chunk size for exporting to zarr:\n\narray_chunk_size = {'time': 100, 'lat': final_ds.lat.shape[0], 'lon': final_ds.lon.shape[0]}\nfinal_ds = final_ds.chunk(array_chunk_size)"
  },
  {
    "objectID": "python-notebooks/04_Combining_Data.html#export-data",
    "href": "python-notebooks/04_Combining_Data.html#export-data",
    "title": "Combine, clean, and export the data",
    "section": "Export data",
    "text": "Export data\n\nfinal_ds.to_zarr('demonstrated data/final-sample.zarr')\n\n&lt;xarray.backends.zarr.ZarrStore at 0x7fc73003ce40&gt;"
  },
  {
    "objectID": "r-tutorials/03-r-read-in-data.html#syntax-tips",
    "href": "r-tutorials/03-r-read-in-data.html#syntax-tips",
    "title": "R - Read in data",
    "section": "Syntax tips",
    "text": "Syntax tips\nxyz::fun() means function fun() in the xyz package. I will often use this when I don‚Äôt want to call library() to load the whole xyz package. Why? Because remembering to call library() slows me down and when I forget I get errors. Instead I tend to load the major packages (like library(tidyverse) and library(sf)) for which I need many functions."
  },
  {
    "objectID": "r-tutorials/03-r-read-in-data.html#file-not-found",
    "href": "r-tutorials/03-r-read-in-data.html#file-not-found",
    "title": "R - Read in data",
    "section": "File not found",
    "text": "File not found\nThe top error for new users is ‚ÄúFile not found.‚Äù You know they file is there, but R says ‚ÄúFile not found.‚Äù File locations are relative to your current location. Your current location if you are in a Quarto or Rmarkdown file is the folder with that file! This means that code that works on the command line, fails in a Quarto or RMarkdown document. Oh no!\nTry this in the Quarto document versus in the Console tab.\n\ngetwd()\n\nThis means that a normal file location can break:\n\n. Current location\n.. Back one folder\nfolder name Enter this folder\n~ Home directory\n\n./data/myfile.csv\nmeans current location &gt; data folder &gt; myfile.csv in that folder. But if your current location changes, that code breaks.\n\nSolution 1 - full file path\nAlways use the full file path:\n~/r-tutorials/data/myfile.csv\nHome &gt; r-tutorials folder &gt; data folder &gt; myfile.csv\nThis works until you move the r-tutorials folder somewhere else.\n\n\nSolution 2: the here package\nThis is the what experienced R users use. here() is the location of the base of your project. As long as you are in an RStudio project, it will return the base of that project.\nThis is what mine shows (I am on my laptop). It is the same inside a Quarto document, at the console and in scripts.\n\nhere::i_am(\"r-tutorials/03-r-read-in-data.qmd\")\n\nhere() starts at /Users/eli.holmes/Documents/GitHub/2023-Cloudbook\n\nhere::here()\n\n[1] \"/Users/eli.holmes/Documents/GitHub/2023-Cloudbook\"\n\n\nNow to specify a data file, I use\n\nfil &lt;- here::here(\"r-tutorials\", \"data\", \"myfile.csv\")\nfil\n\n[1] \"/Users/eli.holmes/Documents/GitHub/2023-Cloudbook/r-tutorials/data/myfile.csv\"\n\n\nThis fully specifies the file location.\n\n\nDon‚Äôt use setwd()\nThis changes your working directory and although it seems to help in the short-run, it causes all kinds of problems later."
  },
  {
    "objectID": "r-tutorials/03-r-read-in-data.html#import-a-csv-file",
    "href": "r-tutorials/03-r-read-in-data.html#import-a-csv-file",
    "title": "R - Read in data",
    "section": "Import a csv file",
    "text": "Import a csv file\nWe will use the Import Dataset widget in the Environment tab.\n\nClick the Environment tag and then Import Dataset\nChoose ‚ÄúFrom Text (readr)‚Äù\nBrowse to obis_seamap_green.csv in the data folder.\nYou should now see a preview of the data.\n\n\nOk first thing we see is that we have 2 lines at the top of our data file. Put 2 in the Skip box to skip past those. Now the data and column headers should look right.\nNotice that it also shows us the R code. We can copy that and run it directly if we wanted. But we can also just click the Import button.\n\nR Code\nThe import box helpfully shows us the R code to use. But this will break inside a Quarto file because the file path is wrong. This works in the Console or a script but not in a Quarto or RMarkdown file.\nlibrary(readr)\nobis_seamap_green &lt;- read_csv(\"r-tutorials/data/obis_seamap_green.csv\", \n    skip = 2)\nLet‚Äôs make this code more robust:\n\nlibrary(readr)\nfil &lt;- here::here(\"r-tutorials\", \"data\", \"obis_seamap_green.csv\")\nobis_seamap_green &lt;- read_csv(fil, \n    skip = 2)\n\nRows: 5356 Columns: 15\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (7): scientific, common, provider, obs_date, date_time, ds_type, platform\ndbl (8): oid, id, dataset_id, tsn, latitude, longitude, lprecision, count\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nShow the data\n\nobis_seamap_green\n\n# A tibble: 5,356 √ó 15\n       oid    id dataset_id    tsn scientific common provider latitude longitude\n     &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;  &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n 1  2.18e8   219       2069 173833 Chelonia ‚Ä¶ Green‚Ä¶ Nicolas‚Ä¶     24.6      53.0\n 2  2.18e8     1       2069 173833 Chelonia ‚Ä¶ Green‚Ä¶ Nicolas‚Ä¶     24.6      53.0\n 3  2.18e8   136       2069 173833 Chelonia ‚Ä¶ Green‚Ä¶ Nicolas‚Ä¶     24.6      53.1\n 4  2.18e8   258       2069 173833 Chelonia ‚Ä¶ Green‚Ä¶ Nicolas‚Ä¶     24.6      53.0\n 5  2.18e8   292       2069 173833 Chelonia ‚Ä¶ Green‚Ä¶ Nicolas‚Ä¶     24.6      53.0\n 6  2.18e8   259       2069 173833 Chelonia ‚Ä¶ Green‚Ä¶ Nicolas‚Ä¶     24.6      53.0\n 7  2.18e8   293       2069 173833 Chelonia ‚Ä¶ Green‚Ä¶ Nicolas‚Ä¶     24.6      53.0\n 8  2.18e8     2       2069 173833 Chelonia ‚Ä¶ Green‚Ä¶ Nicolas‚Ä¶     24.6      53.1\n 9  2.18e8   183       2069 173833 Chelonia ‚Ä¶ Green‚Ä¶ Nicolas‚Ä¶     24.6      53.1\n10  2.18e8   184       2069 173833 Chelonia ‚Ä¶ Green‚Ä¶ Nicolas‚Ä¶     24.6      53.1\n# ‚Ñπ 5,346 more rows\n# ‚Ñπ 6 more variables: lprecision &lt;dbl&gt;, obs_date &lt;chr&gt;, date_time &lt;chr&gt;,\n#   count &lt;dbl&gt;, ds_type &lt;chr&gt;, platform &lt;chr&gt;"
  },
  {
    "objectID": "r-tutorials/03-r-read-in-data.html#get-the-columns",
    "href": "r-tutorials/03-r-read-in-data.html#get-the-columns",
    "title": "R - Read in data",
    "section": "Get the columns",
    "text": "Get the columns\nColumn names.\n\ncolnames(obis_seamap_green)\n\n [1] \"oid\"        \"id\"         \"dataset_id\" \"tsn\"        \"scientific\"\n [6] \"common\"     \"provider\"   \"latitude\"   \"longitude\"  \"lprecision\"\n[11] \"obs_date\"   \"date_time\"  \"count\"      \"ds_type\"    \"platform\"  \n\n\nWe select a column with $ like so. Notice how RStudio suggest the names.\n\nobis_seamap_green$date_time\n\n   [1] \"5/16/16 6:54\"   \"5/16/16 6:54\"   \"5/16/16 6:55\"   \"5/16/16 6:56\"  \n   [5] \"5/16/16 6:56\"   \"5/17/16 5:40\"   \"5/17/16 6:32\"   \"5/17/16 11:30\" \n   [9] \"5/17/16 11:32\"  \"5/17/16 11:32\"  \"5/17/16 13:23\"  \"5/18/16 6:16\"  \n  [13] \"5/18/16 6:17\"   \"5/18/16 9:40\"   \"5/18/16 9:40\"   \"5/18/16 9:41\"  \n  [17] \"5/18/16 11:15\"  \"5/18/16 11:24\"  \"5/18/16 11:25\"  \"5/18/16 12:33\" \n  [21] \"5/18/16 22:12\"  \"5/18/16 23:54\"  \"5/19/16 6:38\"   \"5/19/16 11:09\" \n  [25] \"5/19/16 11:09\"  \"5/19/16 11:12\"  \"5/19/16 11:13\"  \"5/19/16 11:15\" \n  [29] \"5/19/16 12:32\"  \"5/19/16 12:38\"  \"5/19/16 22:03\"  \"5/19/16 22:03\" \n  [33] \"5/19/16 22:05\"  \"5/19/16 23:40\"  \"5/20/16 10:53\"  \"5/20/16 10:54\" \n  [37] \"5/20/16 10:59\"  \"5/20/16 10:59\"  \"5/20/16 12:39\"  \"5/20/16 12:42\" \n  [41] \"5/20/16 13:54\"  \"5/20/16 19:11\"  \"5/20/16 23:30\"  \"5/20/16 23:33\" \n  [45] \"5/20/16 23:36\"  \"5/21/16 6:51\"   \"5/21/16 10:41\"  \"5/21/16 10:41\" \n  [49] \"5/21/16 10:45\"  \"5/21/16 10:53\"  \"5/21/16 12:26\"  \"5/21/16 12:26\" \n  [53] \"5/21/16 15:03\"  \"5/21/16 23:15\"  \"5/21/16 23:20\"  \"5/21/16 23:26\" \n  [57] \"5/22/16 1:52\"   \"5/22/16 10:29\"  \"5/22/16 10:35\"  \"5/22/16 10:35\" \n  [61] \"5/22/16 10:37\"  \"5/22/16 12:14\"  \"5/22/16 12:59\"  \"5/22/16 15:10\" \n  [65] \"5/22/16 23:04\"  \"5/23/16 6:53\"   \"5/23/16 6:57\"   \"5/23/16 10:22\" \n  [69] \"5/23/16 10:25\"  \"5/23/16 10:30\"  \"5/23/16 10:30\"  \"5/23/16 12:06\" \n  [73] \"5/23/16 12:06\"  \"5/23/16 12:31\"  \"5/23/16 22:51\"  \"5/23/16 23:00\" \n  [77] \"5/24/16 5:01\"   \"5/24/16 11:50\"  \"5/24/16 11:50\"  \"5/24/16 11:52\" \n  [81] \"5/24/16 11:55\"  \"5/24/16 11:57\"  \"5/24/16 11:57\"  \"5/24/16 13:48\" \n  [85] \"5/24/16 22:44\"  \"5/24/16 22:47\"  \"5/24/16 22:48\"  \"5/25/16 6:18\"  \n  [89] \"5/25/16 11:40\"  \"5/25/16 11:41\"  \"5/25/16 11:41\"  \"5/25/16 11:42\" \n  [93] \"5/25/16 11:43\"  \"5/25/16 13:21\"  \"5/25/16 13:22\"  \"5/25/16 19:09\" \n  [97] \"5/25/16 22:30\"  \"5/25/16 22:33\"  \"5/26/16 6:51\"   \"5/26/16 9:52\"  \n [101] \"5/26/16 11:27\"  \"5/26/16 11:31\"  \"5/26/16 11:31\"  \"5/26/16 11:35\" \n [105] \"5/26/16 11:36\"  \"5/26/16 13:04\"  \"5/26/16 22:21\"  \"5/26/16 23:59\" \n [109] \"5/27/16 1:26\"   \"5/27/16 6:29\"   \"5/27/16 9:39\"   \"5/27/16 9:39\"  \n [113] \"5/27/16 11:14\"  \"5/27/16 11:18\"  \"5/27/16 12:30\"  \"5/27/16 13:57\" \n [117] \"5/27/16 14:15\"  \"5/27/16 22:12\"  \"5/27/16 23:48\"  \"5/27/16 23:51\" \n [121] \"5/28/16 5:12\"   \"5/28/16 6:48\"   \"5/28/16 9:31\"   \"5/28/16 11:12\" \n [125] \"5/28/16 11:15\"  \"5/28/16 11:15\"  \"5/28/16 13:51\"  \"5/28/16 13:54\" \n [129] \"5/28/16 13:54\"  \"5/28/16 23:37\"  \"5/28/16 23:37\"  \"5/29/16 6:31\"  \n [133] \"5/29/16 6:39\"   \"5/29/16 10:50\"  \"5/29/16 10:50\"  \"5/29/16 11:00\" \n [137] \"5/29/16 11:00\"  \"5/29/16 12:39\"  \"5/29/16 13:21\"  \"5/29/16 13:28\" \n [141] \"5/29/16 19:26\"  \"5/29/16 23:31\"  \"5/30/16 4:30\"   \"5/30/16 7:55\"  \n [145] \"5/30/16 10:40\"  \"5/30/16 10:40\"  \"5/30/16 10:43\"  \"5/30/16 10:43\" \n [149] \"5/30/16 12:56\"  \"5/30/16 13:36\"  \"5/30/16 23:14\"  \"5/30/16 23:16\" \n [153] \"5/30/16 23:20\"  \"5/31/16 1:10\"   \"5/31/16 2:46\"   \"5/31/16 5:53\"  \n [157] \"5/31/16 6:49\"   \"5/31/16 10:31\"  \"5/31/16 10:31\"  \"5/31/16 10:34\" \n [161] \"5/31/16 10:34\"  \"5/31/16 12:13\"  \"5/31/16 12:34\"  \"5/31/16 13:33\" \n [165] \"6/1/16 2:14\"    \"6/1/16 2:43\"    \"6/1/16 5:29\"    \"6/1/16 10:22\"  \n [169] \"6/1/16 10:25\"   \"6/1/16 10:25\"   \"6/1/16 12:05\"   \"6/1/16 13:13\"  \n [173] \"6/1/16 13:47\"   \"6/1/16 14:39\"   \"6/2/16 1:49\"    \"6/2/16 6:49\"   \n [177] \"6/2/16 10:08\"   \"6/2/16 10:08\"   \"6/2/16 10:12\"   \"6/2/16 11:48\"  \n [181] \"6/2/16 11:50\"   \"6/2/16 11:58\"   \"6/2/16 16:27\"   \"6/2/16 22:40\"  \n [185] \"6/2/16 22:42\"   \"6/2/16 22:50\"   \"6/3/16 0:23\"    \"6/3/16 6:35\"   \n [189] \"6/3/16 9:58\"    \"6/3/16 11:36\"   \"6/3/16 11:36\"   \"6/3/16 11:42\"  \n [193] \"6/3/16 12:49\"   \"6/3/16 12:57\"   \"6/3/16 22:29\"   \"6/3/16 22:29\"  \n [197] \"6/4/16 7:05\"    \"6/4/16 11:24\"   \"6/4/16 11:30\"   \"6/4/16 11:32\"  \n [201] \"6/4/16 22:17\"   \"6/4/16 22:17\"   \"6/4/16 22:23\"   \"6/5/16 3:29\"   \n [205] \"6/5/16 9:35\"    \"6/5/16 11:12\"   \"6/5/16 13:50\"   \"6/5/16 14:16\"  \n [209] \"6/5/16 22:11\"   \"6/5/16 23:45\"   \"6/5/16 23:47\"   \"6/6/16 5:30\"   \n [213] \"6/6/16 5:32\"    \"6/6/16 7:09\"    \"6/6/16 11:01\"   \"6/6/16 11:04\"  \n [217] \"6/6/16 11:07\"   \"6/6/16 23:38\"   \"6/7/16 5:55\"    \"6/7/16 6:48\"   \n [221] \"6/7/16 10:53\"   \"6/7/16 12:38\"   \"6/7/16 13:45\"   \"6/8/16 10:39\"  \n [225] \"6/8/16 10:42\"   \"6/8/16 23:15\"   \"6/9/16 10:32\"   \"6/9/16 12:15\"  \n [229] \"6/9/16 23:11\"   \"6/10/16 10:15\"  \"6/10/16 15:04\"  \"6/10/16 22:52\" \n [233] \"6/11/16 6:21\"   \"6/11/16 11:46\"  \"6/11/16 14:31\"  \"6/11/16 22:40\" \n [237] \"6/12/16 1:28\"   \"6/12/16 12:51\"  \"6/12/16 16:13\"  \"6/12/16 22:35\" \n [241] \"6/13/16 15:07\"  \"6/13/16 22:19\"  \"6/14/16 9:39\"   \"6/14/16 23:42\" \n [245] \"6/15/16 1:40\"   \"6/15/16 11:05\"  \"6/15/16 18:37\"  \"6/16/16 1:02\"  \n [249] \"6/16/16 7:01\"   \"6/16/16 13:45\"  \"6/17/16 3:51\"   \"6/17/16 5:49\"  \n [253] \"6/17/16 10:40\"  \"6/17/16 16:10\"  \"6/18/16 8:00\"   \"6/18/16 10:30\" \n [257] \"6/18/16 23:00\"  \"6/19/16 7:40\"   \"6/19/16 11:58\"  \"6/20/16 1:41\"  \n [261] \"6/20/16 11:50\"  \"6/21/16 1:03\"   \"6/21/16 12:44\"  \"8/1/16 13:11\"  \n [265] \"8/2/16 6:36\"    \"8/2/16 6:36\"    \"8/2/16 12:00\"   \"8/2/16 12:03\"  \n [269] \"8/2/16 13:06\"   \"8/2/16 22:53\"   \"8/2/16 22:54\"   \"8/3/16 2:45\"   \n [273] \"8/3/16 6:17\"    \"8/3/16 6:24\"    \"8/3/16 11:52\"   \"8/3/16 11:57\"  \n [277] \"8/3/16 11:58\"   \"8/3/16 12:40\"   \"8/3/16 14:26\"   \"8/3/16 14:28\"  \n [281] \"8/3/16 16:37\"   \"8/3/16 22:41\"   \"8/3/16 22:42\"   \"8/3/16 22:42\"  \n [285] \"8/3/16 22:43\"   \"8/4/16 6:44\"    \"8/4/16 6:48\"    \"8/4/16 10:00\"  \n [289] \"8/4/16 11:35\"   \"8/4/16 11:38\"   \"8/4/16 13:50\"   \"8/4/16 13:56\"  \n [293] \"8/4/16 14:16\"   \"8/4/16 14:33\"   \"8/4/16 22:28\"   \"8/4/16 22:29\"  \n [297] \"8/4/16 22:29\"   \"8/4/16 22:30\"   \"8/4/16 22:31\"   \"8/5/16 6:20\"   \n [301] \"8/5/16 11:22\"   \"8/5/16 11:24\"   \"8/5/16 11:29\"   \"8/5/16 11:33\"  \n [305] \"8/5/16 11:35\"   \"8/5/16 14:07\"   \"8/5/16 14:10\"   \"8/5/16 19:16\"  \n [309] \"8/5/16 22:19\"   \"8/5/16 22:20\"   \"8/5/16 22:20\"   \"8/5/16 23:56\"  \n [313] \"8/5/16 23:58\"   \"8/6/16 6:02\"    \"8/6/16 11:11\"   \"8/6/16 11:12\"  \n [317] \"8/6/16 11:16\"   \"8/6/16 11:16\"   \"8/6/16 13:01\"   \"8/6/16 13:06\"  \n [321] \"8/6/16 15:17\"   \"8/6/16 22:05\"   \"8/6/16 23:45\"   \"8/6/16 23:46\"  \n [325] \"8/6/16 23:46\"   \"8/6/16 23:49\"   \"8/6/16 23:56\"   \"8/7/16 4:11\"   \n [329] \"8/7/16 6:39\"    \"8/7/16 11:03\"   \"8/7/16 11:06\"   \"8/7/16 11:06\"  \n [333] \"8/7/16 12:40\"   \"8/7/16 12:41\"   \"8/7/16 13:42\"   \"8/7/16 14:19\"  \n [337] \"8/7/16 22:02\"   \"8/7/16 23:35\"   \"8/7/16 23:37\"   \"8/7/16 23:39\"  \n [341] \"8/7/16 23:41\"   \"8/8/16 2:48\"    \"8/8/16 6:15\"    \"8/8/16 6:16\"   \n [345] \"8/8/16 6:20\"    \"8/8/16 10:52\"   \"8/8/16 12:33\"   \"8/8/16 12:34\"  \n [349] \"8/8/16 13:27\"   \"8/8/16 13:29\"   \"8/8/16 13:32\"   \"8/8/16 15:50\"  \n [353] \"8/8/16 23:22\"   \"8/8/16 23:24\"   \"8/8/16 23:26\"   \"8/9/16 3:57\"   \n [357] \"8/9/16 6:44\"    \"8/9/16 10:41\"   \"8/9/16 10:46\"   \"8/9/16 12:21\"  \n [361] \"8/9/16 12:25\"   \"8/9/16 12:25\"   \"8/9/16 13:43\"   \"8/9/16 23:09\"  \n [365] \"8/9/16 23:10\"   \"8/9/16 23:16\"   \"8/9/16 23:17\"   \"8/10/16 10:35\" \n [369] \"8/10/16 12:11\"  \"8/10/16 12:12\"  \"8/10/16 13:01\"  \"8/10/16 13:10\" \n [373] \"8/10/16 19:12\"  \"8/10/16 22:59\"  \"8/10/16 22:59\"  \"8/10/16 23:03\" \n [377] \"8/10/16 23:09\"  \"8/11/16 5:56\"   \"8/11/16 6:59\"   \"8/11/16 6:59\"  \n [381] \"8/11/16 10:22\"  \"8/11/16 12:04\"  \"8/11/16 12:07\"  \"8/11/16 12:55\" \n [385] \"8/11/16 14:40\"  \"8/11/16 22:50\"  \"8/11/16 22:55\"  \"8/12/16 3:14\"  \n [389] \"8/12/16 6:33\"   \"8/12/16 6:36\"   \"8/12/16 6:36\"   \"8/12/16 10:07\" \n [393] \"8/12/16 11:54\"  \"8/12/16 13:46\"  \"8/12/16 13:47\"  \"8/12/16 13:59\" \n [397] \"8/12/16 22:39\"  \"8/13/16 2:54\"   \"8/13/16 6:12\"   \"8/13/16 11:35\" \n [401] \"8/13/16 11:42\"  \"8/13/16 11:42\"  \"8/13/16 13:16\"  \"8/13/16 13:18\" \n [405] \"8/13/16 14:57\"  \"8/13/16 22:27\"  \"8/13/16 22:27\"  \"8/13/16 22:30\" \n [409] \"8/13/16 22:34\"  \"8/14/16 11:25\"  \"8/14/16 11:26\"  \"8/14/16 13:02\" \n [413] \"8/14/16 14:40\"  \"8/14/16 15:44\"  \"8/14/16 22:13\"  \"8/14/16 22:16\" \n [417] \"8/14/16 22:19\"  \"8/14/16 23:56\"  \"8/14/16 23:58\"  \"8/15/16 5:29\"  \n [421] \"8/15/16 11:10\"  \"8/15/16 11:11\"  \"8/15/16 13:54\"  \"8/15/16 13:57\" \n [425] \"8/15/16 13:58\"  \"8/15/16 22:07\"  \"8/15/16 23:45\"  \"8/15/16 23:48\" \n [429] \"8/15/16 23:50\"  \"8/15/16 23:51\"  \"8/16/16 4:18\"   \"8/16/16 5:52\"  \n [433] \"8/16/16 6:04\"   \"8/16/16 11:00\"  \"8/16/16 11:02\"  \"8/16/16 11:06\" \n [437] \"8/16/16 12:14\"  \"8/16/16 13:20\"  \"8/16/16 15:20\"  \"8/16/16 23:33\" \n [441] \"8/16/16 23:36\"  \"8/16/16 23:38\"  \"8/17/16 2:18\"   \"8/17/16 5:39\"  \n [445] \"8/17/16 10:49\"  \"8/17/16 12:34\"  \"8/17/16 12:35\"  \"8/17/16 14:29\" \n [449] \"8/17/16 14:37\"  \"8/17/16 16:12\"  \"8/17/16 23:20\"  \"8/17/16 23:26\" \n [453] \"8/17/16 23:29\"  \"8/17/16 23:30\"  \"8/18/16 2:02\"   \"8/18/16 3:39\"  \n [457] \"8/18/16 6:53\"   \"8/18/16 10:38\"  \"8/18/16 12:23\"  \"8/18/16 12:25\" \n [461] \"8/18/16 12:25\"  \"8/18/16 13:16\"  \"8/18/16 13:19\"  \"8/18/16 23:10\" \n [465] \"8/18/16 23:10\"  \"8/19/16 5:45\"   \"8/19/16 6:31\"   \"8/19/16 10:33\" \n [469] \"8/19/16 12:13\"  \"8/19/16 12:39\"  \"8/19/16 12:40\"  \"8/19/16 13:31\" \n [473] \"8/19/16 14:15\"  \"8/19/16 22:56\"  \"8/19/16 23:03\"  \"8/19/16 23:05\" \n [477] \"8/19/16 23:06\"  \"8/20/16 11:56\"  \"8/20/16 11:56\"  \"8/20/16 11:57\" \n [481] \"8/20/16 11:59\"  \"8/20/16 12:06\"  \"8/20/16 12:52\"  \"8/20/16 12:55\" \n [485] \"8/20/16 19:05\"  \"8/20/16 19:06\"  \"8/20/16 19:08\"  \"8/20/16 22:50\" \n [489] \"8/20/16 22:50\"  \"8/20/16 22:51\"  \"8/20/16 22:53\"  \"8/21/16 1:40\"  \n [493] \"8/21/16 5:48\"   \"8/21/16 5:55\"   \"8/21/16 6:49\"   \"8/21/16 11:44\" \n [497] \"8/21/16 11:51\"  \"8/21/16 11:55\"  \"8/21/16 12:39\"  \"8/21/16 13:29\" \n [501] \"8/21/16 14:08\"  \"8/21/16 14:18\"  \"8/21/16 22:35\"  \"8/21/16 22:39\" \n [505] \"8/21/16 22:43\"  \"8/22/16 1:57\"   \"8/22/16 6:25\"   \"8/22/16 9:54\"  \n [509] \"8/22/16 11:33\"  \"8/22/16 11:33\"  \"8/22/16 11:35\"  \"8/22/16 11:36\" \n [513] \"8/22/16 11:39\"  \"8/22/16 13:33\"  \"8/22/16 22:25\"  \"8/22/16 22:26\" \n [517] \"8/22/16 22:28\"  \"8/22/16 22:30\"  \"8/22/16 22:31\"  \"8/23/16 6:48\"  \n [521] \"8/23/16 11:21\"  \"8/23/16 11:23\"  \"8/23/16 11:26\"  \"8/23/16 11:28\" \n [525] \"8/23/16 11:32\"  \"8/23/16 13:59\"  \"8/23/16 14:26\"  \"8/23/16 22:13\" \n [529] \"8/23/16 22:16\"  \"8/23/16 23:51\"  \"8/23/16 23:52\"  \"8/23/16 23:53\" \n [533] \"8/23/16 23:59\"  \"8/24/16 11:13\"  \"8/24/16 11:14\"  \"8/24/16 11:14\" \n [537] \"8/24/16 11:16\"  \"8/24/16 11:17\"  \"8/24/16 13:55\"  \"8/24/16 14:01\" \n [541] \"8/24/16 22:03\"  \"8/24/16 22:06\"  \"8/24/16 23:40\"  \"8/24/16 23:40\" \n [545] \"8/24/16 23:41\"  \"8/24/16 23:44\"  \"8/24/16 23:46\"  \"8/25/16 5:23\"  \n [549] \"8/25/16 6:09\"   \"8/25/16 10:56\"  \"8/25/16 11:00\"  \"8/25/16 11:02\" \n [553] \"8/25/16 11:02\"  \"8/25/16 13:27\"  \"8/25/16 13:28\"  \"8/25/16 15:09\" \n [557] \"8/25/16 23:32\"  \"8/25/16 23:34\"  \"8/25/16 23:36\"  \"8/25/16 23:41\" \n [561] \"8/25/16 23:41\"  \"8/26/16 5:47\"   \"8/26/16 10:44\"  \"8/26/16 12:31\" \n [565] \"8/26/16 12:33\"  \"8/26/16 12:33\"  \"8/26/16 12:33\"  \"8/26/16 12:33\" \n [569] \"8/26/16 13:05\"  \"8/26/16 21:41\"  \"8/26/16 23:17\"  \"8/26/16 23:18\" \n [573] \"8/26/16 23:18\"  \"8/26/16 23:18\"  \"8/26/16 23:25\"  \"8/27/16 1:29\"  \n [577] \"8/27/16 3:35\"   \"8/27/16 5:36\"   \"8/27/16 10:46\"  \"8/27/16 12:20\" \n [581] \"8/27/16 12:20\"  \"8/27/16 12:39\"  \"8/27/16 13:14\"  \"8/27/16 14:17\" \n [585] \"8/27/16 14:49\"  \"8/27/16 23:07\"  \"8/27/16 23:07\"  \"8/27/16 23:12\" \n [589] \"8/27/16 23:15\"  \"8/28/16 6:02\"   \"8/28/16 12:11\"  \"8/28/16 12:11\" \n [593] \"8/28/16 13:00\"  \"8/28/16 13:01\"  \"8/28/16 13:44\"  \"8/28/16 13:51\" \n [597] \"8/28/16 13:56\"  \"8/28/16 22:55\"  \"8/28/16 22:59\"  \"8/28/16 22:59\" \n [601] \"8/28/16 23:02\"  \"8/29/16 10:11\"  \"8/29/16 10:14\"  \"8/29/16 10:17\" \n [605] \"8/29/16 11:58\"  \"8/29/16 12:00\"  \"8/29/16 12:02\"  \"8/29/16 13:16\" \n [609] \"8/29/16 19:24\"  \"8/29/16 22:48\"  \"8/29/16 22:50\"  \"8/29/16 22:51\" \n [613] \"8/29/16 22:52\"  \"8/30/16 2:01\"   \"8/30/16 2:56\"   \"8/30/16 6:08\"  \n [617] \"8/30/16 6:11\"   \"8/30/16 11:42\"  \"8/30/16 11:46\"  \"8/30/16 11:46\" \n [621] \"8/30/16 13:07\"  \"8/30/16 13:09\"  \"8/30/16 14:18\"  \"8/30/16 14:46\" \n [625] \"8/30/16 22:37\"  \"8/30/16 22:42\"  \"8/31/16 0:22\"   \"8/31/16 2:52\"  \n [629] \"8/31/16 5:43\"   \"8/31/16 11:31\"  \"8/31/16 11:36\"  \"8/31/16 11:36\" \n [633] \"8/31/16 11:38\"  \"8/31/16 11:40\"  \"8/31/16 13:53\"  \"8/31/16 22:25\" \n [637] \"8/31/16 22:27\"  \"8/31/16 22:30\"  \"9/1/16 6:25\"    \"9/1/16 11:19\"  \n [641] \"9/1/16 11:20\"   \"9/1/16 11:20\"   \"9/1/16 11:21\"   \"9/1/16 11:29\"  \n [645] \"9/1/16 13:57\"   \"9/1/16 23:49\"   \"9/1/16 23:50\"   \"9/1/16 23:58\"  \n [649] \"9/2/16 6:42\"    \"9/2/16 6:46\"    \"9/2/16 9:31\"    \"9/2/16 11:09\"  \n [653] \"9/2/16 11:12\"   \"9/2/16 11:12\"   \"9/2/16 11:12\"   \"9/2/16 11:16\"  \n [657] \"9/2/16 21:59\"   \"9/2/16 23:44\"   \"9/2/16 23:45\"   \"9/3/16 6:26\"   \n [661] \"9/3/16 11:01\"   \"9/3/16 12:42\"   \"9/3/16 13:34\"   \"9/3/16 14:01\"  \n [665] \"9/3/16 14:02\"   \"9/3/16 14:42\"   \"9/3/16 19:21\"   \"9/3/16 23:28\"  \n [669] \"9/3/16 23:29\"   \"9/3/16 23:33\"   \"9/4/16 3:46\"    \"9/4/16 6:58\"   \n [673] \"9/4/16 10:48\"   \"9/4/16 12:31\"   \"9/4/16 13:25\"   \"9/4/16 13:31\"  \n [677] \"9/4/16 14:19\"   \"9/4/16 18:13\"   \"9/4/16 19:00\"   \"9/4/16 19:01\"  \n [681] \"9/4/16 23:17\"   \"9/4/16 23:19\"   \"9/5/16 6:41\"    \"9/5/16 6:42\"   \n [685] \"9/5/16 10:39\"   \"9/5/16 12:17\"   \"9/5/16 12:19\"   \"9/5/16 12:21\"  \n [689] \"9/5/16 14:41\"   \"9/5/16 14:43\"   \"9/5/16 23:11\"   \"9/5/16 23:12\"  \n [693] \"9/6/16 5:29\"    \"9/6/16 6:15\"    \"9/6/16 10:22\"   \"9/6/16 10:25\"  \n [697] \"9/6/16 10:29\"   \"9/6/16 12:03\"   \"9/6/16 12:09\"   \"9/6/16 12:56\"  \n [701] \"9/6/16 22:59\"   \"9/6/16 23:00\"   \"9/7/16 3:10\"    \"9/7/16 6:39\"   \n [705] \"9/7/16 6:48\"    \"9/7/16 11:54\"   \"9/7/16 11:56\"   \"9/7/16 13:05\"  \n [709] \"9/7/16 13:39\"   \"9/7/16 13:40\"   \"9/7/16 17:53\"   \"9/7/16 22:49\"  \n [713] \"9/7/16 22:52\"   \"9/8/16 6:22\"    \"9/8/16 6:25\"    \"9/8/16 10:05\"  \n [717] \"9/8/16 11:45\"   \"9/8/16 12:40\"   \"9/8/16 13:06\"   \"9/8/16 14:17\"  \n [721] \"9/8/16 22:32\"   \"9/8/16 22:36\"   \"9/9/16 2:41\"    \"9/9/16 5:12\"   \n [725] \"9/9/16 6:03\"    \"9/9/16 6:03\"    \"9/9/16 6:57\"    \"9/9/16 11:32\"  \n [729] \"9/9/16 11:34\"   \"9/9/16 11:36\"   \"9/9/16 13:58\"   \"9/9/16 15:39\"  \n [733] \"9/10/16 2:56\"   \"9/10/16 3:04\"   \"9/10/16 6:38\"   \"9/10/16 6:39\"  \n [737] \"9/10/16 11:19\"  \"9/10/16 11:21\"  \"9/10/16 13:30\"  \"9/10/16 13:31\" \n [741] \"9/10/16 13:32\"  \"9/10/16 23:48\"  \"9/11/16 6:17\"   \"9/11/16 11:05\" \n [745] \"9/11/16 13:07\"  \"9/11/16 13:07\"  \"9/11/16 13:39\"  \"9/11/16 13:40\" \n [749] \"9/11/16 23:37\"  \"9/11/16 23:39\"  \"9/11/16 23:44\"  \"9/12/16 3:38\"  \n [753] \"9/12/16 5:53\"   \"9/12/16 10:53\"  \"9/12/16 10:55\"  \"9/12/16 12:40\" \n [757] \"9/12/16 13:25\"  \"9/12/16 13:28\"  \"9/12/16 14:23\"  \"9/12/16 19:32\" \n [761] \"9/12/16 21:50\"  \"9/12/16 23:34\"  \"9/13/16 4:42\"   \"9/13/16 10:51\" \n [765] \"9/13/16 12:27\"  \"9/13/16 13:14\"  \"9/13/16 14:58\"  \"9/13/16 19:09\" \n [769] \"9/13/16 23:23\"  \"9/14/16 5:58\"   \"9/14/16 6:01\"   \"9/14/16 6:01\"  \n [773] \"9/14/16 13:04\"  \"9/14/16 13:16\"  \"9/14/16 13:18\"  \"9/14/16 13:28\" \n [777] \"9/14/16 14:42\"  \"9/14/16 23:09\"  \"9/14/16 23:11\"  \"9/15/16 3:36\"  \n [781] \"9/15/16 5:44\"   \"9/15/16 6:32\"   \"9/15/16 6:34\"   \"9/15/16 10:25\" \n [785] \"9/15/16 12:04\"  \"9/15/16 13:04\"  \"9/15/16 13:07\"  \"9/15/16 14:29\" \n [789] \"9/15/16 22:57\"  \"9/16/16 6:54\"   \"9/16/16 6:58\"   \"9/16/16 10:13\" \n [793] \"9/16/16 11:49\"  \"9/16/16 12:41\"  \"9/16/16 14:28\"  \"9/16/16 22:43\" \n [797] \"9/16/16 22:44\"  \"9/16/16 22:49\"  \"9/17/16 2:41\"   \"9/17/16 6:33\"  \n [801] \"9/17/16 6:37\"   \"9/17/16 11:37\"  \"9/17/16 11:41\"  \"9/17/16 11:42\" \n [805] \"9/17/16 11:46\"  \"9/17/16 17:46\"  \"9/17/16 22:33\"  \"9/17/16 22:36\" \n [809] \"9/18/16 6:16\"   \"9/18/16 6:16\"   \"9/18/16 11:27\"  \"9/18/16 13:28\" \n [813] \"9/18/16 14:37\"  \"9/18/16 15:09\"  \"9/18/16 19:07\"  \"9/18/16 22:21\" \n [817] \"9/18/16 22:24\"  \"9/19/16 1:45\"   \"9/19/16 1:58\"   \"9/19/16 5:51\"  \n [821] \"9/19/16 5:52\"   \"9/19/16 11:13\"  \"9/19/16 11:14\"  \"9/19/16 13:50\" \n [825] \"9/19/16 14:07\"  \"9/19/16 15:46\"  \"9/20/16 1:06\"   \"9/20/16 6:26\"  \n [829] \"9/20/16 7:11\"   \"9/20/16 11:08\"  \"9/20/16 12:41\"  \"9/20/16 15:07\" \n [833] \"9/20/16 22:03\"  \"9/20/16 23:41\"  \"9/21/16 2:14\"   \"9/21/16 6:49\"  \n [837] \"9/21/16 10:56\"  \"9/21/16 12:41\"  \"9/21/16 13:22\"  \"9/21/16 13:54\" \n [841] \"9/21/16 16:24\"  \"9/21/16 23:24\"  \"9/22/16 4:02\"   \"9/22/16 6:30\"  \n [845] \"9/22/16 6:31\"   \"9/22/16 12:27\"  \"9/22/16 13:16\"  \"9/22/16 14:51\" \n [849] \"9/22/16 19:24\"  \"9/22/16 23:19\"  \"9/23/16 3:23\"   \"9/23/16 6:08\"  \n [853] \"9/23/16 12:11\"  \"9/23/16 12:59\"  \"9/23/16 13:07\"  \"9/23/16 14:36\" \n [857] \"9/23/16 19:02\"  \"9/24/16 3:16\"   \"9/24/16 6:42\"   \"9/24/16 6:47\"  \n [861] \"9/24/16 11:59\"  \"9/24/16 12:40\"  \"9/25/16 3:05\"   \"9/25/16 14:11\" \n [865] \"9/27/16 13:56\"  \"9/27/16 22:23\"  \"9/28/16 11:19\"  \"9/28/16 14:18\" \n [869] \"9/28/16 19:03\"  \"9/28/16 23:46\"  \"9/28/16 23:53\"  \"9/29/16 1:28\"  \n [873] \"9/29/16 5:50\"   \"9/29/16 11:04\"  \"9/29/16 11:04\"  \"9/29/16 23:40\" \n [877] \"9/30/16 6:23\"   \"9/30/16 10:52\"  \"10/1/16 3:46\"   \"10/1/16 12:25\" \n [881] \"10/1/16 14:30\"  \"10/2/16 10:33\"  \"10/2/16 13:57\"  \"10/2/16 19:16\" \n [885] \"10/2/16 23:01\"  \"10/3/16 12:03\"  \"10/3/16 15:07\"  \"10/3/16 19:00\" \n [889] \"10/3/16 22:55\"  \"10/4/16 11:50\"  \"10/4/16 15:56\"  \"10/4/16 22:46\" \n [893] \"10/5/16 6:21\"   \"10/5/16 11:39\"  \"10/6/16 5:07\"   \"10/6/16 11:27\" \n [897] \"10/6/16 14:23\"  \"10/7/16 15:18\"  \"10/7/16 15:19\"  \"10/7/16 23:49\" \n [901] \"10/7/16 23:51\"  \"10/8/16 11:05\"  \"10/8/16 15:55\"  \"10/8/16 23:32\" \n [905] \"10/8/16 23:35\"  \"10/9/16 14:50\"  \"10/9/16 14:59\"  \"10/9/16 23:24\" \n [909] \"10/9/16 23:27\"  \"10/10/16 5:22\"  \"10/10/16 14:55\" \"10/10/16 16:34\"\n [913] \"10/11/16 4:58\"  \"10/11/16 17:06\" \"10/12/16 13:30\" \"10/12/16 22:55\"\n [917] \"10/13/16 11:47\" \"10/13/16 17:08\" \"10/13/16 22:37\" \"10/13/16 22:43\"\n [921] \"10/14/16 3:41\"  \"10/14/16 7:18\"  \"10/14/16 13:55\" \"10/14/16 22:28\"\n [925] \"10/15/16 11:24\" \"10/15/16 14:03\" \"10/15/16 15:42\" \"10/15/16 22:15\"\n [929] \"10/15/16 23:51\" \"10/15/16 23:57\" \"10/16/16 6:31\"  \"10/16/16 14:58\"\n [933] \"10/16/16 15:03\" \"10/16/16 17:44\" \"10/16/16 22:04\" \"10/16/16 23:47\"\n [937] \"10/17/16 13:09\" \"10/17/16 13:14\" \"10/17/16 19:07\" \"10/17/16 23:32\"\n [941] \"10/17/16 23:32\" \"10/18/16 10:45\" \"10/18/16 14:24\" \"10/18/16 23:19\"\n [945] \"10/18/16 23:24\" \"10/18/16 23:27\" \"10/19/16 5:41\"  \"10/19/16 6:28\" \n [949] \"10/19/16 7:15\"  \"10/19/16 10:39\" \"10/19/16 10:42\" \"10/19/16 23:12\"\n [953] \"10/20/16 0:48\"  \"10/20/16 6:57\"  \"10/20/16 10:27\" \"10/20/16 23:02\"\n [957] \"10/21/16 6:32\"  \"10/21/16 11:57\" \"10/21/16 17:48\" \"10/21/16 22:44\"\n [961] \"10/22/16 10:04\" \"10/22/16 13:33\" \"10/22/16 22:38\" \"10/23/16 11:40\"\n [965] \"10/23/16 22:25\" \"10/23/16 22:26\" \"10/24/16 6:28\"  \"10/24/16 11:25\"\n [969] \"10/24/16 22:16\" \"10/25/16 1:25\"  \"10/25/16 6:47\"  \"10/25/16 13:29\"\n [973] \"10/26/16 14:31\" \"10/27/16 13:59\" \"10/27/16 23:16\" \"10/27/16 23:19\"\n [977] \"10/28/16 2:27\"  \"10/28/16 14:36\" \"10/28/16 14:43\" \"10/28/16 23:05\"\n [981] \"10/29/16 12:04\" \"10/29/16 14:30\" \"10/29/16 14:56\" \"10/29/16 22:55\"\n [985] \"10/30/16 6:49\"  \"10/30/16 14:24\" \"10/30/16 16:24\" \"10/30/16 22:43\"\n [989] \"10/30/16 22:46\" \"10/31/16 6:22\"  \"10/31/16 11:43\" \"10/31/16 13:56\"\n [993] \"10/31/16 15:36\" \"10/31/16 22:35\" \"10/31/16 22:36\" \"11/1/16 5:16\"  \n [997] \"11/1/16 11:27\"  \"11/1/16 11:31\"  \"11/1/16 13:26\"  \"11/1/16 22:20\" \n[1001] \"11/2/16 13:43\"  \"11/2/16 14:29\"  \"11/2/16 15:18\"  \"11/2/16 23:54\" \n[1005] \"11/2/16 23:58\"  \"11/3/16 3:56\"   \"11/3/16 5:26\"   \"11/3/16 11:03\" \n[1009] \"11/3/16 15:10\"  \"11/3/16 23:35\"  \"11/4/16 10:58\"  \"11/4/16 13:55\" \n[1013] \"11/4/16 23:29\"  \"11/5/16 7:12\"   \"11/5/16 14:44\"  \"11/5/16 14:44\" \n[1017] \"11/5/16 23:15\"  \"11/5/16 23:17\"  \"11/5/16 23:18\"  \"11/6/16 10:34\" \n[1021] \"11/6/16 13:10\"  \"11/6/16 14:03\"  \"11/6/16 23:04\"  \"11/6/16 23:06\" \n[1025] \"11/7/16 3:09\"   \"11/7/16 6:35\"   \"11/7/16 12:07\"  \"11/7/16 13:33\" \n[1029] \"11/7/16 14:20\"  \"11/7/16 22:53\"  \"11/8/16 0:39\"   \"11/8/16 1:12\"  \n[1033] \"11/8/16 10:17\"  \"11/8/16 11:54\"  \"11/8/16 14:08\"  \"11/9/16 11:37\" \n[1037] \"11/9/16 13:30\"  \"11/9/16 22:32\"  \"11/9/16 22:36\"  \"11/10/16 2:57\" \n[1041] \"11/10/16 11:25\" \"11/10/16 11:26\" \"11/10/16 11:32\" \"11/10/16 22:25\"\n[1045] \"11/10/16 22:25\" \"11/11/16 11:13\" \"11/11/16 11:14\" \"11/11/16 11:15\"\n[1049] \"11/11/16 22:08\" \"11/11/16 23:45\" \"11/11/16 23:45\" \"11/12/16 1:58\" \n[1053] \"11/12/16 1:58\"  \"11/12/16 11:00\" \"11/12/16 11:09\" \"11/12/16 15:00\"\n[1057] \"11/12/16 23:41\" \"11/13/16 1:24\"  \"11/13/16 10:55\" \"11/13/16 10:56\"\n[1061] \"11/13/16 12:37\" \"11/13/16 23:25\" \"11/13/16 23:35\" \"11/14/16 6:35\" \n[1065] \"11/14/16 12:25\" \"11/14/16 13:08\" \"11/14/16 13:13\" \"11/14/16 23:12\"\n[1069] \"11/14/16 23:13\" \"11/15/16 3:42\"  \"11/15/16 3:43\"  \"11/15/16 10:28\"\n[1073] \"11/15/16 12:12\" \"11/15/16 14:22\" \"11/15/16 23:00\" \"11/16/16 3:07\" \n[1077] \"11/16/16 6:54\"  \"11/16/16 14:07\" \"11/16/16 14:16\" \"11/16/16 15:32\"\n[1081] \"11/16/16 22:56\" \"11/17/16 5:35\"  \"11/17/16 5:37\"  \"11/17/16 13:25\"\n[1085] \"11/17/16 13:33\" \"11/17/16 13:36\" \"11/17/16 22:44\" \"11/18/16 4:15\" \n[1089] \"11/18/16 6:53\"  \"11/18/16 11:38\" \"11/18/16 13:12\" \"11/18/16 13:56\"\n[1093] \"11/18/16 22:30\" \"11/19/16 6:34\"  \"11/19/16 11:24\" \"11/19/16 13:45\"\n[1097] \"11/19/16 14:07\" \"11/19/16 22:19\" \"11/19/16 22:19\" \"11/20/16 11:19\"\n[1101] \"11/20/16 11:19\" \"11/20/16 11:20\" \"11/20/16 19:04\" \"11/20/16 19:04\"\n[1105] \"11/20/16 23:47\" \"11/21/16 2:15\"  \"11/21/16 10:58\" \"11/21/16 11:07\"\n[1109] \"11/21/16 13:22\" \"11/21/16 21:58\" \"11/21/16 23:42\" \"11/22/16 6:25\" \n[1113] \"11/22/16 6:29\"  \"11/22/16 10:52\" \"11/22/16 13:11\" \"11/23/16 6:51\" \n[1117] \"11/23/16 12:22\" \"11/23/16 12:24\" \"11/23/16 23:12\" \"11/24/16 6:33\" \n[1121] \"11/24/16 10:28\" \"11/24/16 14:05\" \"11/24/16 23:05\" \"11/25/16 6:12\" \n[1125] \"11/25/16 10:19\" \"11/25/16 12:04\" \"11/25/16 22:53\" \"11/26/16 6:46\" \n[1129] \"11/26/16 13:43\" \"11/26/16 15:22\" \"11/26/16 22:45\" \"11/27/16 4:15\" \n[1133] \"11/27/16 11:40\" \"11/27/16 16:37\" \"11/27/16 22:33\" \"11/28/16 6:46\" \n[1137] \"11/28/16 11:25\" \"11/28/16 13:38\" \"11/28/16 15:18\" \"11/28/16 22:16\"\n[1141] \"11/28/16 23:59\" \"11/29/16 11:14\" \"11/29/16 11:18\" \"11/29/16 13:31\"\n[1145] \"11/29/16 19:18\" \"11/29/16 19:19\" \"11/29/16 23:45\" \"11/30/16 3:47\" \n[1149] \"11/30/16 6:04\"  \"11/30/16 6:08\"  \"11/30/16 11:01\" \"11/30/16 12:42\"\n[1153] \"11/30/16 12:44\" \"12/1/16 3:31\"   \"12/1/16 6:39\"   \"12/1/16 12:34\" \n[1157] \"12/1/16 12:46\"  \"12/1/16 12:47\"  \"12/1/16 23:17\"  \"12/2/16 5:23\"  \n[1161] \"12/2/16 10:42\"  \"12/2/16 12:19\"  \"12/2/16 14:30\"  \"12/2/16 23:13\" \n[1165] \"12/2/16 23:17\"  \"12/3/16 6:40\"   \"12/3/16 6:42\"   \"12/3/16 12:10\" \n[1169] \"12/3/16 13:25\"  \"12/3/16 13:32\"  \"12/3/16 23:03\"  \"12/4/16 3:00\"  \n[1173] \"12/4/16 6:27\"   \"12/4/16 11:52\"  \"12/4/16 11:56\"  \"12/4/16 14:10\" \n[1177] \"12/4/16 22:51\"  \"12/5/16 3:22\"   \"12/5/16 6:04\"   \"12/5/16 6:59\"  \n[1181] \"12/5/16 11:43\"  \"12/5/16 11:46\"  \"12/5/16 12:48\"  \"12/6/16 6:37\"  \n[1185] \"12/6/16 11:37\"  \"12/6/16 11:39\"  \"12/6/16 13:44\"  \"12/6/16 22:23\" \n[1189] \"12/6/16 22:24\"  \"12/7/16 13:31\"  \"12/7/16 13:36\"  \"12/7/16 13:38\" \n[1193] \"12/7/16 23:51\"  \"12/7/16 23:55\"  \"12/8/16 5:51\"   \"12/8/16 11:08\" \n[1197] \"12/8/16 11:09\"  \"12/8/16 15:10\"  \"12/8/16 23:41\"  \"12/9/16 6:15\"  \n[1201] \"12/9/16 6:24\"   \"12/9/16 13:13\"  \"12/9/16 14:27\"  \"12/9/16 14:50\" \n[1205] \"12/9/16 23:32\"  \"12/10/16 6:49\"  \"12/10/16 12:30\" \"12/10/16 17:14\"\n[1209] \"12/10/16 23:25\" \"12/11/16 5:34\"  \"12/11/16 5:37\"  \"12/11/16 10:34\"\n[1213] \"12/11/16 10:39\" \"12/11/16 12:21\" \"12/11/16 23:09\" \"12/12/16 6:07\" \n[1217] \"12/12/16 6:53\"  \"12/12/16 10:33\" \"12/12/16 14:47\" \"12/12/16 14:51\"\n[1221] \"12/13/16 3:19\"  \"12/13/16 11:52\" \"12/13/16 14:03\" \"12/13/16 14:09\"\n[1225] \"12/13/16 22:50\" \"12/13/16 22:50\" \"12/14/16 1:15\"  \"12/14/16 4:36\" \n[1229] \"12/14/16 6:13\"  \"12/14/16 10:07\" \"12/14/16 11:40\" \"12/14/16 13:58\"\n[1233] \"12/15/16 3:13\"  \"12/15/16 4:11\"  \"12/15/16 5:57\"  \"12/15/16 11:28\"\n[1237] \"12/15/16 11:32\" \"12/15/16 13:55\" \"12/16/16 0:07\"  \"12/16/16 6:27\" \n[1241] \"12/16/16 6:31\"  \"12/16/16 11:22\" \"12/16/16 13:09\" \"12/16/16 15:04\"\n[1245] \"12/17/16 3:44\"  \"12/17/16 6:10\"  \"12/17/16 11:07\" \"12/17/16 11:08\"\n[1249] \"12/17/16 18:06\" \"12/17/16 23:45\" \"12/18/16 3:36\"  \"12/18/16 6:31\" \n[1253] \"12/18/16 10:56\" \"12/18/16 10:58\" \"12/18/16 23:30\" \"12/19/16 3:24\" \n[1257] \"12/19/16 12:25\" \"12/19/16 13:34\" \"12/19/16 15:18\" \"12/19/16 19:05\"\n[1261] \"12/20/16 3:15\"  \"12/20/16 5:55\"  \"12/20/16 10:35\" \"12/20/16 12:17\"\n[1265] \"12/20/16 12:19\" \"12/21/16 4:37\"  \"12/21/16 10:21\" \"12/21/16 12:46\"\n[1269] \"12/21/16 12:46\" \"12/21/16 22:56\" \"12/21/16 22:58\" \"12/22/16 2:52\" \n[1273] \"12/22/16 6:45\"  \"12/22/16 6:48\"  \"12/22/16 10:12\" \"12/22/16 11:57\"\n[1277] \"12/22/16 11:58\" \"12/23/16 4:50\"  \"12/23/16 9:58\"  \"12/23/16 11:39\"\n[1281] \"12/23/16 14:43\" \"12/23/16 22:34\" \"12/23/16 22:39\" \"12/24/16 3:26\" \n[1285] \"12/24/16 3:44\"  \"12/24/16 11:28\" \"12/24/16 11:28\" \"12/24/16 14:16\"\n[1289] \"12/24/16 22:18\" \"12/25/16 5:50\"  \"12/25/16 11:21\" \"12/25/16 13:38\"\n[1293] \"12/25/16 15:09\" \"12/25/16 23:55\" \"12/25/16 23:57\" \"12/26/16 5:26\" \n[1297] \"12/26/16 5:26\"  \"12/26/16 5:29\"  \"12/26/16 11:11\" \"12/26/16 13:18\"\n[1301] \"12/26/16 14:50\" \"12/27/16 3:27\"  \"12/27/16 10:58\" \"12/27/16 12:37\"\n[1305] \"12/27/16 13:32\" \"12/27/16 23:29\" \"12/27/16 23:34\" \"12/28/16 4:46\" \n[1309] \"12/28/16 6:21\"  \"12/28/16 6:32\"  \"12/28/16 12:24\" \"12/28/16 12:25\"\n[1313] \"12/28/16 14:37\" \"12/29/16 2:32\"  \"12/29/16 6:01\"  \"12/29/16 12:10\"\n[1317] \"12/29/16 12:13\" \"12/29/16 17:21\" \"12/29/16 23:09\" \"12/30/16 10:21\"\n[1321] \"12/30/16 10:21\" \"12/30/16 14:27\" \"12/30/16 22:50\" \"12/30/16 22:52\"\n[1325] \"12/31/16 5:21\"  \"12/31/16 5:22\"  \"12/31/16 11:46\" \"12/31/16 13:36\"\n[1329] \"12/31/16 14:03\" \"12/31/16 22:47\" \"1/1/17 0:23\"    \"1/1/17 2:39\"   \n[1333] \"1/1/17 2:42\"    \"1/1/17 11:41\"   \"1/1/17 13:12\"   \"1/2/17 3:20\"   \n[1337] \"1/2/17 3:22\"    \"1/2/17 4:00\"    \"1/2/17 11:25\"   \"1/2/17 11:29\"  \n[1341] \"1/2/17 11:30\"   \"1/3/17 2:52\"    \"1/3/17 3:52\"    \"1/3/17 12:57\"  \n[1345] \"1/3/17 13:59\"   \"1/3/17 23:43\"   \"1/4/17 2:44\"    \"1/4/17 5:45\"   \n[1349] \"1/4/17 12:47\"   \"1/4/17 14:49\"   \"1/4/17 18:33\"   \"1/4/17 21:56\"  \n[1353] \"1/5/17 3:33\"    \"1/5/17 5:22\"    \"1/5/17 10:50\"   \"1/5/17 12:37\"  \n[1357] \"1/5/17 14:40\"   \"1/5/17 23:31\"   \"1/6/17 3:19\"    \"1/6/17 10:40\"  \n[1361] \"1/6/17 14:11\"   \"1/6/17 17:52\"   \"1/6/17 23:15\"   \"1/6/17 23:18\"  \n[1365] \"1/7/17 7:15\"    \"1/7/17 12:13\"   \"1/7/17 22:59\"   \"1/7/17 23:08\"  \n[1369] \"1/8/17 0:46\"    \"1/8/17 6:01\"    \"1/8/17 11:59\"   \"1/8/17 12:01\"  \n[1373] \"1/9/17 4:24\"    \"1/9/17 6:35\"    \"1/9/17 10:05\"   \"1/9/17 11:52\"  \n[1377] \"1/10/17 2:55\"   \"1/10/17 11:38\"  \"1/10/17 13:44\"  \"1/10/17 22:33\" \n[1381] \"1/11/17 3:53\"   \"1/11/17 6:31\"   \"1/11/17 13:35\"  \"1/11/17 14:49\" \n[1385] \"1/12/17 2:24\"   \"1/12/17 11:11\"  \"1/12/17 11:13\"  \"1/12/17 22:09\" \n[1389] \"1/12/17 23:44\"  \"1/13/17 3:38\"   \"1/13/17 3:38\"   \"1/13/17 11:01\" \n[1393] \"1/13/17 18:01\"  \"1/14/17 6:28\"   \"1/14/17 10:50\"  \"1/14/17 10:54\" \n[1397] \"1/14/17 23:22\"  \"1/15/17 5:15\"   \"1/15/17 6:09\"   \"1/15/17 10:41\" \n[1401] \"1/15/17 12:19\"  \"1/16/17 4:52\"   \"1/16/17 10:28\"  \"1/16/17 12:08\" \n[1405] \"1/16/17 14:18\"  \"1/16/17 23:07\"  \"1/17/17 6:12\"   \"1/17/17 6:13\"  \n[1409] \"1/17/17 12:02\"  \"1/17/17 14:52\"  \"1/17/17 15:05\"  \"1/18/17 0:29\"  \n[1413] \"1/18/17 5:47\"   \"1/18/17 11:46\"  \"1/18/17 11:51\"  \"1/18/17 16:12\" \n[1417] \"1/19/17 2:53\"   \"1/19/17 4:06\"   \"1/19/17 5:32\"   \"1/19/17 13:41\" \n[1421] \"1/19/17 16:40\"  \"1/20/17 6:02\"   \"1/20/17 11:21\"  \"1/20/17 15:13\" \n[1425] \"1/20/17 22:17\"  \"1/21/17 11:09\"  \"1/21/17 11:18\"  \"1/21/17 22:04\" \n[1429] \"1/21/17 23:51\"  \"1/22/17 6:10\"   \"1/22/17 6:14\"   \"1/22/17 11:01\" \n[1433] \"1/22/17 14:05\"  \"1/23/17 12:33\"  \"1/23/17 14:07\"  \"1/23/17 23:21\" \n[1437] \"1/23/17 23:26\"  \"1/24/17 2:31\"   \"1/24/17 13:36\"  \"1/24/17 13:37\" \n[1441] \"1/24/17 23:05\"  \"1/25/17 5:57\"   \"1/25/17 13:16\"  \"1/25/17 14:51\" \n[1445] \"1/25/17 22:57\"  \"1/26/17 6:20\"   \"1/26/17 6:28\"   \"1/26/17 10:10\" \n[1449] \"1/26/17 14:24\"  \"1/27/17 11:40\"  \"1/27/17 17:13\"  \"1/27/17 22:35\" \n[1453] \"1/27/17 22:39\"  \"1/28/17 2:19\"   \"1/28/17 11:28\"  \"1/28/17 13:41\" \n[1457] \"1/29/17 11:21\"  \"1/29/17 11:23\"  \"1/29/17 15:28\"  \"1/29/17 23:52\" \n[1461] \"1/29/17 23:55\"  \"1/30/17 11:10\"  \"1/30/17 11:10\"  \"1/30/17 23:36\" \n[1465] \"1/30/17 23:45\"  \"1/31/17 4:45\"   \"1/31/17 6:22\"   \"1/31/17 11:03\" \n[1469] \"1/31/17 17:32\"  \"2/1/17 3:20\"    \"2/1/17 6:56\"    \"2/1/17 12:25\"  \n[1473] \"2/2/17 5:37\"    \"2/2/17 14:17\"   \"2/2/17 15:06\"   \"2/2/17 23:07\"  \n[1477] \"2/3/17 5:25\"    \"2/3/17 14:07\"   \"2/3/17 14:08\"   \"2/3/17 22:53\"  \n[1481] \"2/4/17 10:15\"   \"2/4/17 11:58\"   \"2/4/17 22:42\"   \"2/4/17 22:47\"  \n[1485] \"2/5/17 4:06\"    \"2/5/17 6:22\"    \"2/5/17 13:34\"   \"2/6/17 4:17\"   \n[1489] \"2/6/17 13:32\"   \"2/7/17 3:24\"    \"2/7/17 11:16\"   \"2/7/17 14:09\"  \n[1493] \"2/7/17 23:46\"   \"2/8/17 5:20\"    \"2/8/17 5:23\"    \"2/8/17 11:02\"  \n[1497] \"2/9/17 3:28\"    \"2/9/17 10:56\"   \"2/9/17 10:57\"   \"2/9/17 12:36\"  \n[1501] \"2/10/17 1:59\"   \"2/10/17 7:12\"   \"2/10/17 10:45\"  \"2/10/17 14:13\" \n[1505] \"2/11/17 1:40\"   \"2/11/17 3:19\"   \"2/11/17 10:30\"  \"2/11/17 15:23\" \n[1509] \"2/12/17 6:26\"   \"2/12/17 10:25\"  \"2/12/17 12:01\"  \"2/13/17 2:25\"  \n[1513] \"2/13/17 6:55\"   \"2/13/17 11:49\"  \"2/13/17 13:36\"  \"2/14/17 4:09\"  \n[1517] \"2/14/17 11:38\"  \"2/14/17 14:52\"  \"2/14/17 22:30\"  \"2/15/17 6:10\"  \n[1521] \"2/15/17 14:31\"  \"2/15/17 17:19\"  \"2/15/17 22:23\"  \"2/16/17 11:19\" \n[1525] \"2/16/17 23:51\"  \"2/16/17 23:54\"  \"2/17/17 6:28\"   \"2/17/17 10:59\" \n[1529] \"2/17/17 15:34\"  \"2/17/17 23:38\"  \"2/18/17 3:22\"   \"2/18/17 12:32\" \n[1533] \"2/18/17 23:29\"  \"2/19/17 10:38\"  \"2/19/17 23:11\"  \"2/19/17 23:16\" \n[1537] \"2/20/17 3:02\"   \"2/20/17 10:28\"  \"2/20/17 12:11\"  \"2/20/17 22:59\" \n[1541] \"2/21/17 2:46\"   \"2/21/17 2:48\"   \"2/21/17 13:33\"  \"2/21/17 18:38\" \n[1545] \"2/22/17 5:26\"   \"2/22/17 13:15\"  \"2/22/17 13:45\"  \"2/22/17 22:45\" \n[1549] \"2/23/17 5:04\"   \"2/23/17 9:55\"   \"2/23/17 11:41\"  \"2/23/17 22:26\" \n[1553] \"2/24/17 0:10\"   \"2/24/17 6:32\"   \"2/24/17 13:41\"  \"2/24/17 13:43\" \n[1557] \"2/24/17 23:56\"  \"2/25/17 11:09\"  \"2/25/17 11:17\"  \"2/25/17 22:05\" \n[1561] \"2/25/17 23:48\"  \"2/26/17 5:47\"   \"2/26/17 12:41\"  \"2/26/17 14:39\" \n[1565] \"2/26/17 23:40\"  \"2/27/17 5:22\"   \"2/27/17 10:46\"  \"2/27/17 10:57\" \n[1569] \"2/27/17 23:20\"  \"2/28/17 3:02\"   \"2/28/17 12:18\"  \"2/28/17 12:18\" \n[1573] \"2/28/17 23:17\"  \"3/1/17 1:59\"    \"3/1/17 6:18\"    \"3/1/17 12:11\"  \n[1577] \"3/1/17 14:27\"   \"3/2/17 6:08\"    \"3/2/17 10:18\"   \"3/2/17 11:58\"  \n[1581] \"3/2/17 22:55\"   \"3/3/17 4:03\"    \"3/3/17 5:47\"    \"3/3/17 12:46\"  \n[1585] \"3/3/17 14:27\"   \"3/4/17 6:15\"    \"3/4/17 11:41\"   \"3/4/17 13:36\"  \n[1589] \"3/5/17 3:13\"    \"3/5/17 6:41\"    \"3/5/17 11:22\"   \"3/5/17 13:37\"  \n[1593] \"3/6/17 6:20\"    \"3/6/17 12:53\"   \"3/7/17 11:07\"   \"3/8/17 10:50\"  \n[1597] \"3/8/17 23:21\"   \"3/8/17 23:23\"   \"3/9/17 2:33\"    \"3/9/17 10:40\"  \n[1601] \"3/9/17 15:15\"   \"3/9/17 23:14\"   \"3/10/17 6:38\"   \"3/10/17 13:18\" \n[1605] \"3/10/17 14:07\"  \"3/11/17 3:23\"   \"3/11/17 5:27\"   \"3/11/17 13:57\" \n[1609] \"3/11/17 14:07\"  \"3/12/17 5:55\"   \"3/12/17 11:46\"  \"3/12/17 15:24\" \n[1613] \"3/12/17 22:36\"  \"3/13/17 0:18\"   \"3/13/17 6:33\"   \"3/13/17 13:40\" \n[1617] \"3/13/17 13:42\"  \"3/13/17 22:29\"  \"3/14/17 13:03\"  \"3/14/17 23:55\" \n[1621] \"3/15/17 6:27\"   \"3/15/17 14:24\"  \"3/16/17 14:06\"  \"3/16/17 19:00\" \n[1625] \"3/17/17 12:28\"  \"3/17/17 14:18\"  \"3/17/17 15:17\"  \"3/17/17 23:14\" \n[1629] \"3/17/17 23:19\"  \"3/17/17 23:20\"  \"3/18/17 3:12\"   \"3/18/17 5:28\"  \n[1633] \"3/18/17 10:33\"  \"3/18/17 12:18\"  \"3/18/17 18:20\"  \"3/18/17 23:13\" \n[1637] \"3/19/17 6:44\"   \"3/19/17 6:47\"   \"3/19/17 12:07\"  \"3/19/17 14:57\" \n[1641] \"3/19/17 15:40\"  \"3/19/17 22:58\"  \"3/20/17 4:11\"   \"3/20/17 6:34\"  \n[1645] \"3/20/17 10:09\"  \"3/20/17 10:15\"  \"3/20/17 14:04\"  \"3/20/17 22:41\" \n[1649] \"3/21/17 4:03\"   \"3/21/17 6:14\"   \"3/21/17 11:36\"  \"3/21/17 13:33\" \n[1653] \"3/21/17 14:01\"  \"3/21/17 19:04\"  \"3/22/17 0:18\"   \"3/22/17 11:24\" \n[1657] \"3/22/17 13:13\"  \"3/22/17 13:14\"  \"3/22/17 22:21\"  \"3/22/17 23:58\" \n[1661] \"3/23/17 11:18\"  \"3/23/17 14:54\"  \"3/23/17 14:58\"  \"3/23/17 23:48\" \n[1665] \"3/23/17 23:53\"  \"3/23/17 23:54\"  \"3/24/17 3:32\"   \"3/24/17 6:51\"  \n[1669] \"3/24/17 11:05\"  \"3/24/17 12:49\"  \"3/24/17 15:45\"  \"3/24/17 23:40\" \n[1673] \"3/25/17 3:16\"   \"3/25/17 6:27\"   \"3/25/17 6:31\"   \"3/25/17 10:52\" \n[1677] \"3/25/17 12:35\"  \"3/25/17 15:12\"  \"3/26/17 4:27\"   \"3/26/17 13:13\" \n[1681] \"3/26/17 13:17\"  \"3/26/17 15:56\"  \"3/26/17 23:17\"  \"3/26/17 23:23\" \n[1685] \"3/27/17 3:20\"   \"3/27/17 6:39\"   \"3/27/17 10:30\"  \"3/27/17 12:19\" \n[1689] \"3/27/17 14:27\"  \"3/27/17 23:05\"  \"3/28/17 5:27\"   \"3/28/17 10:20\" \n[1693] \"3/28/17 11:59\"  \"3/28/17 12:01\"  \"3/28/17 22:50\"  \"3/28/17 22:54\" \n[1697] \"3/29/17 4:06\"   \"3/29/17 11:53\"  \"3/29/17 14:54\"  \"3/29/17 16:16\" \n[1701] \"3/29/17 22:42\"  \"3/30/17 4:39\"   \"3/30/17 9:56\"   \"3/30/17 9:59\"  \n[1705] \"3/30/17 14:22\"  \"3/30/17 22:30\"  \"3/30/17 22:31\"  \"3/31/17 3:42\"  \n[1709] \"3/31/17 13:18\"  \"3/31/17 13:22\"  \"3/31/17 22:18\"  \"3/31/17 23:56\" \n[1713] \"4/1/17 11:16\"   \"4/1/17 11:20\"   \"4/1/17 18:33\"   \"4/1/17 22:08\"  \n[1717] \"4/1/17 23:54\"   \"4/2/17 12:43\"   \"4/2/17 12:46\"   \"4/2/17 13:36\"  \n[1721] \"4/2/17 23:40\"   \"4/2/17 23:42\"   \"4/3/17 4:58\"    \"4/3/17 6:37\"   \n[1725] \"4/3/17 10:53\"   \"4/3/17 13:14\"   \"4/3/17 14:56\"   \"4/3/17 23:22\"  \n[1729] \"4/4/17 2:59\"    \"4/4/17 6:19\"    \"4/4/17 10:44\"   \"4/4/17 14:12\"  \n[1733] \"4/5/17 4:33\"    \"4/5/17 6:02\"    \"4/5/17 12:06\"   \"4/5/17 12:16\"  \n[1737] \"4/6/17 10:18\"   \"4/6/17 11:59\"   \"4/6/17 22:50\"   \"4/6/17 22:56\"  \n[1741] \"4/7/17 6:51\"    \"4/7/17 11:47\"   \"4/7/17 14:58\"   \"4/7/17 15:19\"  \n[1745] \"4/7/17 22:37\"   \"4/8/17 6:31\"    \"4/8/17 11:32\"   \"4/8/17 11:36\"  \n[1749] \"4/8/17 22:33\"   \"4/9/17 11:19\"   \"4/9/17 11:24\"   \"4/9/17 14:55\"  \n[1753] \"4/9/17 22:16\"   \"4/9/17 23:54\"   \"4/10/17 6:48\"   \"4/10/17 11:12\" \n[1757] \"4/10/17 14:52\"  \"4/10/17 15:16\"  \"4/10/17 23:45\"  \"4/11/17 10:58\" \n[1761] \"4/11/17 13:04\"  \"4/11/17 14:39\"  \"4/11/17 23:30\"  \"4/11/17 23:35\" \n[1765] \"4/12/17 6:48\"   \"4/12/17 10:48\"  \"4/12/17 10:51\"  \"4/12/17 18:05\" \n[1769] \"4/12/17 23:26\"  \"4/13/17 5:46\"   \"4/13/17 10:37\"  \"4/13/17 13:41\" \n[1773] \"4/13/17 23:17\"  \"4/14/17 12:15\"  \"4/14/17 13:09\"  \"4/14/17 23:04\" \n[1777] \"4/14/17 23:05\"  \"4/15/17 10:17\"  \"4/15/17 13:18\"  \"4/15/17 22:46\" \n[1781] \"4/15/17 22:51\"  \"4/16/17 3:21\"   \"4/16/17 6:22\"   \"4/16/17 13:34\" \n[1785] \"4/16/17 13:42\"  \"4/17/17 6:53\"   \"4/17/17 11:29\"  \"4/17/17 14:00\" \n[1789] \"4/17/17 22:25\"  \"4/18/17 4:06\"   \"4/18/17 11:24\"  \"4/18/17 13:44\" \n[1793] \"4/18/17 23:53\"  \"4/19/17 11:10\"  \"4/19/17 11:17\"  \"4/19/17 23:38\" \n[1797] \"4/19/17 23:40\"  \"4/20/17 5:52\"   \"4/20/17 10:54\"  \"4/20/17 13:20\" \n[1801] \"4/20/17 23:28\"  \"4/21/17 2:57\"   \"4/21/17 6:25\"   \"4/21/17 14:10\" \n[1805] \"4/21/17 14:22\"  \"4/22/17 6:42\"   \"4/22/17 10:37\"  \"4/22/17 13:38\" \n[1809] \"4/23/17 12:02\"  \"4/23/17 22:54\"  \"4/24/17 7:48\"   \"4/24/17 11:54\" \n[1813] \"4/24/17 11:54\"  \"4/24/17 13:41\"  \"4/24/17 19:00\"  \"4/24/17 22:41\" \n[1817] \"4/24/17 22:45\"  \"4/24/17 22:46\"  \"4/24/17 22:50\"  \"4/25/17 5:46\"  \n[1821] \"4/25/17 6:36\"   \"4/25/17 6:37\"   \"4/25/17 6:40\"   \"4/25/17 7:22\"  \n[1825] \"4/25/17 11:37\"  \"4/25/17 11:37\"  \"4/25/17 11:42\"  \"4/25/17 11:42\" \n[1829] \"4/25/17 11:43\"  \"4/25/17 11:43\"  \"4/25/17 11:45\"  \"4/25/17 11:48\" \n[1833] \"4/25/17 22:31\"  \"4/25/17 22:32\"  \"4/26/17 2:54\"   \"4/26/17 5:20\"  \n[1837] \"4/26/17 6:12\"   \"4/26/17 6:22\"   \"4/26/17 6:22\"   \"4/26/17 11:25\" \n[1841] \"4/26/17 11:26\"  \"4/26/17 11:26\"  \"4/26/17 11:27\"  \"4/26/17 11:27\" \n[1845] \"4/26/17 11:28\"  \"4/26/17 11:31\"  \"4/26/17 11:32\"  \"4/26/17 11:34\" \n[1849] \"4/26/17 13:19\"  \"4/26/17 16:41\"  \"4/26/17 22:23\"  \"4/26/17 23:58\" \n[1853] \"4/26/17 23:58\"  \"4/26/17 23:59\"  \"4/27/17 5:00\"   \"4/27/17 5:06\"  \n[1857] \"4/27/17 6:01\"   \"4/27/17 6:39\"   \"4/27/17 6:47\"   \"4/27/17 11:14\" \n[1861] \"4/27/17 11:18\"  \"4/27/17 11:20\"  \"4/27/17 11:22\"  \"4/27/17 11:24\" \n[1865] \"4/27/17 11:25\"  \"4/27/17 11:26\"  \"4/27/17 13:05\"  \"4/27/17 13:07\" \n[1869] \"4/27/17 13:12\"  \"4/27/17 22:11\"  \"4/27/17 23:49\"  \"4/27/17 23:52\" \n[1873] \"4/27/17 23:52\"  \"4/27/17 23:58\"  \"4/28/17 1:52\"   \"4/28/17 3:21\"  \n[1877] \"4/28/17 3:25\"   \"4/28/17 6:17\"   \"4/28/17 6:19\"   \"4/28/17 6:20\"  \n[1881] \"4/28/17 11:08\"  \"4/28/17 11:09\"  \"4/28/17 11:09\"  \"4/28/17 11:12\" \n[1885] \"4/28/17 11:14\"  \"4/28/17 12:48\"  \"4/28/17 12:49\"  \"4/28/17 12:50\" \n[1889] \"4/28/17 12:50\"  \"4/28/17 12:51\"  \"4/28/17 12:51\"  \"4/28/17 23:36\" \n[1893] \"4/28/17 23:37\"  \"4/28/17 23:44\"  \"4/28/17 23:45\"  \"4/28/17 23:45\" \n[1897] \"4/29/17 1:16\"   \"4/29/17 4:21\"   \"4/29/17 5:57\"   \"4/29/17 5:58\"  \n[1901] \"4/29/17 5:59\"   \"4/29/17 6:06\"   \"4/29/17 6:07\"   \"4/29/17 6:56\"  \n[1905] \"4/29/17 6:57\"   \"4/29/17 10:51\"  \"4/29/17 10:57\"  \"4/29/17 11:03\" \n[1909] \"4/29/17 12:37\"  \"4/29/17 12:38\"  \"4/29/17 12:38\"  \"4/29/17 12:39\" \n[1913] \"4/29/17 12:41\"  \"4/29/17 13:42\"  \"4/29/17 13:46\"  \"4/29/17 14:05\" \n[1917] \"4/29/17 23:29\"  \"4/29/17 23:31\"  \"4/30/17 5:35\"   \"4/30/17 5:39\"  \n[1921] \"4/30/17 5:44\"   \"4/30/17 5:45\"   \"4/30/17 6:36\"   \"4/30/17 6:37\"  \n[1925] \"4/30/17 6:38\"   \"4/30/17 10:44\"  \"4/30/17 12:21\"  \"4/30/17 12:24\" \n[1929] \"4/30/17 12:26\"  \"4/30/17 12:26\"  \"4/30/17 12:29\"  \"4/30/17 12:29\" \n[1933] \"4/30/17 12:30\"  \"4/30/17 13:38\"  \"4/30/17 13:38\"  \"4/30/17 23:13\" \n[1937] \"4/30/17 23:18\"  \"4/30/17 23:22\"  \"4/30/17 23:23\"  \"5/1/17 0:55\"   \n[1941] \"5/1/17 5:16\"    \"5/1/17 5:22\"    \"5/1/17 5:23\"    \"5/1/17 6:20\"   \n[1945] \"5/1/17 6:57\"    \"5/1/17 6:59\"    \"5/1/17 10:30\"   \"5/1/17 12:10\"  \n[1949] \"5/1/17 12:15\"   \"5/1/17 12:15\"   \"5/1/17 12:19\"   \"5/1/17 12:19\"  \n[1953] \"5/1/17 12:22\"   \"5/1/17 13:12\"   \"5/1/17 13:13\"   \"5/1/17 14:22\"  \n[1957] \"5/1/17 23:04\"   \"5/1/17 23:07\"   \"5/1/17 23:09\"   \"5/1/17 23:10\"  \n[1961] \"5/2/17 3:06\"    \"5/2/17 3:22\"    \"5/2/17 4:14\"    \"5/2/17 4:58\"   \n[1965] \"5/2/17 5:00\"    \"5/2/17 5:01\"    \"5/2/17 5:48\"    \"5/2/17 6:36\"   \n[1969] \"5/2/17 6:37\"    \"5/2/17 6:37\"    \"5/2/17 10:18\"   \"5/2/17 11:58\"  \n[1973] \"5/2/17 12:01\"   \"5/2/17 12:01\"   \"5/2/17 12:01\"   \"5/2/17 12:03\"  \n[1977] \"5/2/17 12:03\"   \"5/2/17 12:04\"   \"5/2/17 12:08\"   \"5/2/17 12:52\"  \n[1981] \"5/2/17 13:51\"   \"5/2/17 22:58\"   \"5/3/17 2:27\"    \"5/3/17 2:53\"   \n[1985] \"5/3/17 6:16\"    \"5/3/17 6:18\"    \"5/3/17 6:20\"    \"5/3/17 6:24\"   \n[1989] \"5/3/17 11:48\"   \"5/3/17 11:49\"   \"5/3/17 11:49\"   \"5/3/17 11:50\"  \n[1993] \"5/3/17 11:52\"   \"5/3/17 11:53\"   \"5/3/17 11:54\"   \"5/3/17 11:55\"  \n[1997] \"5/3/17 11:58\"   \"5/3/17 13:22\"   \"5/3/17 13:39\"   \"5/3/17 19:10\"  \n[2001] \"5/3/17 19:12\"   \"5/3/17 22:41\"   \"5/3/17 22:43\"   \"5/3/17 22:47\"  \n[2005] \"5/4/17 0:29\"    \"5/4/17 2:04\"    \"5/4/17 2:36\"    \"5/4/17 3:54\"   \n[2009] \"5/4/17 4:06\"    \"5/4/17 6:54\"    \"5/4/17 6:54\"    \"5/4/17 6:57\"   \n[2013] \"5/4/17 7:38\"    \"5/4/17 11:34\"   \"5/4/17 11:36\"   \"5/4/17 11:36\"  \n[2017] \"5/4/17 11:38\"   \"5/4/17 11:39\"   \"5/4/17 11:42\"   \"5/4/17 11:43\"  \n[2021] \"5/4/17 11:43\"   \"5/4/17 14:24\"   \"5/4/17 22:30\"   \"5/4/17 22:34\"  \n[2025] \"5/4/17 22:35\"   \"5/5/17 3:47\"    \"5/5/17 3:49\"    \"5/5/17 5:33\"   \n[2029] \"5/5/17 6:33\"    \"5/5/17 6:35\"    \"5/5/17 7:13\"    \"5/5/17 7:20\"   \n[2033] \"5/5/17 11:23\"   \"5/5/17 11:24\"   \"5/5/17 11:25\"   \"5/5/17 11:26\"  \n[2037] \"5/5/17 11:31\"   \"5/5/17 13:15\"   \"5/5/17 13:18\"   \"5/5/17 16:47\"  \n[2041] \"5/5/17 23:56\"   \"5/5/17 23:57\"   \"5/5/17 23:58\"   \"5/5/17 23:58\"  \n[2045] \"5/5/17 23:59\"   \"5/6/17 3:25\"    \"5/6/17 3:27\"    \"5/6/17 6:16\"   \n[2049] \"5/6/17 6:55\"    \"5/6/17 6:58\"    \"5/6/17 6:59\"    \"5/6/17 11:14\"  \n[2053] \"5/6/17 11:17\"   \"5/6/17 11:17\"   \"5/6/17 11:20\"   \"5/6/17 11:23\"  \n[2057] \"5/6/17 13:00\"   \"5/6/17 13:29\"   \"5/6/17 22:09\"   \"5/6/17 22:09\"  \n[2061] \"5/6/17 23:43\"   \"5/6/17 23:44\"   \"5/7/17 3:14\"    \"5/7/17 4:54\"   \n[2065] \"5/7/17 6:31\"    \"5/7/17 6:31\"    \"5/7/17 6:37\"    \"5/7/17 6:40\"   \n[2069] \"5/7/17 11:01\"   \"5/7/17 11:04\"   \"5/7/17 12:45\"   \"5/7/17 12:46\"  \n[2073] \"5/7/17 12:47\"   \"5/7/17 12:47\"   \"5/7/17 12:48\"   \"5/7/17 12:48\"  \n[2077] \"5/7/17 12:48\"   \"5/7/17 19:28\"   \"5/7/17 23:36\"   \"5/7/17 23:44\"  \n[2081] \"5/8/17 5:26\"    \"5/8/17 6:09\"    \"5/8/17 6:13\"    \"5/8/17 6:21\"   \n[2085] \"5/8/17 6:21\"    \"5/8/17 10:55\"   \"5/8/17 10:56\"   \"5/8/17 12:32\"  \n[2089] \"5/8/17 12:33\"   \"5/8/17 12:34\"   \"5/8/17 12:35\"   \"5/8/17 12:37\"  \n[2093] \"5/8/17 12:37\"   \"5/8/17 12:40\"   \"5/8/17 19:08\"   \"5/8/17 23:25\"  \n[2097] \"5/8/17 23:28\"   \"5/9/17 5:57\"    \"5/9/17 6:52\"    \"5/9/17 6:52\"   \n[2101] \"5/9/17 6:53\"    \"5/9/17 6:54\"    \"5/9/17 7:36\"    \"5/9/17 10:44\"  \n[2105] \"5/9/17 12:22\"   \"5/9/17 12:22\"   \"5/9/17 12:24\"   \"5/9/17 12:26\"  \n[2109] \"5/9/17 13:17\"   \"5/9/17 13:32\"   \"5/9/17 13:34\"   \"5/9/17 23:10\"  \n[2113] \"5/9/17 23:21\"   \"5/10/17 1:38\"   \"5/10/17 2:44\"   \"5/10/17 3:21\"  \n[2117] \"5/10/17 5:36\"   \"5/10/17 6:31\"   \"5/10/17 10:27\"  \"5/10/17 12:11\" \n[2121] \"5/10/17 12:11\"  \"5/10/17 12:15\"  \"5/10/17 12:15\"  \"5/10/17 12:51\" \n[2125] \"5/10/17 12:52\"  \"5/10/17 23:02\"  \"5/10/17 23:04\"  \"5/11/17 2:34\"  \n[2129] \"5/11/17 4:11\"   \"5/11/17 6:03\"   \"5/11/17 6:50\"   \"5/11/17 11:55\" \n[2133] \"5/11/17 11:56\"  \"5/11/17 11:59\"  \"5/11/17 12:00\"  \"5/11/17 12:00\" \n[2137] \"5/11/17 18:06\"  \"5/11/17 22:50\"  \"5/11/17 22:51\"  \"5/11/17 22:51\" \n[2141] \"5/12/17 3:59\"   \"5/12/17 6:29\"   \"5/12/17 6:36\"   \"5/12/17 6:39\"  \n[2145] \"5/12/17 11:46\"  \"5/12/17 11:50\"  \"5/12/17 11:51\"  \"5/12/17 11:51\" \n[2149] \"5/12/17 11:54\"  \"5/12/17 13:40\"  \"5/12/17 15:17\"  \"5/12/17 22:40\" \n[2153] \"5/13/17 0:24\"   \"5/13/17 0:49\"   \"5/13/17 3:54\"   \"5/13/17 6:08\"  \n[2157] \"5/13/17 6:16\"   \"5/13/17 11:35\"  \"5/13/17 11:38\"  \"5/13/17 11:38\" \n[2161] \"5/13/17 11:41\"  \"5/13/17 13:05\"  \"5/13/17 14:55\"  \"5/13/17 19:04\" \n[2165] \"5/14/17 0:12\"   \"5/14/17 3:39\"   \"5/14/17 5:52\"   \"5/14/17 6:50\"  \n[2169] \"5/14/17 11:20\"  \"5/14/17 11:22\"  \"5/14/17 11:25\"  \"5/14/17 11:25\" \n[2173] \"5/14/17 11:25\"  \"5/14/17 14:28\"  \"5/14/17 23:54\"  \"5/15/17 5:34\"  \n[2177] \"5/15/17 6:23\"   \"5/15/17 6:29\"   \"5/15/17 11:12\"  \"5/15/17 11:13\" \n[2181] \"5/15/17 12:54\"  \"5/15/17 17:32\"  \"5/15/17 23:49\"  \"5/15/17 23:50\" \n[2185] \"5/16/17 5:06\"   \"5/16/17 6:02\"   \"5/16/17 6:48\"   \"5/16/17 6:49\"  \n[2189] \"5/16/17 12:41\"  \"5/16/17 12:42\"  \"5/16/17 12:42\"  \"5/16/17 12:46\" \n[2193] \"5/16/17 14:54\"  \"5/16/17 23:41\"  \"5/17/17 4:46\"   \"5/17/17 4:52\"  \n[2197] \"5/17/17 6:33\"   \"5/17/17 10:49\"  \"5/17/17 12:33\"  \"5/17/17 12:34\" \n[2201] \"5/17/17 12:36\"  \"5/17/17 13:14\"  \"5/17/17 23:19\"  \"5/17/17 23:28\" \n[2205] \"5/18/17 4:29\"   \"5/18/17 6:09\"   \"5/18/17 6:10\"   \"5/18/17 10:38\" \n[2209] \"5/18/17 10:39\"  \"5/18/17 12:23\"  \"5/18/17 13:51\"  \"5/18/17 15:33\" \n[2213] \"5/19/17 2:38\"   \"5/19/17 5:43\"   \"5/19/17 6:44\"   \"5/19/17 10:29\" \n[2217] \"5/19/17 12:08\"  \"5/19/17 12:09\"  \"5/19/17 12:09\"  \"5/19/17 12:10\" \n[2221] \"5/19/17 23:02\"  \"5/19/17 23:03\"  \"5/20/17 2:37\"   \"5/20/17 5:25\"  \n[2225] \"5/20/17 5:27\"   \"5/20/17 6:26\"   \"5/20/17 11:52\"  \"5/20/17 11:56\" \n[2229] \"5/20/17 11:59\"  \"5/20/17 11:59\"  \"5/20/17 12:01\"  \"5/21/17 2:06\"  \n[2233] \"5/21/17 6:41\"   \"5/21/17 6:43\"   \"5/21/17 6:44\"   \"5/21/17 10:04\" \n[2237] \"5/21/17 11:46\"  \"5/21/17 11:47\"  \"5/21/17 11:48\"  \"5/22/17 1:05\"  \n[2241] \"5/22/17 6:26\"   \"5/22/17 6:27\"   \"5/22/17 6:27\"   \"5/22/17 11:29\" \n[2245] \"5/22/17 11:34\"  \"5/22/17 11:34\"  \"5/22/17 11:35\"  \"5/22/17 13:29\" \n[2249] \"5/22/17 19:20\"  \"5/23/17 3:32\"   \"5/23/17 6:05\"   \"5/23/17 11:20\" \n[2253] \"5/23/17 11:21\"  \"5/23/17 11:21\"  \"5/24/17 3:26\"   \"5/24/17 6:40\"  \n[2257] \"5/24/17 6:42\"   \"5/24/17 11:06\"  \"5/24/17 11:08\"  \"5/24/17 11:09\" \n[2261] \"5/24/17 11:11\"  \"5/24/17 23:45\"  \"5/24/17 23:46\"  \"5/25/17 1:29\"  \n[2265] \"5/25/17 2:10\"   \"5/25/17 5:19\"   \"5/25/17 6:20\"   \"5/25/17 6:20\"  \n[2269] \"5/25/17 10:58\"  \"5/25/17 10:59\"  \"5/25/17 11:01\"  \"5/25/17 12:40\" \n[2273] \"5/25/17 15:11\"  \"5/26/17 6:41\"   \"5/26/17 6:43\"   \"5/26/17 6:46\"  \n[2277] \"5/26/17 10:50\"  \"5/26/17 12:29\"  \"5/26/17 12:31\"  \"5/26/17 14:16\" \n[2281] \"5/26/17 14:31\"  \"5/26/17 23:23\"  \"5/27/17 6:16\"   \"5/27/17 6:18\"  \n[2285] \"5/27/17 6:21\"   \"5/27/17 10:40\"  \"5/27/17 10:41\"  \"5/27/17 12:18\" \n[2289] \"5/27/17 12:19\"  \"5/27/17 23:11\"  \"5/28/17 5:55\"   \"5/28/17 5:59\"  \n[2293] \"5/28/17 5:59\"   \"5/28/17 6:01\"   \"5/28/17 12:01\"  \"5/28/17 12:02\" \n[2297] \"5/28/17 12:02\"  \"5/28/17 12:06\"  \"5/29/17 6:38\"   \"5/29/17 6:38\"  \n[2301] \"5/29/17 11:55\"  \"5/29/17 11:55\"  \"5/29/17 12:01\"  \"5/30/17 6:16\"  \n[2305] \"5/30/17 6:57\"   \"5/30/17 6:59\"   \"5/30/17 10:04\"  \"5/30/17 11:43\" \n[2309] \"5/30/17 11:44\"  \"5/31/17 5:03\"   \"5/31/17 6:41\"   \"5/31/17 6:43\"  \n[2313] \"5/31/17 11:29\"  \"5/31/17 11:30\"  \"5/31/17 11:36\"  \"6/1/17 4:39\"   \n[2317] \"6/1/17 6:14\"    \"6/1/17 6:19\"    \"6/1/17 11:19\"   \"6/1/17 11:19\"  \n[2321] \"6/1/17 11:22\"   \"6/2/17 2:01\"    \"6/2/17 5:54\"    \"6/2/17 5:57\"   \n[2325] \"6/2/17 11:06\"   \"6/2/17 11:09\"   \"6/2/17 13:14\"   \"6/3/17 4:47\"   \n[2329] \"6/3/17 5:36\"    \"6/3/17 5:41\"    \"6/3/17 12:37\"   \"6/3/17 12:40\"  \n[2333] \"6/3/17 12:41\"   \"6/4/17 6:56\"    \"6/4/17 6:57\"    \"6/4/17 6:57\"   \n[2337] \"6/4/17 10:41\"   \"6/4/17 12:30\"   \"6/5/17 6:32\"    \"6/5/17 6:33\"   \n[2341] \"6/5/17 12:16\"   \"6/5/17 12:18\"   \"6/6/17 6:10\"    \"6/6/17 6:21\"   \n[2345] \"6/6/17 12:00\"   \"6/6/17 12:07\"   \"6/7/17 6:50\"    \"6/7/17 6:52\"   \n[2349] \"6/7/17 11:53\"   \"6/7/17 11:54\"   \"6/7/17 14:30\"   \"6/7/17 22:42\"  \n[2353] \"6/8/17 5:36\"    \"6/8/17 5:37\"    \"6/8/17 5:39\"    \"6/8/17 11:37\"  \n[2357] \"6/8/17 14:00\"   \"6/9/17 3:36\"    \"6/9/17 5:12\"    \"6/9/17 6:51\"   \n[2361] \"6/9/17 11:25\"   \"6/9/17 11:29\"   \"6/9/17 14:07\"   \"6/10/17 4:53\"  \n[2365] \"6/10/17 6:28\"   \"6/10/17 11:16\"  \"6/10/17 11:18\"  \"6/10/17 14:53\" \n[2369] \"6/10/17 23:56\"  \"6/11/17 2:23\"   \"6/11/17 4:31\"   \"6/11/17 6:11\"  \n[2373] \"6/11/17 11:07\"  \"6/11/17 12:49\"  \"6/12/17 5:55\"   \"6/12/17 10:55\" \n[2377] \"6/12/17 12:34\"  \"6/12/17 14:03\"  \"6/12/17 23:30\"  \"6/13/17 2:30\"  \n[2381] \"6/13/17 2:53\"   \"6/13/17 5:28\"   \"6/13/17 10:40\"  \"6/13/17 10:43\" \n[2385] \"6/13/17 10:45\"  \"6/14/17 6:50\"   \"6/14/17 6:55\"   \"6/14/17 10:28\" \n[2389] \"6/14/17 12:12\"  \"6/14/17 14:54\"  \"6/14/17 23:06\"  \"6/15/17 6:24\"  \n[2393] \"6/15/17 6:32\"   \"6/15/17 12:00\"  \"6/15/17 12:04\"  \"6/15/17 14:28\" \n[2397] \"6/16/17 3:53\"   \"6/16/17 6:06\"   \"6/16/17 6:12\"   \"6/16/17 11:45\" \n[2401] \"6/16/17 11:52\"  \"6/16/17 13:50\"  \"6/17/17 0:22\"   \"6/17/17 3:49\"  \n[2405] \"6/17/17 6:45\"   \"6/17/17 11:33\"  \"6/17/17 11:33\"  \"6/18/17 6:25\"  \n[2409] \"6/18/17 6:25\"   \"6/18/17 11:23\"  \"6/18/17 11:29\"  \"6/19/17 6:48\"  \n[2413] \"6/19/17 11:12\"  \"6/19/17 11:12\"  \"6/19/17 11:15\"  \"6/19/17 23:48\" \n[2417] \"6/20/17 3:09\"   \"6/20/17 6:21\"   \"6/20/17 12:44\"  \"6/20/17 14:05\" \n[2421] \"6/20/17 23:33\"  \"6/21/17 6:03\"   \"6/21/17 6:03\"   \"6/21/17 6:59\"  \n[2425] \"6/21/17 12:33\"  \"6/21/17 12:33\"  \"6/21/17 12:34\"  \"6/21/17 15:20\" \n[2429] \"6/21/17 23:28\"  \"6/22/17 5:41\"   \"6/22/17 5:44\"   \"6/22/17 6:42\"  \n[2433] \"6/22/17 6:42\"   \"6/22/17 12:21\"  \"6/22/17 12:23\"  \"6/22/17 12:25\" \n[2437] \"6/22/17 12:27\"  \"6/23/17 0:56\"   \"6/23/17 2:43\"   \"6/23/17 5:23\"  \n[2441] \"6/23/17 6:17\"   \"6/23/17 12:10\"  \"6/23/17 12:11\"  \"6/23/17 13:28\" \n[2445] \"6/23/17 15:32\"  \"6/24/17 5:59\"   \"6/24/17 6:47\"   \"6/24/17 10:20\" \n[2449] \"6/24/17 11:59\"  \"6/25/17 6:17\"   \"6/25/17 6:23\"   \"6/25/17 10:06\" \n[2453] \"6/25/17 11:47\"  \"6/26/17 3:48\"   \"6/26/17 6:01\"   \"6/26/17 11:36\" \n[2457] \"6/26/17 13:36\"  \"6/27/17 3:32\"   \"6/27/17 5:43\"   \"6/28/17 3:15\"  \n[2461] \"6/28/17 11:09\"  \"6/28/17 11:11\"  \"6/29/17 2:30\"   \"6/29/17 6:39\"  \n[2465] \"6/29/17 11:01\"  \"6/29/17 11:06\"  \"6/30/17 4:32\"   \"6/30/17 6:22\"  \n[2469] \"6/30/17 10:51\"  \"7/1/17 5:53\"    \"7/1/17 5:59\"    \"7/1/17 12:17\"  \n[2473] \"7/2/17 5:38\"    \"7/2/17 5:40\"    \"7/2/17 12:04\"   \"7/2/17 12:05\"  \n[2477] \"7/3/17 4:04\"    \"7/3/17 6:54\"    \"7/3/17 11:53\"   \"7/3/17 11:53\"  \n[2481] \"7/4/17 2:05\"    \"7/4/17 6:36\"    \"7/4/17 11:39\"   \"7/5/17 6:09\"   \n[2485] \"7/6/17 2:39\"    \"7/6/17 13:24\"   \"7/7/17 6:29\"    \"7/7/17 11:13\"  \n[2489] \"7/8/17 6:56\"    \"7/8/17 12:39\"   \"7/9/17 4:31\"    \"7/9/17 12:27\"  \n[2493] \"7/10/17 6:06\"   \"7/10/17 12:21\"  \"7/11/17 5:45\"   \"7/11/17 12:06\" \n[2497] \"7/12/17 6:28\"   \"7/12/17 11:57\"  \"7/13/17 3:41\"   \"7/14/17 6:33\"  \n[2501] \"7/15/17 6:13\"   \"7/15/17 13:42\"  \"7/16/17 4:52\"   \"7/17/17 5:31\"  \n[2505] \"7/17/17 12:34\"  \"7/18/17 4:25\"   \"7/18/17 10:43\"  \"7/19/17 2:33\"  \n[2509] \"7/19/17 12:13\"  \"7/20/17 6:08\"   \"7/20/17 12:03\"  \"7/21/17 5:39\"  \n[2513] \"7/21/17 13:56\"  \"7/22/17 5:21\"   \"7/22/17 11:37\"  \"7/23/17 2:36\"  \n[2517] \"7/23/17 11:23\"  \"7/24/17 6:25\"   \"7/24/17 11:14\"  \"7/25/17 4:43\"  \n[2521] \"7/25/17 12:43\"  \"7/26/17 5:45\"   \"7/26/17 10:55\"  \"7/27/17 6:17\"  \n[2525] \"7/27/17 7:00\"   \"7/28/17 6:37\"   \"7/28/17 12:13\"  \"7/29/17 11:58\" \n[2529] \"7/30/17 6:03\"   \"7/30/17 11:47\"  \"7/31/17 5:39\"   \"7/31/17 13:17\" \n[2533] \"8/1/17 5:15\"    \"8/1/17 11:23\"   \"8/2/17 3:18\"    \"8/2/17 11:18\"  \n[2537] \"8/3/17 2:57\"    \"8/5/17 12:25\"   \"8/6/17 6:51\"    \"8/6/17 12:12\"  \n[2541] \"8/7/17 6:33\"    \"8/8/17 6:12\"    \"8/9/17 11:34\"   \"8/10/17 5:30\"  \n[2545] \"8/10/17 13:34\"  \"8/11/17 6:50\"   \"8/12/17 6:30\"   \"8/12/17 12:41\" \n[2549] \"8/13/17 4:30\"   \"8/13/17 10:51\"  \"8/14/17 5:44\"   \"8/14/17 10:36\" \n[2553] \"8/15/17 3:58\"   \"8/15/17 12:09\"  \"8/16/17 6:43\"   \"8/16/17 11:56\" \n[2557] \"8/17/17 3:45\"   \"8/17/17 13:24\"  \"8/18/17 3:32\"   \"8/19/17 5:42\"  \n[2561] \"8/19/17 11:24\"  \"8/26/17 5:01\"   \"8/27/17 6:21\"   \"8/27/17 11:32\" \n[2565] \"8/28/17 11:16\"  \"8/29/17 5:40\"   \"8/30/17 5:19\"   \"8/30/17 10:54\" \n[2569] \"8/31/17 4:21\"   \"3/11/18 5:18\"   \"3/11/18 12:18\"  \"3/11/18 12:24\" \n[2573] \"3/12/18 6:35\"   \"3/12/18 6:36\"   \"3/12/18 6:36\"   \"3/12/18 6:37\"  \n[2577] \"3/12/18 6:38\"   \"3/12/18 6:40\"   \"3/12/18 12:07\"  \"3/12/18 12:13\" \n[2581] \"3/12/18 12:14\"  \"3/13/18 2:43\"   \"3/13/18 6:13\"   \"3/13/18 6:13\"  \n[2585] \"3/13/18 6:15\"   \"3/13/18 6:16\"   \"3/13/18 11:55\"  \"3/13/18 11:58\" \n[2589] \"3/13/18 12:02\"  \"3/13/18 12:04\"  \"3/13/18 13:25\"  \"3/13/18 13:30\" \n[2593] \"3/13/18 19:07\"  \"3/13/18 22:52\"  \"3/14/18 0:33\"   \"3/14/18 5:00\"  \n[2597] \"3/14/18 5:03\"   \"3/14/18 5:55\"   \"3/14/18 6:50\"   \"3/14/18 6:55\"  \n[2601] \"3/14/18 6:57\"   \"3/14/18 11:47\"  \"3/14/18 11:49\"  \"3/14/18 11:50\" \n[2605] \"3/14/18 11:52\"  \"3/14/18 11:56\"  \"3/14/18 14:35\"  \"3/15/18 4:53\"  \n[2609] \"3/15/18 5:31\"   \"3/15/18 5:32\"   \"3/15/18 5:34\"   \"3/15/18 5:36\"  \n[2613] \"3/15/18 5:39\"   \"3/15/18 6:36\"   \"3/15/18 11:34\"  \"3/15/18 11:36\" \n[2617] \"3/15/18 11:37\"  \"3/15/18 11:39\"  \"3/15/18 11:42\"  \"3/15/18 13:16\" \n[2621] \"3/15/18 13:20\"  \"3/16/18 4:11\"   \"3/16/18 5:16\"   \"3/16/18 6:13\"  \n[2625] \"3/16/18 6:14\"   \"3/16/18 6:56\"   \"3/16/18 6:57\"   \"3/16/18 11:22\" \n[2629] \"3/16/18 11:24\"  \"3/16/18 11:25\"  \"3/16/18 11:27\"  \"3/16/18 11:28\" \n[2633] \"3/16/18 13:12\"  \"3/16/18 15:17\"  \"3/16/18 23:55\"  \"3/17/18 2:24\"  \n[2637] \"3/17/18 4:25\"   \"3/17/18 6:29\"   \"3/17/18 6:32\"   \"3/17/18 6:33\"  \n[2641] \"3/17/18 6:35\"   \"3/17/18 6:40\"   \"3/17/18 11:16\"  \"3/17/18 11:16\" \n[2645] \"3/17/18 11:16\"  \"3/17/18 11:16\"  \"3/17/18 12:53\"  \"3/17/18 12:53\" \n[2649] \"3/17/18 13:00\"  \"3/17/18 14:41\"  \"3/17/18 23:53\"  \"3/18/18 6:10\"  \n[2653] \"3/18/18 6:11\"   \"3/18/18 6:12\"   \"3/18/18 6:15\"   \"3/18/18 6:15\"  \n[2657] \"3/18/18 6:16\"   \"3/18/18 6:17\"   \"3/18/18 6:19\"   \"3/18/18 11:02\" \n[2661] \"3/18/18 11:06\"  \"3/18/18 11:07\"  \"3/18/18 12:41\"  \"3/18/18 12:42\" \n[2665] \"3/18/18 12:43\"  \"3/19/18 4:07\"   \"3/19/18 4:29\"   \"3/19/18 5:51\"  \n[2669] \"3/19/18 5:58\"   \"3/19/18 6:52\"   \"3/19/18 6:53\"   \"3/19/18 10:49\" \n[2673] \"3/19/18 12:27\"  \"3/19/18 12:30\"  \"3/19/18 12:31\"  \"3/19/18 12:31\" \n[2677] \"3/19/18 12:31\"  \"3/19/18 12:34\"  \"3/19/18 12:38\"  \"3/19/18 23:27\" \n[2681] \"3/20/18 3:52\"   \"3/20/18 4:03\"   \"3/20/18 4:05\"   \"3/20/18 5:28\"  \n[2685] \"3/20/18 5:29\"   \"3/20/18 5:34\"   \"3/20/18 6:31\"   \"3/20/18 6:33\"  \n[2689] \"3/20/18 7:08\"   \"3/20/18 12:15\"  \"3/20/18 12:18\"  \"3/20/18 12:18\" \n[2693] \"3/20/18 12:19\"  \"3/20/18 12:20\"  \"3/20/18 12:23\"  \"3/21/18 3:47\"  \n[2697] \"3/21/18 5:12\"   \"3/21/18 6:08\"   \"3/21/18 6:45\"   \"3/21/18 6:52\"  \n[2701] \"3/21/18 6:54\"   \"3/21/18 6:55\"   \"3/21/18 10:29\"  \"3/21/18 10:29\" \n[2705] \"3/21/18 12:03\"  \"3/21/18 12:03\"  \"3/21/18 12:05\"  \"3/21/18 12:09\" \n[2709] \"3/21/18 12:10\"  \"3/21/18 22:59\"  \"3/22/18 3:04\"   \"3/22/18 3:22\"  \n[2713] \"3/22/18 3:24\"   \"3/22/18 6:25\"   \"3/22/18 6:25\"   \"3/22/18 6:26\"  \n[2717] \"3/22/18 6:27\"   \"3/22/18 6:31\"   \"3/22/18 12:00\"  \"3/22/18 13:40\" \n[2721] \"3/22/18 13:42\"  \"3/22/18 13:44\"  \"3/22/18 13:48\"  \"3/22/18 13:49\" \n[2725] \"3/23/18 3:14\"   \"3/23/18 4:55\"   \"3/23/18 6:04\"   \"3/23/18 6:11\"  \n[2729] \"3/23/18 6:12\"   \"3/23/18 6:13\"   \"3/23/18 11:40\"  \"3/23/18 11:49\" \n[2733] \"3/23/18 11:50\"  \"3/23/18 11:52\"  \"3/23/18 13:27\"  \"3/23/18 15:41\" \n[2737] \"3/23/18 19:02\"  \"3/23/18 19:05\"  \"3/24/18 4:49\"   \"3/24/18 5:44\"  \n[2741] \"3/24/18 5:45\"   \"3/24/18 5:50\"   \"3/24/18 6:47\"   \"3/24/18 6:49\"  \n[2745] \"3/24/18 6:52\"   \"3/24/18 11:32\"  \"3/24/18 11:34\"  \"3/24/18 11:35\" \n[2749] \"3/24/18 11:36\"  \"3/24/18 11:36\"  \"3/24/18 13:11\"  \"3/24/18 13:19\" \n[2753] \"3/25/18 4:29\"   \"3/25/18 5:29\"   \"3/25/18 6:25\"   \"3/25/18 6:25\"  \n[2757] \"3/25/18 6:27\"   \"3/25/18 6:30\"   \"3/25/18 6:30\"   \"3/25/18 6:31\"  \n[2761] \"3/25/18 11:18\"  \"3/25/18 11:25\"  \"3/25/18 11:26\"  \"3/25/18 13:00\" \n[2765] \"3/25/18 13:01\"  \"3/25/18 13:02\"  \"3/25/18 13:02\"  \"3/25/18 13:49\" \n[2769] \"3/26/18 5:08\"   \"3/26/18 6:44\"   \"3/26/18 6:46\"   \"3/26/18 6:48\"  \n[2773] \"3/26/18 11:06\"  \"3/26/18 11:11\"  \"3/26/18 11:13\"  \"3/26/18 11:14\" \n[2777] \"3/26/18 11:16\"  \"3/26/18 13:20\"  \"3/26/18 13:21\"  \"3/26/18 23:40\" \n[2781] \"3/26/18 23:47\"  \"3/26/18 23:48\"  \"3/27/18 2:03\"   \"3/27/18 4:31\"  \n[2785] \"3/27/18 4:48\"   \"3/27/18 5:46\"   \"3/27/18 6:22\"   \"3/27/18 12:35\" \n[2789] \"3/27/18 12:35\"  \"3/27/18 12:37\"  \"3/27/18 12:42\"  \"3/27/18 12:42\" \n[2793] \"3/27/18 12:45\"  \"3/27/18 14:02\"  \"3/27/18 23:29\"  \"3/27/18 23:32\" \n[2797] \"3/27/18 23:35\"  \"3/28/18 2:30\"   \"3/28/18 4:03\"   \"3/28/18 5:35\"  \n[2801] \"3/28/18 5:41\"   \"3/28/18 6:04\"   \"3/28/18 6:07\"   \"3/28/18 6:12\"  \n[2805] \"3/28/18 7:48\"   \"3/28/18 12:23\"  \"3/28/18 12:27\"  \"3/28/18 12:29\" \n[2809] \"3/28/18 12:31\"  \"3/28/18 12:33\"  \"3/28/18 13:35\"  \"3/28/18 13:36\" \n[2813] \"3/28/18 19:02\"  \"3/29/18 3:44\"   \"3/29/18 3:46\"   \"3/29/18 5:43\"  \n[2817] \"3/29/18 5:44\"   \"3/29/18 6:43\"   \"3/29/18 6:47\"   \"3/29/18 10:40\" \n[2821] \"3/29/18 12:18\"  \"3/29/18 12:19\"  \"3/29/18 12:19\"  \"3/29/18 13:10\" \n[2825] \"3/29/18 13:13\"  \"3/29/18 13:24\"  \"3/29/18 23:06\"  \"3/29/18 23:13\" \n[2829] \"3/30/18 4:56\"   \"3/30/18 5:12\"   \"3/30/18 5:16\"   \"3/30/18 5:21\"  \n[2833] \"3/30/18 5:28\"   \"3/30/18 6:24\"   \"3/30/18 10:25\"  \"3/30/18 10:25\" \n[2837] \"3/30/18 12:01\"  \"3/30/18 12:03\"  \"3/30/18 12:05\"  \"3/30/18 12:11\" \n[2841] \"3/30/18 16:29\"  \"3/30/18 22:55\"  \"3/31/18 4:59\"   \"3/31/18 5:07\"  \n[2845] \"3/31/18 6:41\"   \"3/31/18 6:44\"   \"3/31/18 6:45\"   \"3/31/18 11:56\" \n[2849] \"3/31/18 11:56\"  \"3/31/18 11:57\"  \"3/31/18 13:33\"  \"3/31/18 13:35\" \n[2853] \"4/1/18 4:45\"    \"4/1/18 6:18\"    \"4/1/18 6:25\"    \"4/1/18 6:26\"   \n[2857] \"4/1/18 6:29\"    \"4/1/18 11:43\"   \"4/1/18 11:46\"   \"4/1/18 11:48\"  \n[2861] \"4/1/18 13:25\"   \"4/1/18 13:34\"   \"4/1/18 19:14\"   \"4/1/18 19:15\"  \n[2865] \"4/2/18 2:19\"    \"4/2/18 4:38\"    \"4/2/18 4:39\"    \"4/2/18 4:44\"   \n[2869] \"4/2/18 4:44\"    \"4/2/18 5:56\"    \"4/2/18 5:58\"    \"4/2/18 11:27\"  \n[2873] \"4/2/18 11:30\"   \"4/2/18 11:33\"   \"4/2/18 11:33\"   \"4/2/18 11:34\"  \n[2877] \"4/2/18 11:34\"   \"4/2/18 23:57\"   \"4/3/18 4:30\"    \"4/3/18 4:32\"   \n[2881] \"4/3/18 5:37\"    \"4/3/18 5:47\"    \"4/3/18 6:42\"    \"4/3/18 11:18\"  \n[2885] \"4/3/18 11:19\"   \"4/3/18 11:20\"   \"4/3/18 12:57\"   \"4/3/18 12:59\"  \n[2889] \"4/3/18 13:02\"   \"4/3/18 13:03\"   \"4/3/18 23:51\"   \"4/4/18 5:56\"   \n[2893] \"4/4/18 5:57\"    \"4/4/18 5:59\"    \"4/4/18 6:24\"    \"4/4/18 6:24\"   \n[2897] \"4/4/18 6:59\"    \"4/4/18 11:04\"   \"4/4/18 11:13\"   \"4/4/18 12:45\"  \n[2901] \"4/4/18 12:48\"   \"4/4/18 12:48\"   \"4/4/18 12:49\"   \"4/4/18 12:55\"  \n[2905] \"4/4/18 23:41\"   \"4/5/18 5:53\"    \"4/5/18 6:03\"    \"4/5/18 6:36\"   \n[2909] \"4/5/18 6:40\"    \"4/5/18 6:46\"    \"4/5/18 12:32\"   \"4/5/18 12:33\"  \n[2913] \"4/5/18 12:35\"   \"4/5/18 12:35\"   \"4/5/18 12:40\"   \"4/5/18 12:41\"  \n[2917] \"4/5/18 12:41\"   \"4/5/18 23:28\"   \"4/5/18 23:28\"   \"4/5/18 23:33\"  \n[2921] \"4/6/18 3:55\"    \"4/6/18 5:35\"    \"4/6/18 6:14\"    \"4/6/18 6:15\"   \n[2925] \"4/6/18 6:21\"    \"4/6/18 12:20\"   \"4/6/18 12:21\"   \"4/6/18 12:21\"  \n[2929] \"4/6/18 12:21\"   \"4/6/18 12:26\"   \"4/6/18 12:28\"   \"4/6/18 19:09\"  \n[2933] \"4/6/18 19:15\"   \"4/7/18 5:55\"    \"4/7/18 5:58\"    \"4/7/18 5:59\"   \n[2937] \"4/7/18 6:00\"    \"4/7/18 6:00\"    \"4/7/18 6:01\"    \"4/7/18 6:56\"   \n[2941] \"4/7/18 7:40\"    \"4/7/18 12:10\"   \"4/7/18 12:10\"   \"4/7/18 12:16\"  \n[2945] \"4/7/18 12:17\"   \"4/7/18 14:30\"   \"4/8/18 2:54\"    \"4/8/18 5:15\"   \n[2949] \"4/8/18 5:33\"    \"4/8/18 5:43\"    \"4/8/18 5:44\"    \"4/8/18 5:45\"   \n[2953] \"4/8/18 6:39\"    \"4/8/18 6:41\"    \"4/8/18 11:57\"   \"4/8/18 11:59\"  \n[2957] \"4/8/18 12:04\"   \"4/8/18 12:06\"   \"4/8/18 12:06\"   \"4/8/18 13:12\"  \n[2961] \"4/8/18 13:43\"   \"4/9/18 5:05\"    \"4/9/18 5:14\"    \"4/9/18 6:13\"   \n[2965] \"4/9/18 6:19\"    \"4/9/18 6:21\"    \"4/9/18 6:56\"    \"4/9/18 6:56\"   \n[2969] \"4/9/18 6:57\"    \"4/9/18 7:02\"    \"4/9/18 11:46\"   \"4/9/18 11:51\"  \n[2973] \"4/9/18 11:52\"   \"4/9/18 11:52\"   \"4/9/18 11:53\"   \"4/9/18 11:57\"  \n[2977] \"4/10/18 4:51\"   \"4/10/18 4:56\"   \"4/10/18 6:00\"   \"4/10/18 6:32\"  \n[2981] \"4/10/18 6:34\"   \"4/10/18 6:39\"   \"4/10/18 6:40\"   \"4/10/18 6:42\"  \n[2985] \"4/10/18 11:34\"  \"4/10/18 11:37\"  \"4/10/18 11:40\"  \"4/10/18 11:41\" \n[2989] \"4/10/18 11:44\"  \"4/10/18 13:18\"  \"4/10/18 13:48\"  \"4/11/18 2:37\"  \n[2993] \"4/11/18 2:41\"   \"4/11/18 4:36\"   \"4/11/18 4:37\"   \"4/11/18 4:37\"  \n[2997] \"4/11/18 6:16\"   \"4/11/18 6:17\"   \"4/11/18 6:21\"   \"4/11/18 11:23\" \n[3001] \"4/11/18 11:26\"  \"4/11/18 11:27\"  \"4/11/18 11:29\"  \"4/11/18 11:31\" \n[3005] \"4/11/18 11:31\"  \"4/11/18 11:34\"  \"4/11/18 13:06\"  \"4/12/18 2:03\"  \n[3009] \"4/12/18 2:45\"   \"4/12/18 4:30\"   \"4/12/18 4:30\"   \"4/12/18 6:05\"  \n[3013] \"4/12/18 6:56\"   \"4/12/18 7:36\"   \"4/12/18 11:15\"  \"4/12/18 11:17\" \n[3017] \"4/12/18 11:17\"  \"4/12/18 11:18\"  \"4/12/18 12:58\"  \"4/12/18 12:58\" \n[3021] \"4/12/18 13:57\"  \"4/12/18 23:44\"  \"4/13/18 5:33\"   \"4/13/18 5:34\"  \n[3025] \"4/13/18 5:37\"   \"4/13/18 5:54\"   \"4/13/18 6:33\"   \"4/13/18 12:43\" \n[3029] \"4/13/18 12:44\"  \"4/13/18 12:44\"  \"4/13/18 12:47\"  \"4/13/18 12:48\" \n[3033] \"4/13/18 12:51\"  \"4/13/18 23:36\"  \"4/14/18 5:17\"   \"4/14/18 5:17\"  \n[3037] \"4/14/18 6:18\"   \"4/14/18 6:49\"   \"4/14/18 6:57\"   \"4/14/18 10:50\" \n[3041] \"4/14/18 12:28\"  \"4/14/18 12:30\"  \"4/14/18 12:30\"  \"4/14/18 12:32\" \n[3045] \"4/14/18 12:35\"  \"4/14/18 12:38\"  \"4/14/18 23:20\"  \"4/14/18 23:28\" \n[3049] \"4/15/18 3:45\"   \"4/15/18 5:28\"   \"4/15/18 5:54\"   \"4/15/18 6:29\"  \n[3053] \"4/15/18 6:30\"   \"4/15/18 6:37\"   \"4/15/18 12:16\"  \"4/15/18 12:17\" \n[3057] \"4/15/18 12:21\"  \"4/15/18 12:23\"  \"4/15/18 15:11\"  \"4/15/18 23:17\" \n[3061] \"4/16/18 1:44\"   \"4/16/18 2:53\"   \"4/16/18 3:21\"   \"4/16/18 6:11\"  \n[3065] \"4/16/18 6:12\"   \"4/16/18 6:14\"   \"4/16/18 6:17\"   \"4/16/18 10:30\" \n[3069] \"4/16/18 12:05\"  \"4/16/18 12:07\"  \"4/16/18 12:09\"  \"4/16/18 12:10\" \n[3073] \"4/16/18 14:00\"  \"4/17/18 2:48\"   \"4/17/18 5:47\"   \"4/17/18 5:51\"  \n[3077] \"4/17/18 5:55\"   \"4/17/18 5:57\"   \"4/17/18 5:57\"   \"4/17/18 6:50\"  \n[3081] \"4/17/18 11:53\"  \"4/17/18 11:59\"  \"4/17/18 11:59\"  \"4/17/18 12:00\" \n[3085] \"4/17/18 12:01\"  \"4/17/18 12:01\"  \"4/17/18 13:30\"  \"4/18/18 2:04\"  \n[3089] \"4/18/18 3:13\"   \"4/18/18 3:43\"   \"4/18/18 4:55\"   \"4/18/18 4:58\"  \n[3093] \"4/18/18 5:00\"   \"4/18/18 6:25\"   \"4/18/18 6:32\"   \"4/18/18 6:35\"  \n[3097] \"4/18/18 11:41\"  \"4/18/18 11:43\"  \"4/18/18 11:45\"  \"4/18/18 11:47\" \n[3101] \"4/18/18 11:53\"  \"4/18/18 11:53\"  \"4/18/18 13:09\"  \"4/18/18 13:26\" \n[3105] \"4/19/18 3:11\"   \"4/19/18 4:48\"   \"4/19/18 5:16\"   \"4/19/18 6:47\"  \n[3109] \"4/19/18 6:48\"   \"4/19/18 6:50\"   \"4/19/18 6:54\"   \"4/19/18 6:54\"  \n[3113] \"4/19/18 11:31\"  \"4/19/18 11:36\"  \"4/19/18 11:37\"  \"4/19/18 11:39\" \n[3117] \"4/19/18 11:41\"  \"4/19/18 13:17\"  \"4/20/18 2:51\"   \"4/20/18 4:46\"  \n[3121] \"4/20/18 5:52\"   \"4/20/18 6:25\"   \"4/20/18 6:28\"   \"4/20/18 6:30\"  \n[3125] \"4/20/18 6:33\"   \"4/20/18 11:22\"  \"4/20/18 11:25\"  \"4/20/18 11:27\" \n[3129] \"4/20/18 11:30\"  \"4/20/18 13:03\"  \"4/20/18 13:06\"  \"4/20/18 13:39\" \n[3133] \"4/20/18 14:00\"  \"4/20/18 23:57\"  \"4/21/18 4:29\"   \"4/21/18 5:57\"  \n[3137] \"4/21/18 6:01\"   \"4/21/18 6:02\"   \"4/21/18 6:05\"   \"4/21/18 6:05\"  \n[3141] \"4/21/18 6:10\"   \"4/21/18 6:13\"   \"4/21/18 6:14\"   \"4/21/18 11:12\" \n[3145] \"4/21/18 11:13\"  \"4/21/18 11:14\"  \"4/21/18 11:15\"  \"4/21/18 11:18\" \n[3149] \"4/21/18 11:19\"  \"4/21/18 12:57\"  \"4/22/18 4:02\"   \"4/22/18 5:42\"  \n[3153] \"4/22/18 5:48\"   \"4/22/18 5:49\"   \"4/22/18 5:49\"   \"4/22/18 5:51\"  \n[3157] \"4/22/18 6:49\"   \"4/22/18 7:30\"   \"4/22/18 12:36\"  \"4/22/18 12:41\" \n[3161] \"4/22/18 12:41\"  \"4/22/18 12:41\"  \"4/22/18 12:42\"  \"4/22/18 12:44\" \n[3165] \"4/22/18 23:32\"  \"4/23/18 3:52\"   \"4/23/18 3:57\"   \"4/23/18 5:24\"  \n[3169] \"4/23/18 5:25\"   \"4/23/18 5:34\"   \"4/23/18 5:36\"   \"4/23/18 5:37\"  \n[3173] \"4/23/18 6:26\"   \"4/23/18 6:30\"   \"4/23/18 10:51\"  \"4/23/18 12:25\" \n[3177] \"4/23/18 12:26\"  \"4/23/18 12:27\"  \"4/23/18 12:28\"  \"4/23/18 12:31\" \n[3181] \"4/23/18 12:32\"  \"4/23/18 12:33\"  \"4/23/18 13:48\"  \"4/24/18 3:46\"  \n[3185] \"4/24/18 5:22\"   \"4/24/18 5:27\"   \"4/24/18 6:42\"   \"4/24/18 6:43\"  \n[3189] \"4/24/18 6:44\"   \"4/24/18 6:46\"   \"4/24/18 6:48\"   \"4/24/18 6:49\"  \n[3193] \"4/24/18 12:13\"  \"4/24/18 12:14\"  \"4/24/18 12:15\"  \"4/24/18 12:15\" \n[3197] \"4/24/18 12:18\"  \"4/24/18 12:19\"  \"4/24/18 12:19\"  \"4/24/18 12:21\" \n[3201] \"4/24/18 12:21\"  \"4/25/18 2:25\"   \"4/25/18 3:38\"   \"4/25/18 4:44\"  \n[3205] \"4/25/18 4:46\"   \"4/25/18 4:50\"   \"4/25/18 5:48\"   \"4/25/18 6:22\"  \n[3209] \"4/25/18 6:26\"   \"4/25/18 6:31\"   \"4/25/18 7:28\"   \"4/25/18 12:04\" \n[3213] \"4/25/18 12:06\"  \"4/25/18 12:06\"  \"4/25/18 12:08\"  \"4/25/18 12:10\" \n[3217] \"4/26/18 3:09\"   \"4/26/18 3:43\"   \"4/26/18 5:21\"   \"4/26/18 6:08\"  \n[3221] \"4/26/18 6:12\"   \"4/26/18 11:50\"  \"4/26/18 11:59\"  \"4/26/18 11:59\" \n[3225] \"4/26/18 12:00\"  \"4/26/18 13:37\"  \"4/26/18 13:39\"  \"4/26/18 19:00\" \n[3229] \"4/27/18 2:40\"   \"4/27/18 2:44\"   \"4/27/18 3:16\"   \"4/27/18 4:52\"  \n[3233] \"4/27/18 5:42\"   \"4/27/18 5:47\"   \"4/27/18 6:45\"   \"4/27/18 6:46\"  \n[3237] \"4/27/18 6:46\"   \"4/27/18 11:43\"  \"4/27/18 11:43\"  \"4/27/18 11:44\" \n[3241] \"4/27/18 11:46\"  \"4/27/18 11:50\"  \"4/27/18 13:19\"  \"4/27/18 13:20\" \n[3245] \"4/27/18 13:23\"  \"4/28/18 2:55\"   \"4/28/18 4:36\"   \"4/28/18 5:21\"  \n[3249] \"4/28/18 6:25\"   \"4/28/18 6:27\"   \"4/28/18 6:27\"   \"4/28/18 6:29\"  \n[3253] \"4/28/18 6:58\"   \"4/28/18 11:27\"  \"4/28/18 11:31\"  \"4/28/18 11:31\" \n[3257] \"4/28/18 11:32\"  \"4/28/18 11:35\"  \"4/28/18 11:35\"  \"4/28/18 13:10\" \n[3261] \"4/28/18 13:19\"  \"4/29/18 4:32\"   \"4/29/18 5:04\"   \"4/29/18 5:59\"  \n[3265] \"4/29/18 6:42\"   \"4/29/18 6:46\"   \"4/29/18 6:47\"   \"4/29/18 11:16\" \n[3269] \"4/29/18 11:17\"  \"4/29/18 11:21\"  \"4/29/18 11:22\"  \"4/29/18 11:22\" \n[3273] \"4/29/18 11:22\"  \"4/29/18 11:24\"  \"4/29/18 11:24\"  \"4/29/18 23:56\" \n[3277] \"4/30/18 3:40\"   \"4/30/18 4:44\"   \"4/30/18 6:16\"   \"4/30/18 6:17\"  \n[3281] \"4/30/18 6:24\"   \"4/30/18 6:27\"   \"4/30/18 11:10\"  \"4/30/18 11:14\" \n[3285] \"4/30/18 12:44\"  \"4/30/18 12:45\"  \"4/30/18 12:46\"  \"4/30/18 12:51\" \n[3289] \"5/1/18 3:54\"    \"5/1/18 5:40\"    \"5/1/18 5:48\"    \"5/1/18 6:07\"   \n[3293] \"5/1/18 6:57\"    \"5/1/18 12:32\"   \"5/1/18 12:33\"   \"5/1/18 12:38\"  \n[3297] \"5/1/18 12:39\"   \"5/1/18 12:40\"   \"5/1/18 12:43\"   \"5/1/18 23:26\"  \n[3301] \"5/2/18 5:31\"    \"5/2/18 5:35\"    \"5/2/18 5:45\"    \"5/2/18 6:39\"   \n[3305] \"5/2/18 6:43\"    \"5/2/18 6:44\"    \"5/2/18 6:44\"    \"5/2/18 12:21\"  \n[3309] \"5/2/18 12:26\"   \"5/2/18 12:26\"   \"5/2/18 12:29\"   \"5/2/18 12:30\"  \n[3313] \"5/2/18 13:57\"   \"5/2/18 14:00\"   \"5/3/18 3:42\"    \"5/3/18 5:22\"   \n[3317] \"5/3/18 6:20\"    \"5/3/18 6:56\"    \"5/3/18 6:58\"    \"5/3/18 10:34\"  \n[3321] \"5/3/18 10:38\"   \"5/3/18 12:10\"   \"5/3/18 12:11\"   \"5/3/18 12:16\"  \n[3325] \"5/4/18 3:42\"    \"5/4/18 5:56\"    \"5/4/18 6:37\"    \"5/4/18 6:38\"   \n[3329] \"5/4/18 6:44\"    \"5/4/18 6:45\"    \"5/4/18 11:58\"   \"5/4/18 12:02\"  \n[3333] \"5/4/18 12:02\"   \"5/4/18 12:05\"   \"5/4/18 12:11\"   \"5/4/18 22:55\"  \n[3337] \"5/5/18 3:18\"    \"5/5/18 3:33\"    \"5/5/18 4:56\"    \"5/5/18 5:03\"   \n[3341] \"5/5/18 6:18\"    \"5/5/18 11:51\"   \"5/5/18 11:52\"   \"5/5/18 11:54\"  \n[3345] \"5/5/18 11:58\"   \"5/5/18 11:59\"   \"5/5/18 19:12\"   \"5/6/18 4:51\"   \n[3349] \"5/6/18 5:13\"    \"5/6/18 5:56\"    \"5/6/18 5:58\"    \"5/6/18 5:59\"   \n[3353] \"5/6/18 6:58\"    \"5/6/18 11:37\"   \"5/6/18 11:43\"   \"5/6/18 11:45\"  \n[3357] \"5/6/18 11:46\"   \"5/6/18 13:36\"   \"5/7/18 2:22\"    \"5/7/18 4:08\"   \n[3361] \"5/7/18 4:38\"    \"5/7/18 5:35\"    \"5/7/18 6:36\"    \"5/7/18 6:41\"   \n[3365] \"5/7/18 11:27\"   \"5/7/18 11:28\"   \"5/7/18 11:29\"   \"5/7/18 11:31\"  \n[3369] \"5/8/18 4:18\"    \"5/8/18 4:25\"    \"5/8/18 6:51\"    \"5/8/18 6:55\"   \n[3373] \"5/8/18 6:57\"    \"5/8/18 6:58\"    \"5/8/18 11:13\"   \"5/8/18 11:18\"  \n[3377] \"5/8/18 11:19\"   \"5/8/18 11:20\"   \"5/8/18 11:24\"   \"5/8/18 13:03\"  \n[3381] \"5/9/18 3:19\"    \"5/9/18 4:56\"    \"5/9/18 5:50\"    \"5/9/18 5:50\"   \n[3385] \"5/9/18 5:50\"    \"5/9/18 6:39\"    \"5/9/18 11:06\"   \"5/9/18 12:41\"  \n[3389] \"5/9/18 12:42\"   \"5/9/18 12:42\"   \"5/10/18 4:35\"   \"5/10/18 5:39\"  \n[3393] \"5/10/18 5:42\"   \"5/10/18 6:12\"   \"5/10/18 6:13\"   \"5/10/18 12:29\" \n[3397] \"5/10/18 12:33\"  \"5/10/18 12:36\"  \"5/10/18 12:37\"  \"5/10/18 13:59\" \n[3401] \"5/11/18 3:41\"   \"5/11/18 5:56\"   \"5/11/18 6:49\"   \"5/11/18 6:55\"  \n[3405] \"5/11/18 6:55\"   \"5/11/18 7:35\"   \"5/11/18 12:18\"  \"5/11/18 12:21\" \n[3409] \"5/11/18 12:27\"  \"5/12/18 3:15\"   \"5/12/18 5:30\"   \"5/12/18 5:35\"  \n[3413] \"5/12/18 5:37\"   \"5/12/18 5:40\"   \"5/12/18 6:34\"   \"5/12/18 12:07\" \n[3417] \"5/12/18 12:10\"  \"5/12/18 12:11\"  \"5/12/18 12:15\"  \"5/12/18 13:52\" \n[3421] \"5/12/18 13:57\"  \"5/13/18 5:03\"   \"5/13/18 5:08\"   \"5/13/18 6:10\"  \n[3425] \"5/13/18 6:49\"   \"5/13/18 6:50\"   \"5/13/18 6:57\"   \"5/13/18 10:21\" \n[3429] \"5/13/18 11:57\"  \"5/13/18 11:57\"  \"5/13/18 12:03\"  \"5/13/18 13:21\" \n[3433] \"5/14/18 5:55\"   \"5/14/18 6:28\"   \"5/14/18 6:29\"   \"5/14/18 6:31\"  \n[3437] \"5/14/18 6:37\"   \"5/14/18 11:46\"  \"5/14/18 11:47\"  \"5/14/18 11:47\" \n[3441] \"5/15/18 4:31\"   \"5/15/18 4:36\"   \"5/15/18 5:28\"   \"5/15/18 5:28\"  \n[3445] \"5/15/18 6:10\"   \"5/15/18 11:32\"  \"5/15/18 11:35\"  \"5/15/18 13:14\" \n[3449] \"5/15/18 13:17\"  \"5/15/18 14:01\"  \"5/16/18 3:39\"   \"5/16/18 3:45\"  \n[3453] \"5/16/18 4:35\"   \"5/16/18 5:49\"   \"5/16/18 6:51\"   \"5/16/18 11:23\" \n[3457] \"5/16/18 11:26\"  \"5/16/18 11:27\"  \"5/16/18 13:12\"  \"5/16/18 13:32\" \n[3461] \"5/16/18 13:32\"  \"5/17/18 5:29\"   \"5/17/18 5:32\"   \"5/17/18 5:34\"  \n[3465] \"5/17/18 5:55\"   \"5/17/18 5:56\"   \"5/17/18 6:28\"   \"5/17/18 7:08\"  \n[3469] \"5/17/18 11:16\"  \"5/17/18 11:20\"  \"5/17/18 12:51\"  \"5/17/18 12:54\" \n[3473] \"5/17/18 12:57\"  \"5/18/18 6:07\"   \"5/18/18 6:12\"   \"5/18/18 6:52\"  \n[3477] \"5/18/18 6:52\"   \"5/18/18 12:40\"  \"5/18/18 12:41\"  \"5/18/18 12:47\" \n[3481] \"5/18/18 14:00\"  \"5/19/18 5:35\"   \"5/19/18 5:52\"   \"5/19/18 5:53\"  \n[3485] \"5/19/18 6:27\"   \"5/19/18 6:32\"   \"5/19/18 12:32\"  \"5/19/18 12:35\" \n[3489] \"5/19/18 12:35\"  \"5/19/18 13:32\"  \"5/19/18 13:37\"  \"5/20/18 5:25\"  \n[3493] \"5/20/18 5:29\"   \"5/20/18 6:05\"   \"5/20/18 6:14\"   \"5/20/18 10:41\" \n[3497] \"5/20/18 12:17\"  \"5/20/18 12:19\"  \"5/20/18 12:20\"  \"5/20/18 12:22\" \n[3501] \"5/21/18 5:52\"   \"5/21/18 5:53\"   \"5/21/18 6:46\"   \"5/21/18 6:48\"  \n[3505] \"5/21/18 6:50\"   \"5/21/18 7:27\"   \"5/21/18 12:05\"  \"5/21/18 13:48\" \n[3509] \"5/21/18 13:49\"  \"5/21/18 13:50\"  \"5/22/18 5:04\"   \"5/22/18 5:26\"  \n[3513] \"5/22/18 5:31\"   \"5/22/18 5:32\"   \"5/22/18 6:29\"   \"5/22/18 7:07\"  \n[3517] \"5/22/18 11:51\"  \"5/22/18 11:53\"  \"5/22/18 11:53\"  \"5/22/18 11:56\" \n[3521] \"5/23/18 2:26\"   \"5/23/18 5:06\"   \"5/23/18 6:42\"   \"5/23/18 6:42\"  \n[3525] \"5/23/18 6:44\"   \"5/23/18 11:42\"  \"5/23/18 11:44\"  \"5/23/18 11:46\" \n[3529] \"5/23/18 11:46\"  \"5/24/18 2:02\"   \"5/24/18 6:21\"   \"5/24/18 6:24\"  \n[3533] \"5/24/18 6:26\"   \"5/24/18 6:32\"   \"5/24/18 11:30\"  \"5/24/18 11:33\" \n[3537] \"5/24/18 11:34\"  \"5/24/18 11:36\"  \"5/25/18 3:01\"   \"5/25/18 3:05\"  \n[3541] \"5/25/18 6:06\"   \"5/25/18 6:09\"   \"5/25/18 11:19\"  \"5/25/18 11:20\" \n[3545] \"5/25/18 11:26\"  \"5/25/18 13:04\"  \"5/26/18 4:09\"   \"5/26/18 5:46\"  \n[3549] \"5/26/18 6:42\"   \"5/26/18 6:46\"   \"5/26/18 7:27\"   \"5/26/18 12:50\" \n[3553] \"5/26/18 12:50\"  \"5/26/18 12:52\"  \"5/27/18 5:21\"   \"5/27/18 6:25\"  \n[3557] \"5/27/18 6:26\"   \"5/27/18 6:27\"   \"5/27/18 6:28\"   \"5/27/18 11:00\" \n[3561] \"5/27/18 12:41\"  \"5/27/18 12:44\"  \"5/28/18 6:00\"   \"5/28/18 6:00\"  \n[3565] \"5/28/18 6:38\"   \"5/28/18 6:39\"   \"5/28/18 6:40\"   \"5/28/18 12:22\" \n[3569] \"5/28/18 12:24\"  \"5/28/18 12:30\"  \"5/28/18 12:32\"  \"5/28/18 12:35\" \n[3573] \"5/29/18 5:39\"   \"5/29/18 6:18\"   \"5/29/18 6:21\"   \"5/29/18 6:24\"  \n[3577] \"5/29/18 6:26\"   \"5/29/18 12:12\"  \"5/29/18 12:18\"  \"5/29/18 12:19\" \n[3581] \"5/29/18 12:19\"  \"5/29/18 12:20\"  \"5/30/18 5:06\"   \"5/30/18 5:07\"  \n[3585] \"5/30/18 5:59\"   \"5/30/18 6:06\"   \"5/30/18 6:57\"   \"5/30/18 12:07\" \n[3589] \"5/30/18 13:45\"  \"5/30/18 13:47\"  \"5/30/18 13:58\"  \"5/31/18 5:37\"  \n[3593] \"5/31/18 5:37\"   \"5/31/18 5:45\"   \"5/31/18 5:46\"   \"5/31/18 6:45\"  \n[3597] \"5/31/18 11:48\"  \"5/31/18 11:53\"  \"5/31/18 11:57\"  \"5/31/18 14:04\" \n[3601] \"5/31/18 14:05\"  \"6/5/18 3:57\"    \"6/5/18 6:38\"    \"6/5/18 6:40\"   \n[3605] \"6/5/18 6:41\"    \"6/5/18 12:33\"   \"6/5/18 12:33\"   \"6/5/18 12:34\"  \n[3609] \"6/6/18 5:27\"    \"6/6/18 6:18\"    \"6/6/18 6:20\"    \"6/6/18 6:54\"   \n[3613] \"6/6/18 7:00\"    \"6/7/18 5:15\"    \"6/7/18 5:18\"    \"6/7/18 6:33\"   \n[3617] \"6/7/18 6:36\"    \"6/7/18 12:07\"   \"6/7/18 12:07\"   \"6/7/18 12:13\"  \n[3621] \"6/7/18 13:57\"   \"6/8/18 5:03\"    \"6/8/18 6:11\"    \"6/8/18 6:12\"   \n[3625] \"6/8/18 6:12\"    \"6/8/18 11:59\"   \"6/8/18 13:10\"   \"6/8/18 13:12\"  \n[3629] \"6/9/18 4:49\"    \"6/9/18 5:58\"    \"6/9/18 6:56\"    \"6/9/18 6:59\"   \n[3633] \"6/9/18 11:44\"   \"6/9/18 11:52\"   \"6/10/18 3:14\"   \"6/10/18 4:41\"  \n[3637] \"6/10/18 4:44\"   \"6/10/18 6:36\"   \"6/10/18 11:32\"  \"6/10/18 13:20\" \n[3641] \"6/10/18 13:20\"  \"6/11/18 5:16\"   \"6/11/18 6:14\"   \"6/11/18 6:53\"  \n[3645] \"6/11/18 6:55\"   \"6/11/18 11:25\"  \"6/11/18 11:25\"  \"6/11/18 11:25\" \n[3649] \"6/11/18 11:29\"  \"6/12/18 2:29\"   \"6/12/18 5:56\"   \"6/12/18 6:27\"  \n[3653] \"6/12/18 6:31\"   \"6/12/18 11:13\"  \"6/12/18 11:16\"  \"6/12/18 11:19\" \n[3657] \"6/12/18 13:30\"  \"6/13/18 6:06\"   \"6/13/18 6:08\"   \"6/13/18 6:11\"  \n[3661] \"6/13/18 6:14\"   \"6/13/18 7:13\"   \"6/13/18 11:01\"  \"6/13/18 11:02\" \n[3665] \"6/13/18 12:39\"  \"6/13/18 12:41\"  \"6/14/18 3:53\"   \"6/14/18 5:06\"  \n[3669] \"6/14/18 5:35\"   \"6/14/18 5:46\"   \"6/14/18 5:56\"   \"6/14/18 7:29\"  \n[3673] \"6/14/18 7:30\"   \"6/14/18 7:32\"   \"6/14/18 10:50\"  \"6/14/18 12:39\" \n[3677] \"6/24/18 4:03\"   \"6/24/18 4:06\"   \"6/24/18 5:47\"   \"6/24/18 5:48\"  \n[3681] \"6/24/18 6:47\"   \"6/24/18 12:13\"  \"6/24/18 12:17\"  \"6/24/18 13:31\" \n[3685] \"6/25/18 3:37\"   \"6/25/18 5:08\"   \"6/25/18 5:25\"   \"6/25/18 6:21\"  \n[3689] \"6/25/18 12:00\"  \"6/25/18 12:06\"  \"6/25/18 12:06\"  \"6/25/18 12:08\" \n[3693] \"6/25/18 13:49\"  \"6/26/18 5:57\"   \"6/26/18 6:39\"   \"6/26/18 6:42\"  \n[3697] \"6/26/18 6:47\"   \"6/26/18 6:47\"   \"6/26/18 11:50\"  \"6/26/18 11:50\" \n[3701] \"6/26/18 11:52\"  \"6/26/18 11:54\"  \"6/26/18 13:32\"  \"6/27/18 4:27\"  \n[3705] \"6/27/18 4:45\"   \"6/27/18 5:34\"   \"6/27/18 6:21\"   \"6/27/18 11:40\" \n[3709] \"6/27/18 11:41\"  \"6/27/18 11:43\"  \"6/27/18 11:44\"  \"6/27/18 13:16\" \n[3713] \"6/28/18 4:19\"   \"6/28/18 5:56\"   \"6/28/18 6:01\"   \"6/28/18 6:03\"  \n[3717] \"6/28/18 6:04\"   \"6/28/18 11:33\"  \"6/28/18 11:33\"  \"6/28/18 11:33\" \n[3721] \"6/28/18 13:09\"  \"6/28/18 13:13\"  \"6/29/18 4:22\"   \"6/29/18 5:36\"  \n[3725] \"6/29/18 5:43\"   \"6/29/18 5:45\"   \"6/29/18 6:43\"   \"6/29/18 11:17\" \n[3729] \"6/29/18 11:17\"  \"6/29/18 11:20\"  \"6/29/18 11:20\"  \"6/30/18 2:43\"  \n[3733] \"6/30/18 5:15\"   \"6/30/18 5:19\"   \"6/30/18 7:00\"   \"6/30/18 11:07\" \n[3737] \"7/1/18 5:39\"    \"7/1/18 6:01\"    \"7/1/18 6:36\"    \"7/1/18 10:53\"  \n[3741] \"7/1/18 10:59\"   \"7/1/18 13:56\"   \"7/2/18 4:03\"    \"7/2/18 6:12\"   \n[3745] \"7/2/18 6:16\"    \"7/2/18 7:16\"    \"7/3/18 2:49\"    \"7/3/18 5:56\"   \n[3749] \"7/3/18 12:10\"   \"7/3/18 12:11\"   \"7/3/18 13:33\"   \"7/4/18 2:15\"   \n[3753] \"7/4/18 2:26\"    \"7/4/18 3:24\"    \"7/8/18 5:52\"    \"7/8/18 6:55\"   \n[3757] \"7/8/18 11:15\"   \"7/8/18 11:20\"   \"7/9/18 2:51\"    \"7/9/18 7:10\"   \n[3761] \"7/9/18 13:46\"   \"7/10/18 6:50\"   \"7/10/18 6:53\"   \"7/10/18 12:30\" \n[3765] \"7/10/18 13:32\"  \"7/11/18 4:50\"   \"7/11/18 12:27\"  \"7/12/18 6:13\"  \n[3769] \"7/12/18 6:14\"   \"7/12/18 12:09\"  \"7/13/18 5:47\"   \"7/13/18 6:48\"  \n[3773] \"7/13/18 12:00\"  \"7/13/18 13:22\"  \"7/14/18 2:06\"   \"7/14/18 4:42\"  \n[3777] \"7/14/18 11:50\"  \"7/14/18 13:31\"  \"7/15/18 4:36\"   \"7/15/18 5:11\"  \n[3781] \"7/15/18 6:45\"   \"7/15/18 11:37\"  \"7/16/18 2:43\"   \"7/16/18 4:26\"  \n[3785] \"7/16/18 6:31\"   \"7/16/18 13:05\"  \"7/16/18 13:05\"  \"7/17/18 2:47\"  \n[3789] \"7/17/18 4:15\"   \"7/17/18 6:13\"   \"7/17/18 12:50\"  \"7/23/18 5:46\"  \n[3793] \"7/23/18 11:44\"  \"7/24/18 6:26\"   \"7/24/18 11:31\"  \"7/25/18 3:02\"  \n[3797] \"7/26/18 2:34\"   \"7/26/18 12:44\"  \"7/27/18 6:01\"   \"7/28/18 3:07\"  \n[3801] \"7/29/18 6:58\"   \"7/30/18 6:35\"   \"8/1/18 2:43\"    \"8/1/18 13:25\"  \n[3805] \"8/2/18 2:12\"    \"8/2/18 4:23\"    \"8/2/18 11:27\"   \"8/2/18 13:12\"  \n[3809] \"8/3/18 3:21\"    \"8/3/18 5:19\"    \"8/3/18 11:16\"   \"8/3/18 13:29\"  \n[3813] \"8/4/18 3:35\"    \"8/4/18 4:05\"    \"8/4/18 13:33\"   \"8/5/18 3:09\"   \n[3817] \"8/5/18 5:34\"    \"8/5/18 12:32\"   \"8/5/18 12:34\"   \"8/6/18 5:16\"   \n[3821] \"8/6/18 5:56\"    \"8/6/18 12:25\"   \"8/7/18 2:57\"    \"8/7/18 6:36\"   \n[3825] \"8/7/18 7:12\"    \"8/7/18 12:07\"   \"8/8/18 11:59\"   \"8/9/18 5:54\"   \n[3829] \"8/9/18 11:50\"   \"8/10/18 13:49\"  \"8/11/18 6:52\"   \"8/11/18 11:24\" \n[3833] \"8/12/18 4:11\"   \"8/13/18 6:46\"   \"8/13/18 13:52\"  \"8/14/18 6:24\"  \n[3837] \"8/14/18 13:53\"  \"8/15/18 2:07\"   \"8/15/18 6:04\"   \"8/15/18 13:28\" \n[3841] \"8/16/18 5:46\"   \"8/16/18 13:46\"  \"8/17/18 3:12\"   \"8/17/18 7:07\"  \n[3845] \"8/18/18 6:38\"   \"8/18/18 11:44\"  \"8/19/18 6:17\"   \"8/19/18 6:19\"  \n[3849] \"8/19/18 11:32\"  \"8/19/18 11:35\"  \"8/20/18 3:34\"   \"8/20/18 6:06\"  \n[3853] \"8/20/18 12:59\"  \"8/21/18 5:40\"   \"8/21/18 6:45\"   \"8/21/18 12:52\" \n[3857] \"8/22/18 2:47\"   \"8/22/18 5:26\"   \"8/22/18 12:34\"  \"8/22/18 12:37\" \n[3861] \"8/23/18 6:36\"   \"8/23/18 6:42\"   \"8/23/18 12:22\"  \"8/23/18 13:45\" \n[3865] \"8/24/18 6:17\"   \"8/24/18 6:22\"   \"8/24/18 12:09\"  \"8/24/18 13:53\" \n[3869] \"8/25/18 6:03\"   \"8/26/18 5:37\"   \"8/26/18 13:32\"  \"8/27/18 6:53\"  \n[3873] \"8/27/18 13:21\"  \"8/28/18 6:34\"   \"8/28/18 6:37\"   \"8/28/18 13:07\" \n[3877] \"8/29/18 6:19\"   \"8/30/18 5:53\"   \"8/30/18 12:47\"  \"8/31/18 5:37\"  \n[3881] \"8/31/18 12:29\"  \"9/1/18 6:18\"    \"9/1/18 12:25\"   \"9/2/18 3:27\"   \n[3885] \"9/2/18 12:15\"   \"9/3/18 2:14\"    \"9/3/18 13:39\"   \"9/4/18 6:55\"   \n[3889] \"9/4/18 13:27\"   \"9/5/18 5:36\"    \"9/5/18 11:36\"   \"9/6/18 2:18\"   \n[3893] \"9/6/18 11:25\"   \"9/6/18 13:02\"   \"9/7/18 4:47\"    \"9/7/18 5:52\"   \n[3897] \"9/7/18 11:16\"   \"9/8/18 5:41\"    \"9/8/18 6:11\"    \"9/8/18 12:38\"  \n[3901] \"9/8/18 12:41\"   \"9/9/18 3:44\"    \"9/9/18 6:49\"    \"9/9/18 12:28\"  \n[3905] \"9/9/18 12:33\"   \"9/10/18 2:02\"   \"9/10/18 12:18\"  \"9/11/18 5:05\"  \n[3909] \"9/11/18 12:10\"  \"9/12/18 5:50\"   \"9/12/18 11:55\"  \"9/13/18 3:30\"  \n[3913] \"9/13/18 6:06\"   \"9/13/18 11:46\"  \"9/14/18 5:49\"   \"9/14/18 6:47\"  \n[3917] \"9/14/18 7:27\"   \"9/14/18 11:28\"  \"9/15/18 4:12\"   \"9/15/18 4:18\"  \n[3921] \"9/15/18 13:22\"  \"9/15/18 13:50\"  \"9/16/18 2:08\"   \"9/16/18 6:40\"  \n[3925] \"9/16/18 13:30\"  \"9/17/18 6:19\"   \"9/17/18 7:25\"   \"9/18/18 3:05\"  \n[3929] \"9/18/18 10:44\"  \"9/20/18 2:28\"   \"9/20/18 12:06\"  \"9/21/18 4:47\"  \n[3933] \"9/21/18 11:53\"  \"9/22/18 6:13\"   \"9/22/18 13:21\"  \"9/23/18 11:29\" \n[3937] \"9/24/18 5:49\"   \"9/24/18 6:42\"   \"9/24/18 11:15\"  \"9/24/18 11:21\" \n[3941] \"9/24/18 13:01\"  \"9/25/18 5:15\"   \"9/25/18 5:41\"   \"9/25/18 12:49\" \n[3945] \"9/25/18 12:50\"  \"9/26/18 4:51\"   \"9/26/18 6:37\"   \"9/27/18 6:15\"  \n[3949] \"9/27/18 12:21\"  \"9/27/18 12:22\"  \"9/28/18 2:34\"   \"9/28/18 12:17\" \n[3953] \"9/29/18 13:39\"  \"9/30/18 3:13\"   \"9/30/18 11:48\"  \"9/30/18 11:52\" \n[3957] \"10/1/18 4:49\"   \"10/1/18 11:34\"  \"10/1/18 11:37\"  \"10/2/18 4:31\"  \n[3961] \"10/2/18 6:10\"   \"10/2/18 11:28\"  \"10/3/18 6:51\"   \"10/3/18 6:54\"  \n[3965] \"10/3/18 12:49\"  \"10/3/18 12:52\"  \"10/4/18 5:34\"   \"10/4/18 5:34\"  \n[3969] \"10/4/18 7:07\"   \"10/4/18 12:38\"  \"10/5/18 6:50\"   \"10/5/18 6:50\"  \n[3973] \"10/5/18 12:30\"  \"10/6/18 6:27\"   \"10/6/18 6:31\"   \"10/6/18 12:19\" \n[3977] \"10/6/18 12:23\"  \"10/7/18 6:10\"   \"10/7/18 12:04\"  \"10/7/18 13:50\" \n[3981] \"10/8/18 3:05\"   \"10/8/18 4:45\"   \"10/8/18 11:54\"  \"10/8/18 11:59\" \n[3985] \"10/8/18 13:38\"  \"10/9/18 6:32\"   \"10/9/18 13:25\"  \"10/9/18 13:25\" \n[3989] \"10/10/18 6:48\"  \"10/10/18 6:50\"  \"10/10/18 13:10\" \"10/10/18 13:44\"\n[3993] \"10/11/18 6:26\"  \"10/11/18 6:26\"  \"10/11/18 12:59\" \"10/12/18 5:42\" \n[3997] \"10/12/18 6:00\"  \"10/12/18 11:11\" \"10/12/18 11:11\" \"10/13/18 4:24\" \n[4001] \"10/13/18 6:48\"  \"10/13/18 12:33\" \"10/13/18 12:38\" \"10/14/18 5:17\" \n[4005] \"10/14/18 5:26\"  \"10/14/18 12:31\" \"10/15/18 5:00\"  \"10/15/18 6:40\" \n[4009] \"10/15/18 12:15\" \"10/15/18 12:20\" \"10/16/18 4:43\"  \"10/16/18 12:01\"\n[4013] \"10/16/18 12:04\" \"10/17/18 6:04\"  \"10/17/18 11:50\" \"10/17/18 11:51\"\n[4017] \"10/18/18 5:41\"  \"10/18/18 6:42\"  \"10/18/18 11:38\" \"10/18/18 11:41\"\n[4021] \"10/19/18 3:19\"  \"10/19/18 6:23\"  \"10/19/18 13:05\" \"10/20/18 6:36\" \n[4025] \"10/20/18 13:31\" \"10/21/18 6:13\"  \"10/21/18 6:14\"  \"10/21/18 12:41\"\n[4029] \"10/21/18 12:53\" \"10/22/18 5:24\"  \"10/22/18 6:00\"  \"10/22/18 7:02\" \n[4033] \"10/22/18 12:34\" \"10/23/18 5:37\"  \"10/23/18 5:38\"  \"10/23/18 7:16\" \n[4037] \"10/23/18 13:35\" \"10/24/18 3:25\"  \"10/24/18 6:57\"  \"10/24/18 12:06\"\n[4041] \"10/24/18 12:12\" \"10/25/18 5:59\"  \"10/25/18 6:34\"  \"10/25/18 11:58\"\n[4045] \"10/25/18 12:03\" \"10/26/18 2:17\"  \"10/26/18 6:16\"  \"10/26/18 11:48\"\n[4049] \"10/26/18 13:27\" \"10/27/18 3:35\"  \"10/27/18 6:09\"  \"10/27/18 11:34\"\n[4053] \"10/27/18 11:38\" \"10/28/18 5:34\"  \"10/28/18 5:34\"  \"10/28/18 11:22\"\n[4057] \"10/28/18 11:24\" \"10/29/18 6:17\"  \"10/29/18 6:49\"  \"10/29/18 11:11\"\n[4061] \"10/29/18 11:13\" \"10/30/18 6:27\"  \"10/30/18 6:33\"  \"10/30/18 12:37\"\n[4065] \"10/30/18 12:45\" \"10/31/18 5:23\"  \"10/31/18 5:25\"  \"10/31/18 7:15\" \n[4069] \"10/31/18 12:31\" \"11/1/18 3:12\"   \"11/1/18 5:44\"   \"11/1/18 13:58\" \n[4073] \"11/1/18 13:59\"  \"11/2/18 4:16\"   \"11/2/18 5:26\"   \"11/2/18 12:08\" \n[4077] \"11/3/18 5:05\"   \"11/3/18 5:07\"   \"11/3/18 11:59\"  \"11/3/18 13:33\" \n[4081] \"11/4/18 6:21\"   \"11/4/18 6:30\"   \"11/4/18 11:43\"  \"11/4/18 13:59\" \n[4085] \"11/5/18 3:03\"   \"11/5/18 4:20\"   \"11/5/18 11:34\"  \"11/5/18 11:36\" \n[4089] \"11/6/18 4:11\"   \"11/6/18 5:53\"   \"11/6/18 12:57\"  \"11/6/18 13:01\" \n[4093] \"11/7/18 5:37\"   \"11/7/18 5:42\"   \"11/7/18 12:50\"  \"11/7/18 12:51\" \n[4097] \"11/8/18 5:30\"   \"11/8/18 6:47\"   \"11/8/18 11:00\"  \"11/8/18 12:33\" \n[4101] \"11/9/18 6:26\"   \"11/9/18 6:29\"   \"11/9/18 12:21\"  \"11/9/18 12:21\" \n[4105] \"11/10/18 5:01\"  \"11/10/18 5:03\"  \"11/10/18 12:11\" \"11/10/18 13:45\"\n[4109] \"11/11/18 6:33\"  \"11/11/18 6:46\"  \"11/11/18 12:02\" \"11/12/18 4:39\" \n[4113] \"11/12/18 5:25\"  \"11/12/18 7:04\"  \"11/12/18 13:30\" \"11/13/18 3:08\" \n[4117] \"11/13/18 4:59\"  \"11/13/18 11:41\" \"11/14/18 4:40\"  \"11/14/18 4:40\" \n[4121] \"11/14/18 13:09\" \"11/15/18 13:02\" \"11/16/18 11:09\" \"11/17/18 6:24\" \n[4125] \"11/17/18 13:57\" \"11/18/18 2:38\"  \"11/18/18 5:59\"  \"11/18/18 12:21\"\n[4129] \"11/19/18 6:20\"  \"11/19/18 13:52\" \"11/19/18 13:53\" \"11/20/18 5:52\" \n[4133] \"11/20/18 5:56\"  \"11/22/18 6:54\"  \"11/22/18 13:02\" \"11/23/18 6:00\" \n[4137] \"11/23/18 14:07\" \"11/24/18 6:13\"  \"11/24/18 11:16\" \"11/25/18 4:39\" \n[4141] \"11/25/18 12:48\" \"11/26/18 2:41\"  \"11/29/18 6:09\"  \"11/29/18 11:57\"\n[4145] \"11/29/18 12:00\" \"11/30/18 2:35\"  \"11/30/18 5:52\"  \"11/30/18 11:44\"\n[4149] \"11/30/18 13:25\" \"12/1/18 4:18\"   \"12/1/18 6:01\"   \"12/1/18 13:08\" \n[4153] \"12/2/18 4:12\"   \"12/2/18 5:50\"   \"12/2/18 11:20\"  \"12/3/18 3:16\"  \n[4157] \"12/3/18 6:24\"   \"12/3/18 12:48\"  \"12/3/18 12:51\"  \"12/4/18 5:27\"  \n[4161] \"12/4/18 5:29\"   \"12/4/18 12:33\"  \"12/4/18 12:33\"  \"12/5/18 5:40\"  \n[4165] \"12/5/18 6:52\"   \"12/5/18 12:27\"  \"12/5/18 12:33\"  \"12/6/18 5:00\"  \n[4169] \"12/6/18 5:26\"   \"12/6/18 13:55\"  \"12/6/18 14:02\"  \"12/7/18 12:03\" \n[4173] \"12/7/18 13:41\"  \"12/8/18 2:20\"   \"12/8/18 5:49\"   \"12/9/18 4:23\"  \n[4177] \"12/9/18 7:10\"   \"12/10/18 4:21\"  \"12/10/18 13:10\" \"12/11/18 2:59\" \n[4181] \"12/11/18 12:59\" \"12/12/18 5:03\"  \"12/13/18 3:51\"  \"12/14/18 12:26\"\n[4185] \"12/15/18 5:42\"  \"12/15/18 6:44\"  \"12/15/18 12:10\" \"12/15/18 12:12\"\n[4189] \"12/16/18 3:06\"  \"12/16/18 11:57\" \"12/17/18 3:48\"  \"12/17/18 6:34\" \n[4193] \"12/17/18 11:44\" \"12/17/18 11:46\" \"12/18/18 4:23\"  \"12/18/18 11:39\"\n[4197] \"12/19/18 3:15\"  \"12/19/18 13:04\" \"12/19/18 14:03\" \"12/20/18 5:44\" \n[4201] \"12/21/18 3:46\"  \"12/21/18 12:42\" \"12/22/18 6:32\"  \"12/22/18 12:27\"\n[4205] \"12/24/18 13:46\" \"12/25/18 6:22\"  \"12/25/18 14:07\" \"12/26/18 6:48\" \n[4209] \"12/26/18 13:28\" \"12/27/18 6:30\"  \"12/28/18 11:21\" \"12/29/18 6:57\" \n[4213] \"12/30/18 5:26\"  \"12/30/18 13:16\" \"1/1/19 5:56\"    \"1/1/19 12:17\"  \n[4217] \"2/27/19 4:00\"   \"2/27/19 11:17\"  \"2/28/19 6:57\"   \"3/1/19 6:40\"   \n[4221] \"3/1/19 12:35\"   \"3/13/19 4:36\"   \"5/6/19 13:11\"   \"5/7/19 6:52\"   \n[4225] \"5/7/19 11:20\"   \"5/7/19 14:22\"   \"5/7/19 23:50\"   \"5/8/19 6:27\"   \n[4229] \"5/8/19 12:48\"   \"5/8/19 12:49\"   \"5/8/19 16:01\"   \"5/8/19 17:46\"  \n[4233] \"5/8/19 19:22\"   \"5/8/19 19:23\"   \"5/8/19 19:23\"   \"5/8/19 19:23\"  \n[4237] \"5/9/19 3:59\"    \"5/9/19 6:52\"    \"5/9/19 7:08\"    \"5/9/19 12:31\"  \n[4241] \"5/9/19 12:37\"   \"5/9/19 12:41\"   \"5/9/19 18:14\"   \"5/9/19 19:03\"  \n[4245] \"5/9/19 19:04\"   \"5/10/19 6:40\"   \"5/10/19 6:48\"   \"5/10/19 6:51\"  \n[4249] \"5/10/19 6:54\"   \"5/10/19 7:24\"   \"5/10/19 12:23\"  \"5/10/19 12:25\" \n[4253] \"5/10/19 12:30\"  \"5/10/19 18:05\"  \"5/11/19 6:29\"   \"5/11/19 6:31\"  \n[4257] \"5/11/19 6:33\"   \"5/11/19 6:33\"   \"5/11/19 7:11\"   \"5/11/19 12:12\" \n[4261] \"5/11/19 12:14\"  \"5/11/19 13:54\"  \"5/11/19 15:57\"  \"5/12/19 6:08\"  \n[4265] \"5/12/19 6:11\"   \"5/12/19 6:45\"   \"5/12/19 12:03\"  \"5/12/19 12:03\" \n[4269] \"5/12/19 15:33\"  \"5/12/19 17:13\"  \"5/12/19 17:32\"  \"5/12/19 19:00\" \n[4273] \"5/13/19 1:09\"   \"5/13/19 6:31\"   \"5/13/19 7:00\"   \"5/13/19 7:27\"  \n[4277] \"5/13/19 11:49\"  \"5/13/19 11:50\"  \"5/13/19 13:26\"  \"5/13/19 15:14\" \n[4281] \"5/13/19 19:18\"  \"5/14/19 5:24\"   \"5/14/19 6:08\"   \"5/14/19 6:42\"  \n[4285] \"5/14/19 11:38\"  \"5/14/19 11:41\"  \"5/14/19 13:20\"  \"5/14/19 13:20\" \n[4289] \"5/14/19 15:32\"  \"5/14/19 19:01\"  \"5/15/19 5:44\"   \"5/15/19 6:19\"  \n[4293] \"5/15/19 6:24\"   \"5/15/19 6:45\"   \"5/15/19 11:25\"  \"5/15/19 13:03\" \n[4297] \"5/15/19 14:06\"  \"5/15/19 14:09\"  \"5/15/19 15:19\"  \"5/15/19 23:59\" \n[4301] \"5/16/19 4:40\"   \"5/16/19 4:44\"   \"5/16/19 6:01\"   \"5/16/19 6:02\"  \n[4305] \"5/16/19 6:28\"   \"5/16/19 7:07\"   \"5/16/19 13:02\"  \"5/16/19 13:40\" \n[4309] \"5/16/19 15:08\"  \"5/16/19 17:11\"  \"5/17/19 3:59\"   \"5/17/19 4:02\"  \n[4313] \"5/17/19 6:41\"   \"5/17/19 6:50\"   \"5/17/19 6:58\"   \"5/17/19 7:21\"  \n[4317] \"5/17/19 12:44\"  \"5/17/19 12:47\"  \"5/17/19 13:32\"  \"5/17/19 15:11\" \n[4321] \"5/18/19 5:07\"   \"5/18/19 6:24\"   \"5/18/19 8:01\"   \"5/18/19 12:32\" \n[4325] \"5/18/19 12:32\"  \"5/18/19 14:16\"  \"5/18/19 15:56\"  \"5/18/19 19:16\" \n[4329] \"5/18/19 19:17\"  \"5/18/19 19:19\"  \"5/19/19 3:03\"   \"5/19/19 3:09\"  \n[4333] \"5/19/19 4:47\"   \"5/19/19 4:50\"   \"5/19/19 4:59\"   \"5/19/19 12:18\" \n[4337] \"5/19/19 12:21\"  \"5/19/19 15:26\"  \"5/19/19 17:10\"  \"5/20/19 0:58\"  \n[4341] \"5/20/19 2:47\"   \"5/20/19 6:12\"   \"5/20/19 6:19\"   \"5/20/19 12:07\" \n[4345] \"5/20/19 13:49\"  \"5/20/19 13:49\"  \"5/20/19 15:41\"  \"5/20/19 17:28\" \n[4349] \"5/21/19 5:23\"   \"5/21/19 6:11\"   \"5/21/19 6:13\"   \"5/21/19 6:17\"  \n[4353] \"5/21/19 6:22\"   \"5/21/19 7:02\"   \"5/21/19 7:02\"   \"5/21/19 11:55\" \n[4357] \"5/21/19 11:58\"  \"5/21/19 15:09\"  \"5/22/19 6:06\"   \"5/22/19 6:39\"  \n[4361] \"5/22/19 6:41\"   \"5/22/19 7:38\"   \"5/22/19 11:48\"  \"5/22/19 13:21\" \n[4365] \"5/22/19 13:22\"  \"5/22/19 16:11\"  \"5/22/19 19:34\"  \"5/23/19 6:20\"  \n[4369] \"5/23/19 6:27\"   \"5/23/19 7:17\"   \"5/23/19 13:13\"  \"5/23/19 15:01\" \n[4373] \"5/23/19 17:07\"  \"5/23/19 19:11\"  \"5/23/19 19:13\"  \"5/23/19 19:17\" \n[4377] \"5/24/19 5:17\"   \"5/24/19 5:20\"   \"5/24/19 5:58\"   \"5/24/19 11:23\" \n[4381] \"5/24/19 13:01\"  \"5/24/19 15:15\"  \"5/24/19 17:00\"  \"5/24/19 17:12\" \n[4385] \"5/24/19 23:56\"  \"5/25/19 5:35\"   \"5/25/19 5:39\"   \"5/25/19 6:09\"  \n[4389] \"5/25/19 6:40\"   \"5/25/19 7:21\"   \"5/25/19 7:23\"   \"5/25/19 12:45\" \n[4393] \"5/25/19 12:52\"  \"5/25/19 17:26\"  \"5/26/19 3:34\"   \"5/26/19 6:14\"  \n[4397] \"5/26/19 6:15\"   \"5/26/19 6:58\"   \"5/26/19 7:00\"   \"5/26/19 12:40\" \n[4401] \"5/26/19 12:41\"  \"5/26/19 15:10\"  \"5/26/19 17:00\"  \"5/27/19 6:44\"  \n[4405] \"5/27/19 6:46\"   \"5/27/19 7:10\"   \"5/27/19 12:28\"  \"5/27/19 14:08\" \n[4409] \"5/27/19 16:01\"  \"5/27/19 17:03\"  \"5/27/19 19:29\"  \"5/27/19 19:32\" \n[4413] \"5/28/19 6:18\"   \"5/28/19 6:19\"   \"5/28/19 6:54\"   \"5/28/19 7:13\"  \n[4417] \"5/28/19 12:15\"  \"5/28/19 12:18\"  \"5/28/19 16:43\"  \"5/28/19 17:28\" \n[4421] \"5/28/19 19:07\"  \"5/28/19 19:12\"  \"5/29/19 6:19\"   \"5/29/19 6:33\"  \n[4425] \"5/29/19 6:54\"   \"5/29/19 6:56\"   \"5/29/19 7:36\"   \"5/29/19 12:02\" \n[4429] \"5/29/19 12:05\"  \"5/29/19 12:06\"  \"5/29/19 17:07\"  \"5/30/19 4:53\"  \n[4433] \"5/30/19 6:07\"   \"5/30/19 6:07\"   \"5/30/19 6:35\"   \"5/30/19 11:58\" \n[4437] \"5/30/19 16:18\"  \"5/30/19 17:17\"  \"5/30/19 17:18\"  \"5/31/19 6:02\"  \n[4441] \"5/31/19 11:43\"  \"5/31/19 13:26\"  \"5/31/19 15:33\"  \"5/31/19 17:14\" \n[4445] \"5/31/19 17:17\"  \"5/31/19 19:04\"  \"5/31/19 19:05\"  \"5/31/19 19:06\" \n[4449] \"6/1/19 4:23\"    \"6/1/19 6:31\"    \"6/1/19 7:10\"    \"6/1/19 11:29\"  \n[4453] \"6/1/19 13:13\"   \"6/1/19 15:18\"   \"6/1/19 17:08\"   \"6/1/19 19:27\"  \n[4457] \"6/1/19 23:59\"   \"6/1/19 23:59\"   \"6/2/19 5:36\"    \"6/2/19 6:18\"   \n[4461] \"6/2/19 7:12\"    \"6/2/19 13:06\"   \"6/2/19 13:07\"   \"6/2/19 15:14\"  \n[4465] \"6/2/19 17:25\"   \"6/2/19 19:03\"   \"6/2/19 19:06\"   \"6/2/19 23:50\"  \n[4469] \"6/3/19 1:56\"    \"6/3/19 3:11\"    \"6/3/19 5:26\"    \"6/3/19 5:54\"   \n[4473] \"6/3/19 6:48\"    \"6/3/19 12:48\"   \"6/3/19 12:51\"   \"6/3/19 15:57\"  \n[4477] \"6/3/19 17:05\"   \"6/3/19 17:07\"   \"6/4/19 1:17\"    \"6/4/19 3:08\"   \n[4481] \"6/4/19 6:02\"    \"6/4/19 6:32\"    \"6/4/19 6:50\"    \"6/4/19 7:09\"   \n[4485] \"6/4/19 7:11\"    \"6/4/19 7:16\"    \"6/4/19 12:41\"   \"6/4/19 15:24\"  \n[4489] \"6/5/19 4:28\"    \"6/5/19 6:14\"    \"6/5/19 6:52\"    \"6/5/19 12:27\"  \n[4493] \"6/5/19 15:32\"   \"6/5/19 17:00\"   \"6/5/19 17:16\"   \"6/5/19 19:04\"  \n[4497] \"6/5/19 19:06\"   \"6/6/19 6:27\"    \"6/6/19 6:29\"    \"6/6/19 12:13\"  \n[4501] \"6/6/19 12:13\"   \"6/6/19 13:50\"   \"6/6/19 15:10\"   \"6/6/19 17:02\"  \n[4505] \"6/6/19 19:22\"   \"6/6/19 19:23\"   \"6/6/19 19:24\"   \"6/7/19 6:15\"   \n[4509] \"6/7/19 7:09\"    \"6/7/19 11:59\"   \"6/7/19 12:01\"   \"6/7/19 15:32\"  \n[4513] \"6/7/19 17:22\"   \"6/7/19 19:01\"   \"6/7/19 19:02\"   \"6/7/19 19:04\"  \n[4517] \"6/7/19 19:05\"   \"6/8/19 4:23\"    \"6/8/19 5:06\"    \"6/8/19 5:47\"   \n[4521] \"6/8/19 7:33\"    \"6/8/19 11:50\"   \"6/8/19 15:42\"   \"6/8/19 17:03\"  \n[4525] \"6/8/19 17:03\"   \"6/8/19 19:14\"   \"6/9/19 2:49\"    \"6/9/19 4:46\"   \n[4529] \"6/9/19 6:08\"    \"6/9/19 6:28\"    \"6/9/19 14:27\"   \"6/9/19 15:28\"  \n[4533] \"6/9/19 17:07\"   \"6/9/19 17:09\"   \"6/10/19 5:08\"   \"6/10/19 6:49\"  \n[4537] \"6/10/19 13:02\"  \"6/10/19 14:50\"  \"6/10/19 15:06\"  \"6/10/19 17:15\" \n[4541] \"6/10/19 17:15\"  \"6/10/19 23:57\"  \"6/10/19 23:59\"  \"6/10/19 23:59\" \n[4545] \"6/11/19 3:31\"   \"6/11/19 3:34\"   \"6/11/19 4:48\"   \"6/11/19 5:43\"  \n[4549] \"6/11/19 5:53\"   \"6/11/19 12:58\"  \"6/11/19 16:27\"  \"6/11/19 17:44\" \n[4553] \"6/11/19 17:44\"  \"6/12/19 4:46\"   \"6/12/19 5:29\"   \"6/12/19 5:30\"  \n[4557] \"6/12/19 6:07\"   \"6/12/19 7:03\"   \"6/12/19 7:05\"   \"6/12/19 12:45\" \n[4561] \"6/12/19 16:01\"  \"6/12/19 19:04\"  \"6/13/19 1:40\"   \"6/13/19 5:05\"  \n[4565] \"6/13/19 5:49\"   \"6/13/19 6:24\"   \"6/13/19 6:44\"   \"6/13/19 12:36\" \n[4569] \"6/13/19 15:33\"  \"6/13/19 17:02\"  \"6/13/19 17:28\"  \"6/14/19 5:25\"  \n[4573] \"6/14/19 5:57\"   \"6/14/19 6:03\"   \"6/14/19 6:23\"   \"6/14/19 12:21\" \n[4577] \"6/14/19 13:35\"  \"6/14/19 15:10\"  \"6/14/19 17:08\"  \"6/14/19 17:38\" \n[4581] \"6/15/19 3:35\"   \"6/15/19 5:05\"   \"6/15/19 5:07\"   \"6/15/19 6:26\"  \n[4585] \"6/15/19 12:08\"  \"6/15/19 13:46\"  \"6/15/19 15:36\"  \"6/15/19 17:11\" \n[4589] \"6/15/19 17:14\"  \"6/15/19 19:39\"  \"6/16/19 3:29\"   \"6/16/19 6:14\"  \n[4593] \"6/16/19 8:04\"   \"6/16/19 15:45\"  \"6/16/19 17:23\"  \"6/16/19 17:26\" \n[4597] \"6/16/19 19:15\"  \"6/16/19 19:18\"  \"6/17/19 2:58\"   \"6/17/19 4:26\"  \n[4601] \"6/17/19 11:43\"  \"6/17/19 13:26\"  \"6/17/19 15:18\"  \"6/17/19 17:09\" \n[4605] \"6/17/19 17:09\"  \"6/17/19 19:00\"  \"6/17/19 19:00\"  \"6/18/19 4:10\"  \n[4609] \"6/18/19 5:43\"   \"6/18/19 6:21\"   \"6/18/19 6:46\"   \"6/18/19 11:36\" \n[4613] \"6/18/19 13:15\"  \"6/18/19 15:06\"  \"6/18/19 17:04\"  \"6/18/19 17:05\" \n[4617] \"6/19/19 3:57\"   \"6/19/19 3:57\"   \"6/19/19 5:33\"   \"6/19/19 6:25\"  \n[4621] \"6/19/19 13:04\"  \"6/19/19 13:06\"  \"6/19/19 15:53\"  \"6/19/19 17:03\" \n[4625] \"6/19/19 17:04\"  \"6/20/19 3:12\"   \"6/20/19 5:40\"   \"6/20/19 6:46\"  \n[4629] \"6/20/19 12:51\"  \"6/20/19 12:54\"  \"6/20/19 15:28\"  \"6/20/19 17:06\" \n[4633] \"6/20/19 17:09\"  \"6/20/19 19:33\"  \"6/21/19 3:01\"   \"6/21/19 6:22\"  \n[4637] \"6/21/19 6:22\"   \"6/21/19 12:41\"  \"6/21/19 15:01\"  \"6/21/19 17:29\" \n[4641] \"6/21/19 17:33\"  \"6/21/19 19:10\"  \"6/21/19 19:16\"  \"6/22/19 3:43\"  \n[4645] \"6/22/19 4:55\"   \"6/22/19 4:57\"   \"6/22/19 6:40\"   \"6/22/19 7:04\"  \n[4649] \"6/22/19 12:29\"  \"6/22/19 12:30\"  \"6/22/19 15:10\"  \"6/22/19 17:08\" \n[4653] \"6/23/19 3:34\"   \"6/23/19 3:35\"   \"6/23/19 4:48\"   \"6/23/19 5:12\"  \n[4657] \"6/23/19 12:14\"  \"6/23/19 13:51\"  \"6/23/19 15:32\"  \"6/23/19 17:21\" \n[4661] \"6/23/19 17:25\"  \"6/24/19 3:11\"   \"6/24/19 4:38\"   \"6/24/19 5:18\"  \n[4665] \"6/24/19 6:59\"   \"6/24/19 13:45\"  \"6/24/19 15:07\"  \"6/24/19 17:01\" \n[4669] \"6/24/19 17:26\"  \"6/25/19 5:28\"   \"6/25/19 6:03\"   \"6/25/19 6:40\"  \n[4673] \"6/25/19 11:50\"  \"6/25/19 11:53\"  \"6/25/19 15:32\"  \"6/25/19 17:06\" \n[4677] \"6/25/19 17:06\"  \"6/25/19 19:30\"  \"6/26/19 5:59\"   \"6/26/19 7:17\"  \n[4681] \"6/26/19 11:43\"  \"6/26/19 13:20\"  \"6/26/19 15:03\"  \"6/26/19 17:11\" \n[4685] \"6/26/19 19:08\"  \"6/26/19 19:10\"  \"6/26/19 19:11\"  \"6/27/19 3:31\"  \n[4689] \"6/27/19 5:57\"   \"6/27/19 5:57\"   \"6/27/19 7:39\"   \"6/27/19 13:09\" \n[4693] \"6/27/19 13:12\"  \"6/27/19 15:07\"  \"6/27/19 17:02\"  \"6/27/19 23:59\" \n[4697] \"6/28/19 3:06\"   \"6/28/19 4:58\"   \"6/28/19 5:33\"   \"6/28/19 6:34\"  \n[4701] \"6/28/19 7:09\"   \"6/28/19 13:00\"  \"6/28/19 14:41\"  \"6/28/19 15:58\" \n[4705] \"6/28/19 17:17\"  \"6/29/19 6:13\"   \"6/29/19 6:17\"   \"6/29/19 12:44\" \n[4709] \"6/29/19 12:45\"  \"6/29/19 15:33\"  \"6/29/19 17:01\"  \"6/29/19 19:05\" \n[4713] \"6/29/19 19:08\"  \"6/30/19 1:21\"   \"6/30/19 3:56\"   \"6/30/19 4:02\"  \n[4717] \"6/30/19 6:39\"   \"6/30/19 12:30\"  \"6/30/19 12:33\"  \"6/30/19 12:38\" \n[4721] \"6/30/19 15:26\"  \"6/30/19 17:05\"  \"6/30/19 19:26\"  \"7/1/19 6:19\"   \n[4725] \"7/1/19 7:17\"    \"7/1/19 7:53\"    \"7/1/19 12:24\"   \"7/1/19 14:04\"  \n[4729] \"7/1/19 16:16\"   \"7/1/19 19:03\"   \"7/1/19 19:04\"   \"7/1/19 19:10\"  \n[4733] \"7/2/19 0:53\"    \"7/2/19 5:10\"    \"7/2/19 5:53\"    \"7/2/19 6:49\"   \n[4737] \"7/2/19 6:53\"    \"7/2/19 7:30\"    \"7/2/19 12:10\"   \"7/2/19 12:12\"  \n[4741] \"7/2/19 15:49\"   \"7/3/19 4:33\"    \"7/3/19 4:46\"    \"7/3/19 6:16\"   \n[4745] \"7/3/19 6:36\"    \"7/3/19 13:41\"   \"7/3/19 15:23\"   \"7/3/19 15:25\"  \n[4749] \"7/3/19 17:14\"   \"7/3/19 17:17\"   \"7/4/19 4:30\"    \"7/4/19 5:59\"   \n[4753] \"7/4/19 6:50\"    \"7/4/19 11:47\"   \"7/4/19 13:26\"   \"7/4/19 15:05\"  \n[4757] \"7/4/19 17:22\"   \"7/4/19 17:23\"   \"7/4/19 19:03\"   \"7/5/19 6:27\"   \n[4761] \"7/5/19 6:31\"    \"7/5/19 13:14\"   \"7/5/19 13:16\"   \"7/5/19 15:23\"  \n[4765] \"7/5/19 17:07\"   \"7/5/19 19:22\"   \"7/5/19 19:23\"   \"7/6/19 3:28\"   \n[4769] \"7/6/19 5:41\"    \"7/6/19 6:48\"    \"7/6/19 11:23\"   \"7/6/19 13:09\"  \n[4773] \"7/6/19 13:48\"   \"7/6/19 15:11\"   \"7/6/19 17:20\"   \"7/6/19 19:05\"  \n[4777] \"7/7/19 3:46\"    \"7/7/19 5:14\"    \"7/7/19 5:51\"    \"7/7/19 6:48\"   \n[4781] \"7/7/19 7:30\"    \"7/7/19 12:54\"   \"7/7/19 15:01\"   \"7/7/19 17:05\"  \n[4785] \"7/8/19 3:42\"    \"7/8/19 5:17\"    \"7/8/19 6:27\"    \"7/8/19 6:57\"   \n[4789] \"7/8/19 7:05\"    \"7/8/19 15:08\"   \"7/8/19 16:07\"   \"7/8/19 17:35\"  \n[4793] \"7/9/19 6:45\"    \"7/9/19 6:50\"    \"7/9/19 6:52\"    \"7/9/19 12:31\"  \n[4797] \"7/9/19 15:35\"   \"7/9/19 17:13\"   \"7/9/19 17:17\"   \"7/9/19 19:00\"  \n[4801] \"7/10/19 6:31\"   \"7/10/19 7:24\"   \"7/10/19 15:56\"  \"7/10/19 17:00\" \n[4805] \"7/10/19 19:18\"  \"7/10/19 19:20\"  \"7/10/19 19:21\"  \"7/11/19 6:10\"  \n[4809] \"7/11/19 6:43\"   \"7/11/19 7:05\"   \"7/11/19 13:45\"  \"7/11/19 15:26\" \n[4813] \"7/11/19 17:22\"  \"7/11/19 19:00\"  \"7/11/19 19:02\"  \"7/12/19 2:16\"  \n[4817] \"7/12/19 3:20\"   \"7/12/19 6:46\"   \"7/12/19 6:49\"   \"7/12/19 7:29\"  \n[4821] \"7/12/19 13:34\"  \"7/12/19 15:04\"  \"7/12/19 17:27\"  \"7/13/19 3:33\"  \n[4825] \"7/13/19 6:02\"   \"7/13/19 6:21\"   \"7/13/19 6:28\"   \"7/13/19 7:42\"  \n[4829] \"7/13/19 13:20\"  \"7/13/19 15:06\"  \"7/13/19 17:11\"  \"7/14/19 3:37\"  \n[4833] \"7/14/19 4:41\"   \"7/14/19 6:02\"   \"7/14/19 6:39\"   \"7/14/19 11:34\" \n[4837] \"7/14/19 15:35\"  \"7/14/19 17:04\"  \"7/15/19 3:24\"   \"7/15/19 12:58\" \n[4841] \"7/15/19 15:28\"  \"7/15/19 17:33\"  \"7/15/19 19:14\"  \"7/15/19 19:16\" \n[4845] \"7/15/19 19:17\"  \"7/16/19 2:16\"   \"7/16/19 2:52\"   \"7/16/19 2:53\"  \n[4849] \"7/16/19 6:40\"   \"7/16/19 13:41\"  \"7/16/19 15:06\"  \"7/16/19 17:10\" \n[4853] \"7/16/19 17:14\"  \"7/17/19 3:00\"   \"7/17/19 5:13\"   \"7/17/19 6:45\"  \n[4857] \"7/17/19 6:50\"   \"7/17/19 12:38\"  \"7/17/19 16:11\"  \"7/17/19 17:51\" \n[4861] \"7/17/19 17:51\"  \"7/18/19 3:31\"   \"7/18/19 3:34\"   \"7/18/19 6:39\"  \n[4865] \"7/18/19 6:42\"   \"7/18/19 7:03\"   \"7/18/19 12:26\"  \"7/18/19 15:54\" \n[4869] \"7/18/19 17:27\"  \"7/19/19 6:00\"   \"7/19/19 6:31\"   \"7/19/19 7:16\"  \n[4873] \"7/19/19 12:13\"  \"7/19/19 15:21\"  \"7/19/19 17:08\"  \"7/19/19 19:33\" \n[4877] \"7/19/19 19:34\"  \"7/20/19 4:40\"   \"7/20/19 6:26\"   \"7/20/19 15:08\" \n[4881] \"7/20/19 17:27\"  \"7/20/19 17:29\"  \"7/20/19 19:11\"  \"7/20/19 19:18\" \n[4885] \"7/21/19 3:38\"   \"7/21/19 3:41\"   \"7/21/19 6:33\"   \"7/21/19 6:35\"  \n[4889] \"7/21/19 7:38\"   \"7/21/19 16:20\"  \"7/21/19 17:10\"  \"7/22/19 1:58\"  \n[4893] \"7/22/19 4:06\"   \"7/22/19 5:41\"   \"7/22/19 6:35\"   \"7/22/19 7:23\"  \n[4897] \"7/22/19 13:23\"  \"7/22/19 15:27\"  \"7/22/19 17:08\"  \"7/23/19 2:42\"  \n[4901] \"7/23/19 5:21\"   \"7/23/19 6:21\"   \"7/23/19 6:59\"   \"7/23/19 13:03\" \n[4905] \"7/23/19 15:33\"  \"7/23/19 17:26\"  \"7/24/19 3:44\"   \"7/24/19 4:16\"  \n[4909] \"7/24/19 5:33\"   \"7/24/19 6:35\"   \"7/24/19 12:59\"  \"7/24/19 15:06\" \n[4913] \"7/24/19 17:03\"  \"7/25/19 3:31\"   \"7/25/19 6:51\"   \"7/25/19 12:49\" \n[4917] \"7/25/19 15:38\"  \"7/25/19 17:30\"  \"7/25/19 19:06\"  \"7/25/19 19:07\" \n[4921] \"7/26/19 6:32\"   \"7/26/19 6:42\"   \"7/26/19 6:45\"   \"7/26/19 12:27\" \n[4925] \"7/26/19 12:29\"  \"7/26/19 15:08\"  \"7/26/19 17:07\"  \"7/27/19 4:20\"  \n[4929] \"7/27/19 6:28\"   \"7/27/19 6:33\"   \"7/27/19 6:35\"   \"7/27/19 12:26\" \n[4933] \"7/27/19 15:26\"  \"7/27/19 17:45\"  \"7/28/19 3:54\"   \"7/28/19 6:14\"  \n[4937] \"7/28/19 6:51\"   \"7/28/19 6:55\"   \"7/28/19 12:08\"  \"7/28/19 12:11\" \n[4941] \"7/28/19 15:55\"  \"7/28/19 17:22\"  \"7/29/19 5:25\"   \"7/29/19 6:40\"  \n[4945] \"7/29/19 7:33\"   \"7/29/19 13:43\"  \"7/29/19 15:24\"  \"7/29/19 17:22\" \n[4949] \"7/29/19 19:25\"  \"7/29/19 19:26\"  \"7/30/19 5:30\"   \"7/30/19 7:14\"  \n[4953] \"7/30/19 7:16\"   \"7/30/19 13:32\"  \"7/30/19 15:32\"  \"7/30/19 19:05\" \n[4957] \"7/30/19 19:07\"  \"7/30/19 19:08\"  \"7/31/19 4:18\"   \"7/31/19 4:23\"  \n[4961] \"7/31/19 5:49\"   \"7/31/19 6:23\"   \"7/31/19 7:33\"   \"7/31/19 13:46\" \n[4965] \"7/31/19 15:28\"  \"7/31/19 17:37\"  \"8/1/19 3:53\"    \"8/1/19 3:55\"   \n[4969] \"8/1/19 6:06\"    \"8/1/19 6:12\"    \"8/1/19 7:11\"    \"8/1/19 7:15\"   \n[4973] \"8/1/19 15:02\"   \"8/1/19 17:14\"   \"8/2/19 3:29\"    \"8/2/19 5:11\"   \n[4977] \"8/2/19 5:12\"    \"8/2/19 6:49\"    \"8/2/19 12:47\"   \"8/2/19 16:21\"  \n[4981] \"8/2/19 18:05\"   \"8/2/19 18:18\"   \"8/3/19 1:28\"    \"8/3/19 6:34\"   \n[4985] \"8/3/19 12:39\"   \"8/3/19 12:42\"   \"8/3/19 15:56\"   \"8/3/19 17:38\"  \n[4989] \"8/3/19 19:22\"   \"8/4/19 6:42\"    \"8/4/19 12:24\"   \"8/4/19 12:28\"  \n[4993] \"8/4/19 17:28\"   \"8/4/19 19:00\"   \"8/4/19 19:03\"   \"8/5/19 6:26\"   \n[4997] \"8/5/19 6:31\"    \"8/5/19 6:32\"    \"8/5/19 12:16\"   \"8/5/19 12:18\"  \n[5001] \"8/5/19 17:31\"   \"8/6/19 4:35\"    \"8/6/19 6:06\"    \"8/6/19 6:14\"   \n[5005] \"8/6/19 7:07\"    \"8/6/19 12:06\"   \"8/6/19 15:50\"   \"8/6/19 17:33\"  \n[5009] \"8/7/19 3:03\"    \"8/7/19 5:45\"    \"8/7/19 6:46\"    \"8/7/19 13:35\"  \n[5013] \"8/7/19 15:39\"   \"8/7/19 17:15\"   \"8/7/19 17:20\"   \"8/8/19 6:23\"   \n[5017] \"8/8/19 6:28\"    \"8/8/19 8:08\"    \"8/8/19 13:18\"   \"8/8/19 15:25\"  \n[5021] \"8/8/19 17:05\"   \"8/8/19 19:20\"   \"8/9/19 5:48\"    \"8/9/19 6:39\"   \n[5025] \"8/9/19 7:50\"    \"8/9/19 13:10\"   \"8/9/19 15:17\"   \"8/9/19 17:18\"  \n[5029] \"8/9/19 19:00\"   \"8/10/19 3:27\"   \"8/10/19 6:43\"   \"8/10/19 6:46\"  \n[5033] \"8/10/19 7:27\"   \"8/10/19 12:54\"  \"8/10/19 16:19\"  \"8/10/19 17:26\" \n[5037] \"8/11/19 6:21\"   \"8/11/19 6:22\"   \"8/11/19 6:23\"   \"8/11/19 7:00\"  \n[5041] \"8/11/19 7:06\"   \"8/11/19 13:24\"  \"8/11/19 16:30\"  \"8/12/19 2:39\"  \n[5045] \"8/12/19 6:44\"   \"8/12/19 7:19\"   \"8/12/19 15:27\"  \"8/12/19 17:19\" \n[5049] \"8/13/19 6:39\"   \"8/13/19 6:58\"   \"8/13/19 7:20\"   \"8/13/19 12:27\" \n[5053] \"8/13/19 16:12\"  \"8/13/19 17:41\"  \"8/13/19 19:15\"  \"8/14/19 3:28\"  \n[5057] \"8/14/19 6:10\"   \"8/14/19 6:20\"   \"8/14/19 12:11\"  \"8/14/19 13:28\" \n[5061] \"8/14/19 17:13\"  \"8/15/19 5:41\"   \"8/15/19 5:47\"   \"8/15/19 6:20\"  \n[5065] \"8/15/19 7:22\"   \"8/15/19 14:41\"  \"8/15/19 15:53\"  \"8/16/19 4:19\"  \n[5069] \"8/16/19 6:00\"   \"8/16/19 6:58\"   \"8/16/19 7:02\"   \"8/16/19 11:51\" \n[5073] \"8/16/19 15:38\"  \"8/16/19 17:09\"  \"8/17/19 6:43\"   \"8/17/19 6:45\"  \n[5077] \"8/17/19 6:47\"   \"8/17/19 13:13\"  \"8/17/19 13:18\"  \"8/17/19 15:27\" \n[5081] \"8/17/19 17:08\"  \"8/18/19 6:55\"   \"8/18/19 13:06\"  \"8/18/19 13:07\" \n[5085] \"8/18/19 16:14\"  \"8/18/19 17:28\"  \"8/18/19 19:14\"  \"8/18/19 19:15\" \n[5089] \"8/19/19 5:24\"   \"8/19/19 6:54\"   \"8/19/19 6:55\"   \"8/19/19 12:51\" \n[5093] \"8/19/19 12:55\"  \"8/19/19 16:29\"  \"8/19/19 17:17\"  \"8/20/19 6:37\"  \n[5097] \"8/20/19 6:52\"   \"8/20/19 6:53\"   \"8/20/19 12:42\"  \"8/20/19 13:45\" \n[5101] \"8/20/19 16:29\"  \"8/20/19 17:23\"  \"8/21/19 4:41\"   \"8/21/19 6:43\"  \n[5105] \"8/21/19 6:57\"   \"8/21/19 12:38\"  \"8/21/19 13:13\"  \"8/21/19 15:47\" \n[5109] \"8/21/19 17:01\"  \"8/22/19 2:49\"   \"8/22/19 4:56\"   \"8/22/19 6:35\"  \n[5113] \"8/22/19 12:18\"  \"8/22/19 16:04\"  \"8/22/19 17:03\"  \"8/23/19 6:51\"  \n[5117] \"8/23/19 7:13\"   \"8/23/19 12:07\"  \"8/23/19 15:32\"  \"8/23/19 19:13\" \n[5121] \"8/24/19 6:52\"   \"8/24/19 6:56\"   \"8/24/19 13:40\"  \"8/24/19 15:25\" \n[5125] \"8/24/19 17:19\"  \"8/25/19 6:06\"   \"8/25/19 6:33\"   \"8/25/19 11:43\" \n[5129] \"8/25/19 15:03\"  \"8/25/19 17:14\"  \"8/26/19 5:39\"   \"8/26/19 5:49\"  \n[5133] \"8/26/19 6:52\"   \"8/26/19 11:36\"  \"8/26/19 13:14\"  \"8/26/19 15:36\" \n[5137] \"8/26/19 17:02\"  \"8/27/19 4:54\"   \"8/27/19 6:34\"   \"8/27/19 7:11\"  \n[5141] \"8/27/19 7:13\"   \"8/27/19 15:09\"  \"8/27/19 17:50\"  \"8/27/19 19:28\" \n[5145] \"8/28/19 5:20\"   \"8/28/19 6:48\"   \"8/28/19 7:50\"   \"8/28/19 14:33\" \n[5149] \"8/28/19 15:26\"  \"8/29/19 6:30\"   \"8/29/19 6:32\"   \"8/29/19 12:40\" \n[5153] \"8/29/19 12:41\"  \"8/29/19 15:40\"  \"8/29/19 17:06\"  \"8/30/19 5:29\"  \n[5157] \"8/30/19 5:34\"   \"8/30/19 6:26\"   \"8/30/19 7:16\"   \"8/30/19 12:30\" \n[5161] \"8/30/19 12:32\"  \"8/30/19 15:08\"  \"8/31/19 4:33\"   \"8/31/19 6:54\"  \n[5165] \"8/31/19 7:28\"   \"8/31/19 7:29\"   \"8/31/19 12:13\"  \"8/31/19 16:01\" \n[5169] \"8/31/19 19:45\"  \"9/1/19 5:54\"    \"9/1/19 6:36\"    \"9/1/19 7:08\"   \n[5173] \"9/1/19 12:01\"   \"9/1/19 12:06\"   \"9/1/19 15:27\"   \"9/1/19 19:24\"  \n[5177] \"9/2/19 6:12\"    \"9/2/19 6:49\"    \"9/2/19 7:05\"    \"9/2/19 11:52\"  \n[5181] \"9/2/19 13:35\"   \"9/2/19 15:40\"   \"9/2/19 19:06\"   \"9/3/19 6:28\"   \n[5185] \"9/3/19 6:51\"    \"9/3/19 6:52\"    \"9/3/19 7:30\"    \"9/3/19 13:08\"  \n[5189] \"9/3/19 15:05\"   \"9/4/19 5:30\"    \"9/4/19 6:04\"    \"9/4/19 6:31\"   \n[5193] \"9/4/19 7:11\"    \"9/4/19 13:05\"   \"9/4/19 13:15\"   \"9/4/19 16:43\"  \n[5197] \"9/5/19 4:15\"    \"9/5/19 6:03\"    \"9/5/19 6:09\"    \"9/5/19 12:53\"  \n[5201] \"9/5/19 13:03\"   \"9/5/19 15:40\"   \"9/5/19 18:23\"   \"9/6/19 6:32\"   \n[5205] \"9/6/19 12:45\"   \"9/6/19 13:14\"   \"9/6/19 16:01\"   \"9/6/19 17:00\"  \n[5209] \"9/6/19 19:20\"   \"9/6/19 19:20\"   \"9/7/19 5:27\"    \"9/7/19 7:46\"   \n[5213] \"9/7/19 12:34\"   \"9/7/19 16:18\"   \"9/7/19 19:01\"   \"9/8/19 6:20\"   \n[5217] \"9/8/19 6:48\"    \"9/8/19 12:25\"   \"9/8/19 12:26\"   \"9/9/19 5:27\"   \n[5221] \"9/9/19 6:28\"    \"9/9/19 12:09\"   \"9/9/19 12:13\"   \"9/9/19 15:52\"  \n[5225] \"9/10/19 6:40\"   \"9/10/19 6:46\"   \"9/10/19 11:59\"  \"9/10/19 13:39\" \n[5229] \"9/10/19 16:38\"  \"9/11/19 6:01\"   \"9/11/19 11:47\"  \"9/11/19 15:57\" \n[5233] \"9/11/19 19:17\"  \"9/12/19 6:09\"   \"9/12/19 6:40\"   \"9/12/19 11:37\" \n[5237] \"9/12/19 13:14\"  \"9/12/19 15:06\"  \"9/13/19 5:42\"   \"9/13/19 6:43\"  \n[5241] \"9/13/19 7:25\"   \"9/13/19 13:01\"  \"9/13/19 15:17\"  \"9/14/19 3:48\"  \n[5245] \"9/14/19 5:54\"   \"9/14/19 12:48\"  \"9/14/19 12:54\"  \"9/14/19 15:03\" \n[5249] \"9/15/19 5:38\"   \"9/15/19 6:44\"   \"9/15/19 12:44\"  \"9/15/19 13:29\" \n[5253] \"9/15/19 15:13\"  \"9/16/19 6:54\"   \"9/16/19 6:55\"   \"9/16/19 12:30\" \n[5257] \"9/16/19 15:51\"  \"9/17/19 5:57\"   \"9/17/19 6:59\"   \"9/17/19 12:22\" \n[5261] \"9/17/19 15:58\"  \"9/18/19 6:15\"   \"9/18/19 6:34\"   \"9/18/19 12:08\" \n[5265] \"9/18/19 15:46\"  \"9/19/19 5:20\"   \"9/19/19 6:21\"   \"9/19/19 7:01\"  \n[5269] \"9/19/19 15:38\"  \"9/20/19 6:42\"   \"9/20/19 13:22\"  \"9/20/19 15:46\" \n[5273] \"9/20/19 17:01\"  \"9/20/19 19:31\"  \"9/21/19 6:14\"   \"9/21/19 6:49\"  \n[5277] \"9/21/19 7:19\"   \"9/21/19 13:45\"  \"9/21/19 15:25\"  \"9/22/19 5:57\"  \n[5281] \"9/22/19 6:51\"   \"9/22/19 13:02\"  \"9/22/19 13:03\"  \"9/22/19 16:30\" \n[5285] \"9/23/19 6:34\"   \"9/23/19 6:58\"   \"9/23/19 12:45\"  \"9/23/19 12:49\" \n[5289] \"9/24/19 6:14\"   \"9/24/19 7:30\"   \"9/24/19 12:37\"  \"9/24/19 19:07\" \n[5293] \"9/25/19 6:34\"   \"9/25/19 7:10\"   \"9/25/19 7:30\"   \"9/25/19 15:06\" \n[5297] \"9/25/19 19:26\"  \"9/26/19 6:18\"   \"9/26/19 7:51\"   \"9/26/19 13:49\" \n[5301] \"9/26/19 19:10\"  \"9/27/19 6:54\"   \"9/27/19 12:04\"  \"9/28/19 6:29\"  \n[5305] \"9/28/19 14:03\"  \"9/28/19 15:05\"  \"9/29/19 15:25\"  \"9/29/19 19:01\" \n[5309] \"9/30/19 16:35\"  \"10/1/19 6:47\"   \"10/1/19 12:59\"  \"10/1/19 16:40\" \n[5313] \"10/2/19 6:55\"   \"10/2/19 15:49\"  \"10/3/19 6:40\"   \"10/3/19 12:35\" \n[5317] \"10/3/19 15:19\"  \"10/4/19 6:54\"   \"10/4/19 7:25\"   \"10/4/19 15:20\" \n[5321] \"10/5/19 6:21\"   \"10/5/19 16:27\"  \"10/6/19 5:20\"   \"10/6/19 15:40\" \n[5325] \"10/6/19 18:59\"  \"10/7/19 6:48\"   \"10/7/19 7:23\"   \"10/7/19 15:19\" \n[5329] \"10/8/19 5:29\"   \"10/8/19 7:01\"   \"10/8/19 16:57\"  \"10/9/19 6:42\"  \n[5333] \"10/9/19 7:07\"   \"10/9/19 15:35\"  \"10/10/19 15:30\" \"10/10/19 17:38\"\n[5337] \"10/10/19 19:16\" \"10/11/19 6:38\"  \"10/11/19 12:41\" \"10/11/19 15:04\"\n[5341] \"10/12/19 6:37\"  \"10/12/19 12:28\" \"10/12/19 16:10\" \"10/13/19 6:26\" \n[5345] \"10/13/19 16:08\" \"10/13/19 17:28\" \"10/14/19 15:09\" \"10/14/19 17:09\"\n[5349] \"10/14/19 19:34\" \"10/15/19 7:17\"  \"10/15/19 15:25\" \"10/15/19 19:17\"\n[5353] \"10/16/19 5:55\"  \"10/16/19 15:47\" \"10/16/19 17:07\" \"10/17/19 5:37\" \n\n\nLet‚Äôs get one value.\n\nobis_seamap_green$date_time[1]\n\n[1] \"5/16/16 6:54\""
  },
  {
    "objectID": "r-tutorials/03-r-read-in-data.html#get-the-size",
    "href": "r-tutorials/03-r-read-in-data.html#get-the-size",
    "title": "R - Read in data",
    "section": "Get the size",
    "text": "Get the size\n\ndim(obis_seamap_green)\n\n[1] 5356   15\n\n\nLet‚Äôs make it a bit smaller. Select the first 500 rows because it takes a long time to plot otherwise.\n\nobis_seamap_green &lt;- obis_seamap_green[1:500,]"
  },
  {
    "objectID": "r-tutorials/03-r-read-in-data.html#look-at-the-top-of-the-file",
    "href": "r-tutorials/03-r-read-in-data.html#look-at-the-top-of-the-file",
    "title": "R - Read in data",
    "section": "Look at the top of the file",
    "text": "Look at the top of the file\n\nhead(obis_seamap_green)\n\n# A tibble: 6 √ó 15\n       oid    id dataset_id    tsn scientific common provider latitude longitude\n     &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;  &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n1   2.18e8   219       2069 173833 Chelonia ‚Ä¶ Green‚Ä¶ Nicolas‚Ä¶     24.6      53.0\n2   2.18e8     1       2069 173833 Chelonia ‚Ä¶ Green‚Ä¶ Nicolas‚Ä¶     24.6      53.0\n3   2.18e8   136       2069 173833 Chelonia ‚Ä¶ Green‚Ä¶ Nicolas‚Ä¶     24.6      53.1\n4   2.18e8   258       2069 173833 Chelonia ‚Ä¶ Green‚Ä¶ Nicolas‚Ä¶     24.6      53.0\n5   2.18e8   292       2069 173833 Chelonia ‚Ä¶ Green‚Ä¶ Nicolas‚Ä¶     24.6      53.0\n6   2.18e8   259       2069 173833 Chelonia ‚Ä¶ Green‚Ä¶ Nicolas‚Ä¶     24.6      53.0\n# ‚Ñπ 6 more variables: lprecision &lt;dbl&gt;, obs_date &lt;chr&gt;, date_time &lt;chr&gt;,\n#   count &lt;dbl&gt;, ds_type &lt;chr&gt;, platform &lt;chr&gt;"
  },
  {
    "objectID": "r-tutorials/03-r-read-in-data.html#lets-load-new-data-from-robis",
    "href": "r-tutorials/03-r-read-in-data.html#lets-load-new-data-from-robis",
    "title": "R - Read in data",
    "section": "Let‚Äôs load new data from robis",
    "text": "Let‚Äôs load new data from robis\nSpecify a bounding box.\n\nlibrary(dplyr)\nlibrary(sf)\nextent_polygon &lt;- sf::st_bbox(c(xmin = 41.875, xmax = 65.125, \n                            ymax = -0.125, ymin = 32.125), \n                          #Assign reference system\n                          crs = sf::st_crs(4326)) %&gt;% st_as_sfc()\npol_geometry &lt;- sf::st_as_text(extent_polygon[[1]])\npol_geometry\n\n[1] \"POLYGON ((41.875 32.125, 65.125 32.125, 65.125 -0.125, 41.875 -0.125, 41.875 32.125))\"\n\n\nLet‚Äôs get some sharks and rays data. Sharks and rays in Bay of Bengal ref1 ref2 ref3\n\nfil &lt;- here::here(\"r-tutorials\", \"data\", \"sharks.csv\")\nif(!exists(fil)){\nsharks &lt;- c(\"Scoliodon laticaudus\", \"Sphrna lewin\", \"Rhizoprionodon acutus\", \"Chiloscyllium indicum\", \"Galeocerdo cuvier\", \"Carcharhinus melanopterus\", \"Carcharhinus falciformis\", \"Rhizoprionodon acutus\", \"Carcharhinus leucas\", \"Carcharhinus  sorrah\")\nobs &lt;- robis::occurrence(sharks, geometry = pol_geometry)\nreadr::write_csv(obs, file=here::here(\"r-tutorials\", \"data\", \"sharks.csv\"))\n}else{\n  obs &lt;- readr::read_csv(fil)\n}\n\n\nRetrieved 2257 records of approximately 2257 (100%)\n\n\nNow we can explore what kind of shark data we have for these species found in the Bay of Bengal study.\n\nlibrary(mapview)\nrequire(dplyr)\nlibrary(sf)\nobs_sf &lt;- obs[1:100,] %&gt;% as_tibble() %&gt;% \n  st_as_sf(coords = c(\"decimalLongitude\", \"decimalLatitude\"), crs = 4326)\nmapview(obs_sf, col.regions = \"gray\") + mapview(extent_polygon)"
  },
  {
    "objectID": "r-tutorials/03-r-read-in-data.html#ask-chatgpt",
    "href": "r-tutorials/03-r-read-in-data.html#ask-chatgpt",
    "title": "R - Read in data",
    "section": "Ask ChatGPT",
    "text": "Ask ChatGPT\nWhen you are starting to code, you will not know how to do things. Obviously I can tell you but that won‚Äôt help you when I am not here. Instead I‚Äôll have you ask a bot because the bot will be here when I am not. Also this is how people code now that AI is here to be our personal coding assistant.\nOpen https://chat.openai.com/ and login. It‚Äôs free.\nPaste this into the Chat box.\nI have a data.frame named obis_seamap_green with columns scientific, latitude, longitude, and platform. How do I make a histogram of the platform types? Only show the code. I want to use tidyverse.\nChatGPT returns this (or something similar)\n\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî forcats   1.0.0     ‚úî stringr   1.5.0\n‚úî ggplot2   3.4.2     ‚úî tibble    3.2.1\n‚úî lubridate 1.9.2     ‚úî tidyr     1.3.0\n‚úî purrr     1.0.1     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nobis_seamap_green %&gt;%\n  ggplot(aes(x = platform)) +\n  geom_bar() +\n  labs(title = \"Histogram of Platform Types\", x = \"Platform Type\", y = \"Frequency\")\n\n\n\n\nHmm that look weird? How many platform types are there? Let‚Äôs ask ChatGPT.\nHow do I find out the values for platform in my file\nChatGPT will return something like\n\nunique_platforms &lt;- unique(obis_seamap_green$platform)\n\nand we have to type\n\nunique_platforms\n\n[1] \"tag\"\n\n\nto see the values. Oh, there is only one value."
  },
  {
    "objectID": "r-tutorials/03-r-read-in-data.html#make-a-map",
    "href": "r-tutorials/03-r-read-in-data.html#make-a-map",
    "title": "R - Read in data",
    "section": "Make a map",
    "text": "Make a map\nLet‚Äôs get ChatGPT to help us map our points.\n‚Äúhow can I make a map with my latitude longitude values on it‚Äù ChatGPT will most likely show you ggplot2 code like so\n\n# Install and load necessary packages (if not already installed)\n# install.packages(\"ggplot2\")\n# install.packages(\"maps\")\nlibrary(ggplot2)\nlibrary(maps)\n\n\nAttaching package: 'maps'\n\n\nThe following object is masked from 'package:purrr':\n\n    map\n\n# Assuming your data frame is named \"obis_seamap_green\"\n# Replace \"latitude\" and \"longitude\" with your actual column names\n\n# Create a basic map\nworld_map &lt;- map_data(\"world\")\n\n# Plot latitude and longitude points on the map\nggplot(obis_seamap_green, aes(x = longitude, y = latitude)) +\n  geom_polygon(data = world_map, aes(x = long, y = lat, group = group), fill = \"white\", color = \"black\") +\n  geom_point() +\n  labs(title = \"Map of Latitude and Longitude Points\")\n\n\n\n\n\nTell ChatGPT: ‚ÄúThe map is the whole world and my points are in one small region. How can I fix that?‚Äù\n\n\n# Assuming your data frame is named \"obis_seamap_green\"\n# Replace \"latitude\" and \"longitude\" with your actual column names\n\n# Define the latitude and longitude limits for your region of interest\n# Adjust the values accordingly to zoom in on your specific region\nlat_limit &lt;- c(min(obis_seamap_green$latitude), max(obis_seamap_green$latitude))\nlon_limit &lt;- c(min(obis_seamap_green$longitude), max(obis_seamap_green$longitude))\n\n# Create a map focused on your region of interest\nggplot(obis_seamap_green, aes(x = longitude, y = latitude)) +\n  geom_polygon(data = world_map, aes(x = long, y = lat, group = group), fill = \"white\", color = \"black\") +\n  geom_point() +\n  labs(title = \"Map of Latitude and Longitude Points\") +\n  xlim(lon_limit) + ylim(lat_limit)\n\n\n\n\n\nThat looks terrible! At this point, you could describe the problem to ChatGPT and it will help you. But better yet, is to know about the mapview package.\nAsk ChatGPT: ‚ÄúCan I use mapview?‚Äù\n\n\n# Install and load the mapview package (if not already installed)\n# install.packages(\"mapview\")\nlibrary(mapview)\n\n# Assuming your data frame is named \"obis_seamap_green\"\n# Replace \"latitude\" and \"longitude\" with your actual column names\n\n# Create a map view\nmapview(obis_seamap_green, zcol = NULL, map.types = \"OpenStreetMap\")\n\n\nIt doesn‚Äôt work! Tell ChatGPT: ‚ÄúI get an error that says Either convert obis_seamap_green to a spatial object or provide xcol and ycol. oops! Arguments xcol and/or ycol are missing!‚Äù It apologizes and gives you a fix.\n\n\n# Install and load the required packages (if not already installed)\n# install.packages(\"mapview\")\n# install.packages(\"sf\")\nlibrary(mapview)\nlibrary(sf)\n\n# Assuming your data frame is named \"obis_seamap_green\"\n# Replace \"latitude\" and \"longitude\" with your actual column names\n\n# Convert your data frame to a spatial object\nobis_seamap_sf &lt;- st_as_sf(obis_seamap_green, coords = c(\"longitude\", \"latitude\"))\n\n# Create a map view\nmapview(obis_seamap_sf)\n\n\n\n\n\n\n\nTell ChatGPT: ‚ÄúBetter! But the land is not shown‚Äù\n\n\n# Assuming your data frame is named \"obis_seamap_green\"\n# Replace \"latitude\" and \"longitude\" with your actual column names\nlibrary(mapview)\nlibrary(sf)\n\n# Convert your data frame to a spatial object\nobis_seamap_sf &lt;- st_as_sf(obis_seamap_green, coords = c(\"longitude\", \"latitude\"))\n\n# Create a map view with Esri.WorldImagery basemap\nmapview(obis_seamap_sf, map.types = \"Esri.WorldImagery\")\n\n\n\n\n\n\n\nTell ChatGPT: ‚ÄúI get another warning writing: substituting ENGCRS[‚ÄùUndefined Cartesian SRS with unknown unit‚Äù] for missing CRS]‚Äù. It gives us a fix.\n\n\n# Assuming your data frame is named \"obis_seamap_green\"\n# Replace \"latitude\" and \"longitude\" with your actual column names\nlibrary(mapview)\nlibrary(sf)\n\n# Convert your data frame to a spatial object and set the CRS explicitly\nobis_seamap_sf &lt;- st_as_sf(obis_seamap_green, coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\n# Create a map view with Esri.WorldImagery basemap\nmapview(obis_seamap_sf, map.types = \"Esri.WorldImagery\")"
  },
  {
    "objectID": "r-tutorials/03-r-read-in-data.html#clean-up-the-date",
    "href": "r-tutorials/03-r-read-in-data.html#clean-up-the-date",
    "title": "R - Read in data",
    "section": "Clean up the date",
    "text": "Clean up the date\nIt is common that the date columns are in different formats and we need to change to a format that R will understand.\n\nobis_seamap_green$date_time[1]\n\n[1] \"5/16/16 6:54\"\n\n\nI don‚Äôt have about the time, I just want date in a standard format. Let‚Äôs ask ChatGPT.\n\n‚ÄúMy date_time column looks like this‚Äù5/16/16 6:54‚Äù. I need a date column in a standard format.‚Äù\n\n\n# Assuming your data frame is named \"obis_seamap_green\"\n# Replace \"date_time\" with your actual column name\nlibrary(mapview)\nlibrary(sf)\n\n# Convert the date_time column to a standard date format\nobis_seamap_green$date &lt;- as.Date(obis_seamap_green$date_time, format = \"%m/%d/%y %H:%M\")\n\n# Remove the original date_time column if needed\nobis_seamap_green &lt;- obis_seamap_green[, !names(obis_seamap_green) %in% \"date_time\"]\n\nLet‚Äôs check if it worked.\n\nobis_seamap_green$date[1]\n\nWarning: Unknown or uninitialised column: `date`.\n\n\nNULL\n\nclass(obis_seamap_green$date)\n\nWarning: Unknown or uninitialised column: `date`.\n\n\n[1] \"NULL\""
  },
  {
    "objectID": "r-tutorials/03-r-read-in-data.html#save-the-data-frame",
    "href": "r-tutorials/03-r-read-in-data.html#save-the-data-frame",
    "title": "R - Read in data",
    "section": "Save the data frame",
    "text": "Save the data frame\nMany times we do a bit of work on our data by cleaning up an such. Once we are done, we should save our data so we can load it in easily later.\nFirst we make a R script that shows how we created our data. We do this to be nice to our ‚Äúfuture‚Äù selves. Create a R script and save in the data folder as create-obis-seamap-green.R.\nlibrary(readr)\nobis_seamap_green &lt;- read_csv(\"data/obis_seamap_green.csv\", \n    skip = 2)\nobis_seamap_green$date &lt;- as.Date(obis_seamap_green$date_time, format = \"%m/%d/%y %H:%M\")\n\n# Remove the original date_time column if needed\nobis_seamap_green &lt;- obis_seamap_green[, !names(obis_seamap_green) %in% \"date_time\"]\nNow that we have recorded how we made our file, we save our data to a file.\n\nsave(obis_seamap_green, file=\"data/obis_seamap_green.RData\")\n\nNext time we want to load these data we can run\n\nload(\"data/obis_seamap_green.RData\")"
  },
  {
    "objectID": "r-tutorials/03-r-read-in-data.html#summary",
    "href": "r-tutorials/03-r-read-in-data.html#summary",
    "title": "R - Read in data",
    "section": "Summary",
    "text": "Summary\n\nYou learned how to import csv and Excel files using the Import widget. Tip: use NA for missing values!\nYou learned how to interact with ChatGPT as your personal coding helper. Tip: We are using tidyverse so let it know that you want that. Use ‚Äúonly show code‚Äù if you don‚Äôt want all the narrative.\nIn order to work with ChatGPT, you need to be able to tell it what your data look like. Core commands you need to know:\n\nClass class(object)\nColumn names colnames(object)\nSize dim(object)\n\nUse these commands to learn what to tell ChatGPT.\nYou learned how to ask ChatGPT to help you plot and how to tell it what the problems are so it can help you. You learned about the mapview R package. ChatGPT knows about it, but rarely will it suggest it on it‚Äôs own.\nYou made a (boring) plot with the ggplot2 plotting package. This is part of the tidyverse. It is 2023, we use ggplot2 for plotting not base R (plot()). We will cover much more plotting with ggplot2 in another tutorial."
  },
  {
    "objectID": "r-tutorials/03-r-read-in-data.html#your-turn",
    "href": "r-tutorials/03-r-read-in-data.html#your-turn",
    "title": "R - Read in data",
    "section": "Your turn!",
    "text": "Your turn!\nImport some of the other files in the data directory."
  },
  {
    "objectID": "r-tutorials/04-r-ggplot2.html",
    "href": "r-tutorials/04-r-ggplot2.html",
    "title": "R - ggplot2",
    "section": "",
    "text": "This tutorial will give an introduction to plotting with the ggplot2 package, which is the modern way to make general plots with R. I will talk tomorrow about plots of spatial data.\nThis is oriented to those with little or no experience using ggplot2 or those who have tried it and gotten frustrated. The goal is to get you started. Once you have the basics, there are a many many ggplot tutorials online ‚Äì though personally the basics are all I need.\nThe goal is to give you enough of an understanding of ggplot2 so that you can create prompts for ChatGPT. You need to know the basics of ggplot2 to understand ChatGPT‚Äôs code and know what sort of things to ask ChatGPT. But if you are lost, you can always tell it what you want and ask it to ask you questions."
  },
  {
    "objectID": "r-tutorials/04-r-ggplot2.html#load-the-library",
    "href": "r-tutorials/04-r-ggplot2.html#load-the-library",
    "title": "R - ggplot2",
    "section": "Load the library",
    "text": "Load the library\n\nlibrary(ggplot2)"
  },
  {
    "objectID": "r-tutorials/04-r-ggplot2.html#basic-x-versus-y-plot",
    "href": "r-tutorials/04-r-ggplot2.html#basic-x-versus-y-plot",
    "title": "R - ggplot2",
    "section": "Basic x versus y plot",
    "text": "Basic x versus y plot\n\nWorkflow\nLet‚Äôs say you wanted to make a simple x versus y plot. Here‚Äôs your ggplot() workflow.\n\nDecide on x.\nDecide on y.\nMake a data frame with x and y.\nCreate the plot object with a call to ggplot() to tell it the data frame and then what the x and y to use: aes(x=..., y=...).\nAdd points or lines to the plot with geom_line() or geom_point().\n\n\n\nCreate data frame\n\nval &lt;- mtcars$mpg\nx &lt;- mtcars$hp\ndf &lt;- data.frame(x=x, val=val)\n\n\n\nggplot call\n\np1 &lt;- ggplot(df, aes(x=x, y=val)) + \n        geom_point() # plot a line\np1 # plot it\n\n\n\n\np1 is a ggplot object.\n\nclass(p1)\n\n[1] \"gg\"     \"ggplot\"\n\n\nWe can add elements to a ggplot object\n\np1 + geom_line()\n\n\n\n\n\n\n\n\n\n\nAn x y plot\n\n\n\nggplot(df, aes(x=x, y=val)) +\n geom_point() +\n geom_line()"
  },
  {
    "objectID": "r-tutorials/04-r-ggplot2.html#changing-the-whole-look",
    "href": "r-tutorials/04-r-ggplot2.html#changing-the-whole-look",
    "title": "R - ggplot2",
    "section": "Changing the whole look",
    "text": "Changing the whole look\nggplot uses themes to set the look of your plot and you can change the whole look by setting a different theme. You can also just tweak one element of the plot‚Äôs existing theme. Note because we fixed the line and point colors, we override some theme elements (eg, line colors). See ?theme_bw to see all the themes. See ?theme to learn how to change one element of your plot design.\n\np1 + theme_classic()"
  },
  {
    "objectID": "r-tutorials/04-r-ggplot2.html#using-chatgpt",
    "href": "r-tutorials/04-r-ggplot2.html#using-chatgpt",
    "title": "R - ggplot2",
    "section": "Using ChatGPT",
    "text": "Using ChatGPT\n\nBoxplots\nNow that you see the basic structure of a simple x versus y plot, let‚Äôs use ChatGPT to help us make plots.\n\n\n\n\n\n\nPrompt\n\n\n\nI have a data frame with species, gender and bill length. I want to make a boxplot by species with ggplot2 where the genders are separate and side by side.\n\n\nThis what ChatGPT gave me. Try using this prompt and adding more info until you get a plot you like.\n\n# Load the ggplot2 library\nlibrary(ggplot2)\n\n# Sample data frame (replace with your actual data)\ndata &lt;- data.frame(\n  species = c(\"Species A\", \"Species A\", \"Species B\", \"Species B\", \"Species C\", \"Species C\"),\n  gender = c(\"Male\", \"Female\", \"Male\", \"Female\", \"Male\", \"Female\"),\n  bill_length = c(10.2, 9.8, 11.5, 10.7, 9.3, 9.9)\n)\n\n# Create the side-by-side boxplot\nggplot(data, aes(x = species, y = bill_length, fill = gender)) +\n  geom_boxplot(position = position_dodge(width = 0.8)) +\n  labs(title = \"Boxplot of Bill Length by Species and Gender\",\n       x = \"Species\",\n       y = \"Bill Length\") +\n  scale_fill_manual(values = c(\"Male\" = \"blue\", \"Female\" = \"red\"))\n\n\n\n\n\n\nAdding trends\n\n\n\n\n\n\nPrompt\n\n\n\nI have a data frame with value and time for 3 regions. I want to make a scatter plot of time versus value for each region and add a linear regression to each. I would like each region to be in a separate panel. Show only the code. Please make a sample data frame that I can use to run the code.\n\n\nChatGPT gives:\n\n# Load the required libraries\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Create a sample data frame\ndf &lt;- data.frame(\n  Region = rep(c(\"Region A\", \"Region B\", \"Region C\"), each = 100),\n  Time = rep(seq(1, 100), times = 3),\n  Value = c(rnorm(100, mean = 50, sd = 10),\n            rnorm(100, mean = 70, sd = 12),\n            rnorm(100, mean = 60, sd = 15))\n)\n\n# Create the scatter plot with separate panels for each region\nggplot(df, aes(x = Time, y = Value, color = Region)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(title = \"Scatter Plot with Linear Regression Lines\",\n       x = \"Time\",\n       y = \"Value\") +\n  facet_wrap(~Region)\n\n\n\n\n\n\nGiving your data to ChatGPT\nYou can paste in the top of your data (if it is csv) and ChatGPT can figure out the structure. Then you can say what you want to do with this data.\nDemo"
  },
  {
    "objectID": "r-tutorials/04-r-ggplot2.html#arranging-plots-into-grids",
    "href": "r-tutorials/04-r-ggplot2.html#arranging-plots-into-grids",
    "title": "R - ggplot2",
    "section": "Arranging plots into grids",
    "text": "Arranging plots into grids\n\nDynamically\nggplot() will dynamically break your data into plots for you using facet_wrap().\n\ndf1 &lt;- data.frame(t=1:10, val=10:1, name=\"df1\")\ndf2 &lt;- data.frame(t=1:20, val=1:20+100, name=\"df2\")\ndf &lt;- rbind(df1, df2)\np1 &lt;- ggplot(df, aes(x=t, y=val, col=name)) + \n  geom_line() +\n  facet_wrap(~name)\np1\n\n\n\n\nWe can ‚Äúfree‚Äù the scales.\n\np1 + facet_wrap(~name, scales=\"free\")\n\n\n\n\n\n\nManually\nWe can arrange plots into a grid manually with gridExtra::grid.arrange() (from the gridExtra package).\n\nlibrary(gridExtra)\ndf &lt;- data.frame(t = 1:10, val = rnorm(10))\np1 &lt;- ggplot(df, aes(x=t, y=val)) + geom_line() + geom_point()\np2 &lt;- ggplot(df, aes(x=t, y=val+100)) + geom_line()\np3 &lt;- ggplot(df, aes(x=t, y=100*val)) + geom_point()\ndf2 &lt;- data.frame(t = 1:4, val = 1:4, se=.1*(1:4))\np4 &lt;- ggplot(df2, aes(x=t, y=val)) + geom_col()\ngridExtra::grid.arrange(p1, p2, p3, p4)\n\n\n\n\nInstead of gridExtra and grid.arrange(), check out the patchwork package. It does similar jobs as grid.arrange() but is better.\nLet‚Äôs do two plots in one column but add new info to plot 1.\n\ngridExtra::grid.arrange(p1+geom_smooth(method=\"lm\"), \n                        p4+geom_errorbar(aes(ymin=val-se, ymax=val+se), width=0.3), ncol=1)\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "r-tutorials/04-r-ggplot2.html#getting-your-data-in-the-right-format",
    "href": "r-tutorials/04-r-ggplot2.html#getting-your-data-in-the-right-format",
    "title": "R - ggplot2",
    "section": "Getting your data in the right format",
    "text": "Getting your data in the right format\nYou need your data in long format.\nLet‚Äôs say you had temperature data for 3 rivers. It would be common to have your data like this. This is wide format.\nDate, River1, River2, River3\n1, 45, 35, 20\n2, 25, 30, 21\nYou need to get this in long format like so.\nDate, River, Temperature\n1, River1, 45\n1, River2, 35\n1. River3, 20\n2, River1, 25\n2, River2, 30\n2, River3, 21\nYou do this with code not manually! This is hard to remember how to do, so just ask ChatGPT. Pass the data into ChatGPT and ask it how to put it in long format.\n\n\n\n\n\n\nPrompt\n\n\n\nhow can I read a text string of my data in\n\n\n\n# Your data as a text string\ndata_string &lt;- \"Date, River1, River2, River3\n1, 45, 35, 20\n2, 25, 30, 21\"\n\n# Use read.table() to read the data from the text string\ndf &lt;- read.table(text = data_string, header = TRUE, sep = \",\")\n\n# Print the resulting data frame\nprint(df)\n\n  Date River1 River2 River3\n1    1     45     35     20\n2    2     25     30     21\n\n\n\n\n\n\n\n\nPrompt\n\n\n\nHow do I convert that to long format\n\n\n\nlibrary(tidyr)\ndf_long &lt;- gather(df, key = \"River\", value = \"Value\", -Date)\ndf_long\n\n  Date  River Value\n1    1 River1    45\n2    2 River1    25\n3    1 River2    35\n4    2 River2    30\n5    1 River3    20\n6    2 River3    21"
  },
  {
    "objectID": "r-tutorials/04-r-ggplot2.html#creating-plot-templates",
    "href": "r-tutorials/04-r-ggplot2.html#creating-plot-templates",
    "title": "R - ggplot2",
    "section": "Creating plot templates",
    "text": "Creating plot templates\nIf you are creating plots with the same features over and over, you can hold the features in a list and add that on to your plot.\nExample where I want all my plots to have red points and a regression line:\n\np3 &lt;- geom_point(col=\"red\")\np2 &lt;- geom_smooth(method=\"lm\")\np4 &lt;- xlab(\"MPG\")\np5 &lt;- ylab(\"HP\")\n# pt is my template\npt &lt;- list(p2, p3, p4, p5)\n\np1 &lt;- ggplot(mtcars, aes(x=mpg, y=hp)) + pt\np2 &lt;- ggplot(mpg, aes(x=hwy,y=cyl)) + pt\ngrid.arrange(p1, p2)"
  },
  {
    "objectID": "r-tutorials/04-r-ggplot3.html",
    "href": "r-tutorials/04-r-ggplot3.html",
    "title": "R - ggplot2",
    "section": "",
    "text": "This tutorial will give an introduction to plotting with the ggplot2 package. Getting a basic familiarity with ggplot2 will really save you a lot of time.\nThis is oriented to those with little or no experience using ggplot2 or those who have tried it and gotten frustrated. The goal is to get you started. Once you have the basics, there are a so many ggplot tutorials online ‚Äì though personally the basics are all I need.\nlibrary(ggplot2)\nlibrary(gridExtra)"
  },
  {
    "objectID": "r-tutorials/04-r-ggplot3.html#basic-x-versus-y-plot",
    "href": "r-tutorials/04-r-ggplot3.html#basic-x-versus-y-plot",
    "title": "R - ggplot2",
    "section": "Basic x versus y plot",
    "text": "Basic x versus y plot\n\nWorkflow\nLet‚Äôs say you wanted to make a simple xy plot with plot(). Here‚Äôs your workflow.\n\nDecide on x or use the default (1 to the number of data points).\nDecide on y.\nPlot with plot(x,y)\n\n\nval &lt;- mtcars$mpg\nx &lt;- mtcars$hp\nplot(x, val, type=\"p\")\n\n\n\n\nHere‚Äôs your ggplot() workflow.\n\nDecide on x or use the default (1 to the number of data points).\nDecide on y.\nMake a data frame with x and y.\nCreate the plot object with a call to ggplot() to tell it the data frame and then what the x and y to use: aes(x=..., y=...).\nAdd points or lines to the plot with geom_line() or geom_point().\n\n\ndf &lt;- data.frame(x=mtcars$hp, val=mtcars$mpg)\np1 &lt;- ggplot(df, aes(x=x, y=val)) + # set up\n        geom_point() # plot a line\np1 # plot it\n\n\n\n\nTypically you‚Äôd just write the call like so\nggplot(df, aes(x=x, y=val)) + geom_point()\np1 is a ggplot object.\n\nclass(p1)\n\n[1] \"gg\"     \"ggplot\"\n\n\nWe can add elements to a ggplot object\n\np1 + geom_line()\n\n\n\n\n\n\n\n\n\n\nAn x y plot\n\n\n\nggplot(df, aes(x=x, y=val)) +\n geom_point() +\n geom_line()"
  },
  {
    "objectID": "r-tutorials/04-r-ggplot3.html#modifying-your-plot",
    "href": "r-tutorials/04-r-ggplot3.html#modifying-your-plot",
    "title": "R - ggplot2",
    "section": "Modifying your plot",
    "text": "Modifying your plot\nThere are two different workflows that you need to decide on.\n\nDynamic colors etc: Let ggplot() pick your colors, points, line widths etc.\nFixed colors etc: Manually choose your colors, points, line widths etc, aka use a fixed value.\n\nGravitating to option 1 will make your life with ggplot() easier, but let‚Äôs start with option 2.\n\n\n\n\n\n\nImportant\n\n\n\nFixed lines, points attributes go outside of aes() in a geom_...() call. Dynamic attributes go inside of aes().\n\n\nLook at ?geom_point to see the attributes that you can pass in.\n\nggplot(df, aes(x=x, y=val)) + geom_point(col=\"blue\")\n\n\n\n\nThe length of the fixed attribute must be 1 or the length of the data.\n\nggplot(df, aes(x=x, y=val)) + \n  geom_point(shape=21, col=\"blue\", fill=mtcars$carb, size=6, alpha=.5, stroke=2)\n\n\n\n\nWays to set attributes that won‚Äôt work as you think:\nPutting color outside of aes() in ggplot() does nothing. ggplot() sets up the data to use, but information outside aes() doesn‚Äôt flow to the plotting functions like geom_point().\n\nggplot(df, aes(x=x, y=val), col=\"blue\") + geom_point()\n\n\n\n\nPutting color inside of aes() in ggplot() has a non-intuitive effect. Plot attributes, like color, in aes() are converted to factors and the colors (etc) will be chosen dynamically. The name ‚Äúblue‚Äù is not a color is the a factor and ggplot() gives the first factor the color red in this case. Information in aes() will flow to the rest of the plot unless you tell the geom_point() otherwise).\n\nggplot(df, aes(x=x, y=val, color=\"blue\")) + geom_point()\n\n\n\n\nHow do we tell geom_point() not to use the color value in aes() in ggplot()? Tell it to use a fixed value by putting col=\"blue\" outside of an aes() call in geom_point().\n\nggplot(df, aes(x=x, y=val)) + geom_point(col=\"blue\")\n\n\n\n\nWhat happens if we put the color in aes() in geom_point()?\n\nggplot(df, aes(x=x, y=val)) + geom_point(aes(color=\"blue\"))\n\n\n\n\nLet‚Äôs make a plot with big red points and a thick blue line.\n\np1 &lt;- ggplot(df, aes(x=x, y=val)) + \n  geom_line(col=\"blue\", size=2) +\n  geom_point(col=\"red\", size=3)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead.\n\np1\n\n\n\n\nLet‚Äôs add a column plot to that. I pass in alpha to add some transparency to the columns so they don‚Äôt wipe out the line.\n\np1 + geom_col(alpha=0.5, position=\"dodge\")\n\n\n\n\n\nLabels and limits\nWith ggplot, use xlab() ylab() and xlim() ylim(). Ignore the NA warning. I‚Äôll show how to stop that later.\n\nggplot(df, aes(x=x, y=val)) +\n  geom_point() +\n  xlab(\"time\") + ylab(\"value\") +\n  ggtitle(\"ggplot plot\") +\n  xlim(c(100,200)) + ylim(c(0,40))\n\nWarning: Removed 16 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "r-tutorials/04-r-ggplot3.html#changing-the-whole-look",
    "href": "r-tutorials/04-r-ggplot3.html#changing-the-whole-look",
    "title": "R - ggplot2",
    "section": "Changing the whole look",
    "text": "Changing the whole look\nggplot uses themes to set the look of your plot and you can change the whole look by setting a different theme. You can also just tweak one element of the plot‚Äôs existing theme. Note because we fixed the line and point colors, we override some theme elements (eg, line colors). See ?theme_bw to see all the themes. See ?theme to learn how to change one element of your plot design.\n\np1 + theme_classic()"
  },
  {
    "objectID": "r-tutorials/04-r-ggplot3.html#adding-lines-or-points",
    "href": "r-tutorials/04-r-ggplot3.html#adding-lines-or-points",
    "title": "R - ggplot2",
    "section": "Adding lines or points",
    "text": "Adding lines or points\nLet‚Äôs say you want to plot 2 lines.\n\nWorkflow\nIn ggplot() you have two possible workflows.\nOption 1. This workflow is kind of bad but often easier for beginners.\n\nMake a data frame for each line: df and df2.\nCall geom_line() with data=df or data=df and a aes()\n\n\ndf &lt;- data.frame(x=mtcars$hp, val=mtcars$mpg)\ndf2 &lt;- data.frame(x=df$x, val2=df$val+10)\nggplot() +\n  geom_point(data=df, aes(x=x, y=val)) +\n  geom_line(data=df2, aes(x=x, y=val2))\n\n\n\n\nOne issue with aes() and data is that it is inherited rightward. This inheriting feature is usually very handy, but when working with multiple data frames, it is often clearer if you keep the data and aes() with the points and lines.\nHere is another example of plotting data from two different data frames:\n\nggplot() +\n  geom_line(data=iris, aes(x = Sepal.Length, y=Petal.Length)) +\n  geom_bar(data=mtcars, aes(x=cyl), alpha=0.3) +\n  ylab(\"y\") + xlab(\"x\")\n\n\n\n\n\n\nAdding a legend\nggplot makes it rather hard to modify your legend if you create a plot this way. Creating a manual legend, as opposed to dynamically as ggplot is supposed to work, can be quite hard. First thing to know is that the color, linestyle, and/or shape must be in aes() to appear in the legend. If it‚Äôs not there you can‚Äôt control it in the legend.\nNote: What I am about to show is really hacky and not the way ggplot is intended to be used, but it comes up so often for new ggplot users that I want you to see a solution so you don‚Äôt give up on ggplot because of legends. Jump ahead to the correct ggplot workflow with long-form data frames to see how to avoid this.\nThis works. Ignore the warning about unknown aesthetics.\n\nggplot() +\n  geom_point(data=df, aes(x=x, y=val, col=\"df\", linetype=\"df\")) +\n  geom_line(data=df2, aes(x=x, y=val2, col=\"df2\", linetype=\"df2\")) +\n  scale_color_manual(\"name\", values=c(\"blue\", \"black\")) +\n  scale_linetype_manual(\"name\",values=c(0,1))\n\nWarning in geom_point(data = df, aes(x = x, y = val, col = \"df\", linetype =\n\"df\")): Ignoring unknown aesthetics: linetype"
  },
  {
    "objectID": "r-tutorials/04-r-ggplot3.html#correct-ggplot-workflow",
    "href": "r-tutorials/04-r-ggplot3.html#correct-ggplot-workflow",
    "title": "R - ggplot2",
    "section": "Correct ggplot workflow",
    "text": "Correct ggplot workflow\nThis is how ggplot() is intended to be used: one long data frame.\n\nMake data frames with df and df2 data and a ‚Äúname‚Äù column.\nSet up the plot with ggplot()\nMake points or line different using the ‚Äúname‚Äù column\n\n\ndf1 &lt;- data.frame(x=x, val=val, name=\"df1\")\ndf2 &lt;- data.frame(x=x, val=val+10, name=\"df2\")\ndf3 &lt;- rbind(df1, df2)\nggplot(df3, aes(x=x, y=val, col=name, shape=name)) + \n  geom_line() +\n  geom_point()\n\n\n\n\n\n\nIf the color, shape, linetype is not in an aes() it won‚Äôt appear in the legend.\nIf the color, shape, or linetype is in an aes() it will appear in the legend.\naes() info in ggplot() flows to the other elements. Put the aes() info in the individual geom_...() calls if you don‚Äôt want that. Use inherit.aes=FALSE to stop this inheriting behavior.\nWant to mix points and lines? You need to use scale_..._manual() to manually turn-off points or lines for some of the data.\nYou can always force colors, shapes, linetypes by passing in color, shape, size etc outside of aes() but it won‚Äôt appear in the legend. Only colors, shapes, etc, that appear in aes() will appear in a legend.\n\nExample, points are all black since the aes(col=name) only appears in the geom_line() call.\n\nggplot(df3, aes(x=x, y=val)) + \n  geom_line(aes(col=name)) +\n  geom_point(aes(shape=name))\n\n\n\n\n\n\nModifying the legend\nYou can control all aspects of the legend. Read up on it here.\nManually changing data points and other lines will require scale_...() and gets hacky.\n\nggplot(df3, aes(x=x, y=val)) + \n  geom_line(aes(col=name, linetype=name)) +\n  geom_point(aes(shape=name)) +\n  scale_color_manual(\"name\", values=c(\"blue\", \"black\")) +\n  scale_shape_manual(\"name\",values=c(NA,1)) +\n  scale_linetype_manual(\"name\",values=c(1,0))\n\nWarning: Removed 32 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\nNA warnings\nAck all those NA warnings!\n\ndf4 &lt;- data.frame(x=1:10, y=c(1,NA,NA,4:10), name=rep(c(\"a\",\"b\"),5))\nggplot(df4, aes(x=x, y=y, col=name)) + \n  geom_point()\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\nGet rid of them using na.rm=TRUE.\n\nggplot(df4, aes(x=x, y=y, col=name)) + \n  geom_point(na.rm=TRUE)"
  },
  {
    "objectID": "r-tutorials/04-r-ggplot3.html#making-a-boxplot",
    "href": "r-tutorials/04-r-ggplot3.html#making-a-boxplot",
    "title": "R - ggplot2",
    "section": "Making a boxplot",
    "text": "Making a boxplot\nLet‚Äôs look at the mpg data frame.\n\nmpg\n\n# A tibble: 234 √ó 11\n   manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n   &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n 1 audi         a4           1.8  1999     4 auto‚Ä¶ f        18    29 p     comp‚Ä¶\n 2 audi         a4           1.8  1999     4 manu‚Ä¶ f        21    29 p     comp‚Ä¶\n 3 audi         a4           2    2008     4 manu‚Ä¶ f        20    31 p     comp‚Ä¶\n 4 audi         a4           2    2008     4 auto‚Ä¶ f        21    30 p     comp‚Ä¶\n 5 audi         a4           2.8  1999     6 auto‚Ä¶ f        16    26 p     comp‚Ä¶\n 6 audi         a4           2.8  1999     6 manu‚Ä¶ f        18    26 p     comp‚Ä¶\n 7 audi         a4           3.1  2008     6 auto‚Ä¶ f        18    27 p     comp‚Ä¶\n 8 audi         a4 quattro   1.8  1999     4 manu‚Ä¶ 4        18    26 p     comp‚Ä¶\n 9 audi         a4 quattro   1.8  1999     4 auto‚Ä¶ 4        16    25 p     comp‚Ä¶\n10 audi         a4 quattro   2    2008     4 manu‚Ä¶ 4        20    28 p     comp‚Ä¶\n# ‚Ñπ 224 more rows\n\n\n\nggplot(mpg, aes(x=drv, y=hwy)) +\n  geom_boxplot()\n\n\n\n\n\nggplot(mpg, aes(x=drv, y=hwy)) +\n  geom_boxplot() +\n  facet_wrap(~class) +\n  xlab(\"\") + ylab(\"mpg\") +\n  ggtitle(\"highway miles per gallon by class\")"
  },
  {
    "objectID": "r-tutorials/04-r-ggplot3.html#arranging-plots-into-grids",
    "href": "r-tutorials/04-r-ggplot3.html#arranging-plots-into-grids",
    "title": "R - ggplot2",
    "section": "Arranging plots into grids",
    "text": "Arranging plots into grids"
  },
  {
    "objectID": "r-tutorials/04-r-ggplot3.html#dynamically",
    "href": "r-tutorials/04-r-ggplot3.html#dynamically",
    "title": "R - ggplot2",
    "section": "Dynamically",
    "text": "Dynamically\nggplot() will also dynamically break your data into plots for you.\n\ndf1 &lt;- data.frame(t=1:10, val=10:1, name=\"df1\")\ndf2 &lt;- data.frame(t=1:20, val=1:20+1, name=\"df2\")\ndf &lt;- rbind(df1, df2)\np1 &lt;- ggplot(df, aes(x=t, y=val, col=name)) + geom_line()\np1\n\n\n\n\n\np1 + facet_wrap(~name)\n\n\n\n\n\nhead(mpg)\n\n# A tibble: 6 √ó 11\n  manufacturer model displ  year   cyl trans      drv     cty   hwy fl    class \n  &lt;chr&gt;        &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; \n1 audi         a4      1.8  1999     4 auto(l5)   f        18    29 p     compa‚Ä¶\n2 audi         a4      1.8  1999     4 manual(m5) f        21    29 p     compa‚Ä¶\n3 audi         a4      2    2008     4 manual(m6) f        20    31 p     compa‚Ä¶\n4 audi         a4      2    2008     4 auto(av)   f        21    30 p     compa‚Ä¶\n5 audi         a4      2.8  1999     6 auto(l5)   f        16    26 p     compa‚Ä¶\n6 audi         a4      2.8  1999     6 manual(m5) f        18    26 p     compa‚Ä¶\n\n\nLet‚Äôs plot city mpg versus number of cylinders.\n\npc &lt;- ggplot(mpg, aes(x=cyl, y=cty)) + geom_point()\npc\n\n\n\n\nNow we can divide this up by different factors in our the mpg data frame.\n\npc + facet_wrap(~class)\n\n\n\n\nWe can add some things to our plot and free the scales.\n\npc + facet_wrap(~class, scales=\"free\") + geom_smooth(method=\"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\nManually\nNow let‚Äôs do this in with ggplot() in a similar workflow. The difference with ggplot() is that we save the plots and then arrange them into a grid with gridExtra::grid.arrange() (from the gridExtra package).\n\nlibrary(gridExtra)\ndf &lt;- data.frame(t = 1:10, val = rnorm(10))\np1 &lt;- ggplot(df, aes(x=t, y=val)) + geom_line() + geom_point()\np2 &lt;- ggplot(df, aes(x=t, y=val+100)) + geom_line()\np3 &lt;- ggplot(df, aes(x=t, y=100*val)) + geom_point()\ndf2 &lt;- data.frame(t = 1:4, val = 1:4, se=.1*(1:4))\np4 &lt;- ggplot(df2, aes(x=t, y=val)) + geom_col()\ngridExtra::grid.arrange(p1, p2, p3, p4)\n\n\n\n\nInstead of gridExtra and grid.arrange(), check out the patchwork package. It does similar jobs as grid.arrange() but is better.\nLet‚Äôs do two plots in one column but add new info to plot 1.\n\ngridExtra::grid.arrange(p1+geom_smooth(method=\"lm\"), \n                        p4+geom_errorbar(aes(ymin=val-se, ymax=val+se), width=0.3), ncol=1)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nWhen you don‚Äôt know how many plots you‚Äôll be arranging. You can assemble ggplot objects into a list and pass that to grid.arrange().\n\ndf &lt;- data.frame(t = 1:10, val = rnorm(10))\nplist &lt;- list()\nn &lt;- 3\nfor(i in 1:n) plist[[i]] &lt;- ggplot(df, aes(x=t, y=val)) + geom_line() + geom_point()\ndf2 &lt;- data.frame(t = 1:4, val = 1:4, se=.1*(1:4))\nplist[[n+1]] &lt;- ggplot(df2, aes(x=t, y=val)) + geom_col()\ngridExtra::grid.arrange(grobs=plist)\n\n\n\n\nThe patchwork package gives you the same functionality.\npatchwork::wrap_plots(plist)\nWe can combine these facet wrapped figures into a grid because they are ggplot objects. Making this plot in base R would take you forever and another 2 forevers if you wanted to change it around or if the number of classes in your data changed.\n\npf &lt;- pc + facet_wrap(~class)\ngrid.arrange( p1, pf, ncol=2)"
  },
  {
    "objectID": "r-tutorials/04-r-ggplot3.html#notes",
    "href": "r-tutorials/04-r-ggplot3.html#notes",
    "title": "R - ggplot2",
    "section": "Notes",
    "text": "Notes\n\nfacet_wrap() often balks if you use different data frames in your plot construction, i.e.¬†you doing something kind of hacky.\n\nneeds a column in your data frame factors (or characters it can coerce into factors). Might work with multiple data frames in your plot as long as each data frame has the same ‚Äúname‚Äù column.\nwants all the data frame to be the same length. This is only when you use different data frames. Fine if you have all data in one data frame."
  },
  {
    "objectID": "r-tutorials/05-r-geospatial.html",
    "href": "r-tutorials/05-r-geospatial.html",
    "title": "Geospatial in R - lab 1",
    "section": "",
    "text": "require(sf)\nrequire(mapview)\nrequire(readr)\nrequire(readxl)\nhere::i_am(\"r-tutorials/05-r-geospatial.qmd\")\ndir_data &lt;- here::here(\"r-tutorials\", \"data\")\nIn this lab, you will learn basic skills of working with points. We will store our points in data frames. All our points data frames will have these columns:\nBut the columns names are often shortened to lon, lat or lng, latd or anything else.\nIn addition they will have other info in other columns."
  },
  {
    "objectID": "r-tutorials/05-r-geospatial.html#create-a-data-frame-of-points",
    "href": "r-tutorials/05-r-geospatial.html#create-a-data-frame-of-points",
    "title": "Geospatial in R - lab 1",
    "section": "Create a data frame of points",
    "text": "Create a data frame of points\n\nlibrary(mapview)\n  \n# Create example data of points\nlon &lt;- c(85.21, 80.23, 77.28)\nlat = c(25.59, 12.99, 28.56)\nnames = c(\"Patna\", \"Chennai\", \"New Delhi\")\n  \n# Create a data frame with the point data\ndf &lt;- data.frame(lon, lat, names)"
  },
  {
    "objectID": "r-tutorials/05-r-geospatial.html#convert-to-a-spatial-points-data-frame",
    "href": "r-tutorials/05-r-geospatial.html#convert-to-a-spatial-points-data-frame",
    "title": "Geospatial in R - lab 1",
    "section": "Convert to a spatial points data frame",
    "text": "Convert to a spatial points data frame\n\n\n\n\n\n\nCore skill\n\n\n\nConvert data frame with latitude and longitude columns to a geospatial object with a geometry column and coordinate system. We are setting the coordinate system to WGS 84 with crs = 4326.\n\nsf::st_as_sf() function\n\n\n\nThis is a special data frame where the location data is converted to a single point object.\n\ncities3 &lt;- sf::st_as_sf(\n    df, # the data frame\n    coords = c(\"lon\", \"lat\"), # what are the x and y dimension names\n    crs = 4326)\n\nLook at the class of the object\n\nclass(cities3)\n\n[1] \"sf\"         \"data.frame\""
  },
  {
    "objectID": "r-tutorials/05-r-geospatial.html#plot-the-points",
    "href": "r-tutorials/05-r-geospatial.html#plot-the-points",
    "title": "Geospatial in R - lab 1",
    "section": "Plot the points",
    "text": "Plot the points\n\nplot(cities3)\n\n\n\n\nIt plotted but it is not very useful. Let‚Äôs use the helper package mapview. That‚Äôs more useful.\n\nmapview::mapview(cities3, label = cities3$names)"
  },
  {
    "objectID": "r-tutorials/05-r-geospatial.html#read-in-points-from-files",
    "href": "r-tutorials/05-r-geospatial.html#read-in-points-from-files",
    "title": "Geospatial in R - lab 1",
    "section": "Read in points from files",
    "text": "Read in points from files\n\n\n\n\n\n\nCore skill\n\n\n\nRead in tabular data with latitude, longitude into a data frame.\n\nreadr::read_csv() or readxl::read_excel()"
  },
  {
    "objectID": "r-tutorials/05-r-geospatial.html#from-a-csv-file",
    "href": "r-tutorials/05-r-geospatial.html#from-a-csv-file",
    "title": "Geospatial in R - lab 1",
    "section": "from a csv file",
    "text": "from a csv file\nHere I use a URL to a csv file. However I could use fil &lt;- file.path(\"data\", \"india_tide_guages.csv\") since I have the data file in a directory data in the same folder as my Quarto file (or RMarkdown or R script).\n\nfil &lt;- here::here(\"r-tutorials\", \"data\", \"india_tide_guages.csv\")\ndf2 &lt;- readr::read_csv(fil, show_col_types = FALSE)\n\nConvert to spatial data frame. Notice, I had to change the latitude and longitude to match the columns names in the dataframe.\n\nsdf &lt;- sf::st_as_sf(\n    df2, \n    coords = c(\"Longitude\", \"Latitude\"), # what are the x and y dimension names\n    crs = 4326)\n\nMap. You can click on the points to get more info.\n\nmapview::mapview(sdf)\n\n\n\n\n\n\nIf you want state labels, you need to only have the geometry and label columns in the dataframe.\n\nsdf2 &lt;- sdf %&gt;% select(geometry, State)\nmapview::mapview(sdf2, label = sdf2$State)\n\n\nfrom Excel file\n\nfil &lt;- here::here(\"r-tutorials\", \"data\", \"india_tide_guages.xlsx\")\ndf3 &lt;- readxl::read_excel(fil, sheet = \"Kerala\")\n\nConvert to spatial points.\n\nsdf = sf::st_as_sf(\n    df3, \n    coords = c(\"Longitude\", \"Latitude\"), \n    crs = 4326)\n\n\nmapview::mapview(sdf)"
  },
  {
    "objectID": "r-tutorials/05-r-geospatial.html#using-ggplot2",
    "href": "r-tutorials/05-r-geospatial.html#using-ggplot2",
    "title": "Geospatial in R - lab 1",
    "section": "Using ggplot2",
    "text": "Using ggplot2\nHere is a gallery of some basic plots you can make. There are many ways to make maps with ggplot2. I will use a single approach that is fairly flexible.\n\n\n\n\n\n\nCore skill\n\n\n\nCreate a base world and India map from rnaturalearth. Plot with ggplot2.\n\nne_countries()\nggplot() + geom_sf()\ncoord_sf()\n\n\n\n\nIndia alone\n\nlibrary(ggplot2)\nlibrary(sf)\nlibrary(rnaturalearth)\nindia_sf &lt;- ne_countries(country = \"India\", scale = \"medium\", returnclass = \"sf\")\nbasemap &lt;- ggplot() + \n  geom_sf(data = india_sf, color = \"black\", size = 2, fill=\"green\") +\n  coord_sf(xlim = c(58, 98), ylim = c(6, 30))\nbasemap\n\n\n\n\n\nfil &lt;- here::here(\"r-tutorials\", \"data\", \"india.jpeg\")\nggsave(filename = fil, plot = basemap, device = \"jpeg\")\n\nSaving 7 x 5 in image\n\n\n\n\n\n\n\n\nCore skill\n\n\n\nAdd points to a plot.\n\ngeom_sf(data=points_df)\n\n\n\nAdd points\n\nbasemap + \n  geom_sf(data = cities3, aes(color = names), size = 3) +\n  theme_void()\n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n\n\n\n\n\n\n\nThe world\n\nlibrary(ggplot2)\nlibrary(sf)\nlibrary(rnaturalearth)\nworld_sf &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")\nbasemap &lt;- ggplot() + \n  geom_sf(data = world_sf, color = \"black\", size = 0.2, fill=\"lightblue\")\nbasemap\n\n\n\n\nAdd points\n\nbasemap + \n  geom_sf(data = cities3, aes(color = names), size = 1)\n\n\n\n\nZoom in\n\nlibrary(ggplot2)\nlibrary(sf)\nlibrary(rnaturalearth)\nworld_sf &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")\nbasemap &lt;- ggplot() + \n  geom_sf(data = world_sf, color = \"black\", size = 0.2, fill=\"lightblue\") +\n  coord_sf(xlim = c(58, 98), ylim = c(0, 30))\nbasemap\n\n\n\n\nAdd points\n\nbasemap + \n  geom_sf(data = cities3, aes(color = names), size = 2) +\n  coord_sf(xlim = c(58, 98), ylim = c(0, 30))\n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one."
  },
  {
    "objectID": "r-tutorials/05-r-geospatial.html#change-the-projection",
    "href": "r-tutorials/05-r-geospatial.html#change-the-projection",
    "title": "Geospatial in R - lab 1",
    "section": "Change the projection",
    "text": "Change the projection\n\n\n\n\n\n\nCore skill\n\n\n\nApply a coordinate reference system to a sf object.\n\nst_transform(sf_object, crs=crs)\n\nCommon CRS‚Äôs\n\ncrs = 4326 WGS 84\nRobinson crs = \"+proj=robin +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs\"\nGlobe crs = \"+proj=laea +lon_0=77 +lat_0=20 +ellps=WGS84 +no_defs\"\n\n\n\nMake a world in Robinson coord system.\n\nlibrary(ggplot2)\nlibrary(sf)\nlibrary(rnaturalearth)\nworld_sf &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")\ncrs &lt;- \"+proj=robin +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs\" \nworld_crs &lt;- st_transform(world_sf, crs=crs)\nbasemap &lt;- ggplot() +\n  geom_sf(data = world_crs, color = \"black\", size = 0.2, fill=\"lightblue\") +\n  theme_minimal()\nbasemap\n\n\n\n\nAdd points.\n\ncities3_robin &lt;- st_transform(cities3, crs=crs)\nbasemap + \n  geom_sf(data = cities3_robin, aes(color = names), size = 2)\n\n\n\n\nMake a globe.\n\nlibrary(ggplot2)\nlibrary(sf)\nlibrary(rnaturalearth)\nworld_sf &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")\ncrs &lt;- \"+proj=laea +lon_0=77 +lat_0=20 +ellps=WGS84 +no_defs\"\nworld_crs &lt;- sf::st_transform(world_sf, crs)\nbasemap &lt;- ggplot() +\n  geom_sf(data = world_crs, color = \"black\", size = 0.2, fill=\"lightblue\") +\n  theme_minimal()\nbasemap\n\n\n\n\nAdd points.\n\ncities3_crs &lt;- st_transform(cities3, crs=crs)\nbasemap + \n  geom_sf(data = cities3_crs, aes(color = names), size = 2)\n\n\n\n\nAdd a circle around the globe.\n\nlibrary(ggplot2)\nlibrary(sf)\nlibrary(rnaturalearth)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nworld_sf &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")\ncrs &lt;- \"+proj=laea +lon_0=77 +lat_0=20 +ellps=WGS84 +no_defs\"\nworld_crs &lt;- sf::st_transform(world_sf, crs)\n\nsphere &lt;- st_graticule(ndiscr = 10000, margin = 10e-6) %&gt;%\n  st_transform(crs = crs) %&gt;%\n  st_convex_hull() %&gt;%\n  summarise(geometry = st_union(geometry))\n\nbasemap &lt;- ggplot()  +\n  geom_sf(data = sphere, fill = \"#D8F4FF\", alpha = 0.7) +\n  geom_sf(data = world_crs, fill=\"grey\") +\n  theme_bw()\nbasemap\n\n\n\n\nAdd points and remove legend.\n\n# Add crs\ncities3_crs &lt;- st_transform(cities3, crs=crs)\nbasemap + \n  geom_sf(data = cities3_crs, aes(color = names), size = 1) +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "r-tutorials/05-r-geospatial.html#getting-help-from-chatgpt",
    "href": "r-tutorials/05-r-geospatial.html#getting-help-from-chatgpt",
    "title": "Geospatial in R - lab 1",
    "section": "Getting help from ChatGPT",
    "text": "Getting help from ChatGPT\nUnfortunately, ChatGPT often gets confused with mapping and gives you code that doesn‚Äôt fully work. This can be hard for beginners (and experts) to debug.\nTry telling it\n\nUse only the sf, rnaturalearth, and ggplot2 packages\nWork in steps, ‚ÄúMake a sf points object and name it sf_points‚Äù, ‚ÄúUsing sf_points‚Äù, add these points to a map of the world.‚Äù"
  },
  {
    "objectID": "r-tutorials/05-r-geospatial.html#your-turn",
    "href": "r-tutorials/05-r-geospatial.html#your-turn",
    "title": "Geospatial in R - lab 1",
    "section": "Your Turn!",
    "text": "Your Turn!\nMake some maps using mapview of your own data, data in the ‚Äúr-tutorials/data‚Äù directory or data you can find on-line.\nTry the layer feature to change the base map."
  },
  {
    "objectID": "r-tutorials/05-r-geospatial.html#advanced-programmers",
    "href": "r-tutorials/05-r-geospatial.html#advanced-programmers",
    "title": "Geospatial in R - lab 1",
    "section": "Advanced programmers",
    "text": "Advanced programmers\nTry using customizing mapview to create some pretty maps of the tide guage data!\nHere are some ideas\n\nhttps://www.paulamoraga.com/book-spatial/making-maps-with-r.html#mapview\nArticles tab here https://r-spatial.github.io/mapview/index.html\nThis shows a nicer example of maps with ggplot https://r-spatial.org/r/2018/10/25/ggplot2-sf.html.\nHere are some examples of maps I made in R. Can you adapt the globe example to show India and add the tide guage points? https://eeholmes.github.io/30Maps/"
  },
  {
    "objectID": "r-tutorials/Lab-setup.html",
    "href": "r-tutorials/Lab-setup.html",
    "title": "Labs - Set up",
    "section": "",
    "text": "For initial set-up, you will need to create an folder sdm-labs and then create an RStudio project in that folder. Then every day, when you work on the tutorials, you will copy the tutorial into that folder. That copy is what you can alter, save and render (create html)."
  },
  {
    "objectID": "r-tutorials/Lab-setup.html#set-up-a-new-project",
    "href": "r-tutorials/Lab-setup.html#set-up-a-new-project",
    "title": "Labs - Set up",
    "section": "Set up a new project",
    "text": "Set up a new project\nYou will need to create a directory called sdm-labs in your home directory. You home directory is called home/jovyan. Navigate there and then click the New Folder icon and create sdm-labs.\n\n\n\nCreate a sdm-labs folder if you have not already\n\n\nCreate an RStudio project at the sdm-labs directory. Go to\nFile &gt; New Project &gt; Existing directory\nThen navigate to the sdm-labs directory and create the project.\n\nCreate a data folder\nWithin the sdm-labs directory, also create a folder called data. You will store data there."
  },
  {
    "objectID": "r-tutorials/Lab-setup.html#returning-to-your-project",
    "href": "r-tutorials/Lab-setup.html#returning-to-your-project",
    "title": "Labs - Set up",
    "section": "Returning to your project",
    "text": "Returning to your project\nWhen you reopen RStudio, it should go to the last project that you had open. But if not you can select the sdm-labs project by click the ‚Äúblue cube‚Äù dropdown in the top right and selecting the sdm-labs project."
  },
  {
    "objectID": "r-tutorials/Lab-setup.html#copy-the-lab-into-the-sdm-labs-directory",
    "href": "r-tutorials/Lab-setup.html#copy-the-lab-into-the-sdm-labs-directory",
    "title": "Labs - Set up",
    "section": "Copy the lab into the sdm-labs directory",
    "text": "Copy the lab into the sdm-labs directory\nThe R tutorials are in the folder r-tutorials in the shared/2023-Hackbook directory. You can open the lab we are working on in that directory and run the code, but you will not be able to save your work. There are 2 ways you can save your work.\n\nOpen file and save when ready\nYou can work in the file in the r-tutorials directory as long as you don‚Äôt ‚ÄúRender‚Äù the document. If you try to do that, it will complain that you do not have write permission.\nSo you work in the file and run code with the ‚ÄúRun‚Äù icon and then when you want to save, use File &gt; Save As. Then navigate to your \\home\\jovyan\\sdm-labs directory and save there.\n\n\nSave the file first\n\nNavigate to the r-notebooks folder\nSelect the file you will be using by clicking the check box\n\n * Click the Cog icon and select ‚ÄúCopy to‚Äù * Copy to to your \\home\\jovyan\\sdm-labs directory.\n\n\n\nCopy to\n\n\nNow navigate to the sdm-labs folder and open the file."
  },
  {
    "objectID": "r-tutorials/SDM-Lab1.html",
    "href": "r-tutorials/SDM-Lab1.html",
    "title": "SDM Lab 1",
    "section": "",
    "text": "Save the files to your own folder to work though the examples or follow along but don‚Äôt save."
  },
  {
    "objectID": "r-tutorials/SDM-Lab1.html#topics",
    "href": "r-tutorials/SDM-Lab1.html#topics",
    "title": "SDM Lab 1",
    "section": "Topics",
    "text": "Topics\nOHW marine SDM tutorial\n\nDownload occurence data.\n\nshared/2023-Hackbook/r-tutorials/SDM-lab-spocc.qmd\nshared/2023-Hackbook/r-tutorials/SDM-lab-robis.qmd\n\n\n\nDownload sdmpredictors data\n\nThe spreadsheet with all the variables\nshared/tutorials-marine-sdm/tutorial/Steps_occ_env.Rmd\n\n\n\nFull example\n\nshared/tutorials-marine-sdm/SDM/Turtle_maxnet.Rmd"
  },
  {
    "objectID": "r-tutorials/SDM-lab-robis.html",
    "href": "r-tutorials/SDM-lab-robis.html",
    "title": "SDM lab 1 - Downloading species data with robis",
    "section": "",
    "text": "Here we download from OBIS using the robis package."
  },
  {
    "objectID": "r-tutorials/SDM-lab-robis.html#set-up",
    "href": "r-tutorials/SDM-lab-robis.html#set-up",
    "title": "SDM lab 1 - Downloading species data with robis",
    "section": "Set up",
    "text": "Set up\n\nhere::i_am(\"r-tutorials/SDM-lab-robis.qmd\")\n\nhere() starts at /Users/eli.holmes/Documents/GitHub/2023-Cloudbook\n\ndir_data &lt;- here::here(\"r-tutorials\", \"data\")\n\n\nlibrary(ggplot2)\nlibrary(sf)\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\nlibrary(\"rnaturalearth\")\nlibrary(\"rnaturalearthdata\")\n\n\nAttaching package: 'rnaturalearthdata'\n\n\nThe following object is masked from 'package:rnaturalearth':\n\n    countries110\n\nlibrary(raster)\n\nLoading required package: sp\n\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.3     ‚úî readr     2.1.4\n‚úî forcats   1.0.0     ‚úî stringr   1.5.0\n‚úî lubridate 1.9.2     ‚úî tibble    3.2.1\n‚úî purrr     1.0.1     ‚úî tidyr     1.3.0\n\n\n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ tidyr::extract() masks raster::extract()\n‚úñ dplyr::filter()  masks stats::filter()\n‚úñ dplyr::lag()     masks stats::lag()\n‚úñ dplyr::select()  masks raster::select()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(robis)\n\n\nAttaching package: 'robis'\n\nThe following object is masked from 'package:raster':\n\n    area"
  },
  {
    "objectID": "r-tutorials/SDM-lab-robis.html#create-a-bounding-box",
    "href": "r-tutorials/SDM-lab-robis.html#create-a-bounding-box",
    "title": "SDM lab 1 - Downloading species data with robis",
    "section": "Create a bounding box",
    "text": "Create a bounding box\n\nbbox &lt;- sf::st_bbox(c(xmin = 41.875, xmax = 65.125, ymax = -0.125, ymin = 32.125), \n                          crs = sf::st_crs(4326))\n\nCreates a sf object with a sfs_POLYGON from which we can get a polygon string. We also use this for cropping with the raster package, while we will need bbox for cropping with the stars package.\n\nextent_polygon &lt;-  bbox %&gt;% sf::st_as_sfc() %&gt;% st_sf()\n\nThen for the robis package we need a bounding box string.\n\nwkt_geometry &lt;- extent_polygon$geometry %&gt;% st_as_text()\n\nMake a map of our region so we know we have the right area.\n\nworld &lt;- rnaturalearth::ne_countries(scale = \"medium\", returnclass = \"sf\")\nggplot(data = world) + geom_sf() +\n  geom_sf(data = extent_polygon, color = \"red\", fill=NA)"
  },
  {
    "objectID": "r-tutorials/SDM-lab-robis.html#get-occurrence-data-from-robis",
    "href": "r-tutorials/SDM-lab-robis.html#get-occurrence-data-from-robis",
    "title": "SDM lab 1 - Downloading species data with robis",
    "section": "Get occurrence data from robis",
    "text": "Get occurrence data from robis\nWe will download data for four sea turtles found in the Arabian sea and save to one file. We will use the occurrence() function in the robis package.\n\nspp &lt;- c(\"Chelonia mydas\", \"Caretta caretta\", \"Eretmochelys imbricata\", \"Lepidochelys olivacea\", \"Natator depressus\", \"Dermochelys coriacea\")\n\nobs &lt;- robis::occurrence(spp, startdate = as.Date(\"2000-01-01\"), geometry = wkt_geometry)\n\nThis has many columns that we don‚Äôt need. We reduced to fewer columns.\n\ncols.to.use &lt;- c(\"occurrenceID\", \"scientificName\", \n                 \"dateIdentified\", \"eventDate\", \n                 \"decimalLatitude\", \"decimalLongitude\", \"coordinateUncertaintyInMeters\",\n                 \"individualCount\",\"lifeStage\", \"sex\",\n                 \"bathymetry\",  \"shoredistance\", \"sst\", \"sss\")\nobs &lt;- obs[,cols.to.use]\n\nWe also added a cleaner date with YYYY-MM-DD format.\n\nobs$date &lt;- as.Date(obs$eventDate)"
  },
  {
    "objectID": "r-tutorials/SDM-lab-robis.html#save-our-data",
    "href": "r-tutorials/SDM-lab-robis.html#save-our-data",
    "title": "SDM lab 1 - Downloading species data with robis",
    "section": "Save our data",
    "text": "Save our data\nSet up the file names\n\ndir_data &lt;- here::here(\"data\")\nfil &lt;- here::here(\"data\", \"io-sea-turtles.csv\")\nreadr::write_csv(obs, file=fil)\n\nLater we can reload our data as\n\nfil &lt;- here::here(\"r-tutorials\", \"data\", \"io-sea-turtles.csv\")\nobs &lt;- read.csv(fil)\n\n\nClean the data\nSelect species.\n\n# subset the occurrences to include just those in the water\nobs &lt;- obs %&gt;% \n  subset(bathymetry &gt; 0 & shoredistance &gt; 0 & coordinateUncertaintyInMeters &lt; 200)\n\n# seeing how often each species occurs\ntable(obs$scientificName) \n\n\nCaretta caretta  Chelonia mydas \n           5141            7060"
  },
  {
    "objectID": "r-tutorials/SDM-lab-spocc.html",
    "href": "r-tutorials/SDM-lab-spocc.html",
    "title": "SDM lab 1 - Downloading species data with spocc",
    "section": "",
    "text": "Set the data directory.\nhere::i_am(\"r-tutorials/SDM-lab-spocc.qmd\")\n\nhere() starts at /Users/eli.holmes/Documents/GitHub/2023-Cloudbook\n\ndir_data &lt;- here::here(\"r-tutorials\", \"data\")\nSet the file paths.\n# data as csv file\nobs_csv &lt;- file.path(dir_data, \"obs.csv\")\n# data as geojson\nobs_geo &lt;- file.path(dir_data, \"obs.geojson\")\nLoad libraries.\nrequire(sf)\nrequire(spocc)\nrequire(knitr)\nrequire(dplyr)\nrequire(readr)"
  },
  {
    "objectID": "r-tutorials/SDM-lab-spocc.html#spocc-r-package",
    "href": "r-tutorials/SDM-lab-spocc.html#spocc-r-package",
    "title": "SDM lab 1 - Downloading species data with spocc",
    "section": "spocc R package",
    "text": "spocc R package\nThe spocc R package which allows us to query species occurrence data from a variety of sources:\nspocc: A programmatic interface to many species occurrence data sources, including GBIF, iNaturalist, Berkeley Ecoinformatics Engine, eBird, iDigBio, VertNet, OBIS, and ALA. Includes functionality for retrieving species occurrence data, and combining that data.\n\nBrown-throated sloth\nThis is a classic example for Species Distribution modeling. It is a sloth found in South America.\n\n\nif (!file.exists(obs_csv)){\n# get species occurrence data from GBIF with coordinates\nres &lt;- spocc::occ(\n    query = 'Bradypus variegatus', \n    from = 'gbif', has_coords = TRUE)\n  \n# extract data frame from result\ndf &lt;- res$gbif$data[[1]] \n# write data to a file\nreadr::write_csv(df, obs_csv)\n}else{\n  df &lt;- readr::read_csv(obs_csv)\n}\n\nRows: 500 Columns: 83\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr  (55): name, issues, prov, scientificName, datasetKey, publishingOrgKey,...\ndbl  (21): longitude, latitude, key, crawlId, taxonKey, kingdomKey, phylumKe...\nlgl   (1): isInCluster\ndttm  (5): lastCrawled, lastParsed, dateIdentified, modified, lastInterpreted\ndate  (1): eventDate\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\n\n\nLook at the objects\n\n\n\nType names(res) and then names(res$gbif). Then type df to see what is in the dataframe.\n\n\nMake a table of the first few lines of the dataframe.\n\nknitr::kable(df %&gt;% head)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\nlongitude\nlatitude\nissues\nprov\nkey\nscientificName\ndatasetKey\npublishingOrgKey\ninstallationKey\nhostingOrganizationKey\npublishingCountry\nprotocol\nlastCrawled\nlastParsed\ncrawlId\nbasisOfRecord\noccurrenceStatus\ntaxonKey\nkingdomKey\nphylumKey\nclassKey\norderKey\nfamilyKey\ngenusKey\nspeciesKey\nacceptedTaxonKey\nacceptedScientificName\nkingdom\nphylum\norder\nfamily\ngenus\nspecies\ngenericName\nspecificEpithet\ntaxonRank\ntaxonomicStatus\niucnRedListCategory\ndateIdentified\ncoordinateUncertaintyInMeters\ncontinent\nstateProvince\nyear\nmonth\nday\neventDate\nmodified\nlastInterpreted\nreferences\nlicense\nisInCluster\ndatasetName\nrecordedBy\nidentifiedBy\ngeodeticDatum\nclass\ncountryCode\ncountry\nrightsHolder\nidentifier\nhttp://unknown.org/nick\nverbatimEventDate\ncollectionCode\ngbifID\nverbatimLocality\noccurrenceID\ntaxonID\ncatalogNumber\ninstitutionCode\neventTime\nhttp://unknown.org/captive\nidentificationID\noccurrenceRemarks\nsex\nlifeStage\nindividualCount\nvernacularName\nlocality\nhigherClassification\ninformationWithheld\ninfraspecificEpithet\nidentificationRemarks\n\n\n\n\nBradypus variegatus Schinz, 1825\n-79.54650\n8.987403\ncdc,cdround\ngbif\n4011797263\nBradypus variegatus Schinz, 1825\n50c9509d-22c7-4a22-a47d-8c48425ef4a7\n28eb1a3f-1c15-4a95-931a-4af90ecb574d\n997448a8-f762-11e1-a439-00145eb45e9a\n28eb1a3f-1c15-4a95-931a-4af90ecb574d\nUS\nDWC_ARCHIVE\n2023-08-17 14:21:14\n2023-08-18 13:22:12\n390\nHUMAN_OBSERVATION\nPRESENT\n2436353\n1\n44\n359\n1494\n9418\n2436350\n2436353\n2436353\nBradypus variegatus Schinz, 1825\nAnimalia\nChordata\nPilosa\nBradypodidae\nBradypus\nBradypus variegatus\nBradypus\nvariegatus\nSPECIES\nACCEPTED\nLC\n2023-01-02 21:10:21\n61\nNORTH_AMERICA\nPanam√°\n2023\n1\n2\n2023-01-02\n2023-03-09 20:50:19\n2023-08-18 13:22:12\nhttps://www.inaturalist.org/observations/145694886\nhttp://creativecommons.org/licenses/by/4.0/legalcode\nFALSE\niNaturalist research-grade observations\nKai Squires\nKai Squires\nWGS84\nMammalia\nPA\nPanama\nKai Squires\n145694886\nsquiresk\n2023/01/02 9:49 AM\nObservations\n4011797263\nPanama City, Panama\nhttps://www.inaturalist.org/observations/145694886\n47067\n145694886\niNaturalist\n09:49:00-05:00\nwild\n324468453\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nBradypus variegatus Schinz, 1825\n-84.68933\n10.519427\ncdc,cdround\ngbif\n4091593493\nBradypus variegatus Schinz, 1825\n50c9509d-22c7-4a22-a47d-8c48425ef4a7\n28eb1a3f-1c15-4a95-931a-4af90ecb574d\n997448a8-f762-11e1-a439-00145eb45e9a\n28eb1a3f-1c15-4a95-931a-4af90ecb574d\nUS\nDWC_ARCHIVE\n2023-08-17 14:21:14\n2023-08-18 13:41:19\n390\nHUMAN_OBSERVATION\nPRESENT\n2436353\n1\n44\n359\n1494\n9418\n2436350\n2436353\n2436353\nBradypus variegatus Schinz, 1825\nAnimalia\nChordata\nPilosa\nBradypodidae\nBradypus\nBradypus variegatus\nBradypus\nvariegatus\nSPECIES\nACCEPTED\nLC\n2023-01-31 13:22:05\n4\nNORTH_AMERICA\nAlajuela\n2023\n1\n2\n2023-01-02\n2023-04-13 01:01:22\n2023-08-18 13:41:19\nhttps://www.inaturalist.org/observations/145696593\nhttp://creativecommons.org/licenses/by-nc/4.0/legalcode\nFALSE\niNaturalist research-grade observations\nJessica Rae\nWren\nWGS84\nMammalia\nCR\nCosta Rica\nJessica Rae\n145696593\naltoidsboi\n2023/01/02 3:54 PM\nObservations\n4091593493\nAlajuela Province, San Carlos, Costa Rica\nhttps://www.inaturalist.org/observations/145696593\n47067\n145696593\niNaturalist\n15:54:00-06:00\nwild\n330896608\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nBradypus variegatus Schinz, 1825\n-82.15604\n9.316584\ncdc,cdround\ngbif\n4028722131\nBradypus variegatus Schinz, 1825\n50c9509d-22c7-4a22-a47d-8c48425ef4a7\n28eb1a3f-1c15-4a95-931a-4af90ecb574d\n997448a8-f762-11e1-a439-00145eb45e9a\n28eb1a3f-1c15-4a95-931a-4af90ecb574d\nUS\nDWC_ARCHIVE\n2023-08-17 14:21:14\n2023-08-18 13:41:57\n390\nHUMAN_OBSERVATION\nPRESENT\n2436353\n1\n44\n359\n1494\n9418\n2436350\n2436353\n2436353\nBradypus variegatus Schinz, 1825\nAnimalia\nChordata\nPilosa\nBradypodidae\nBradypus\nBradypus variegatus\nBradypus\nvariegatus\nSPECIES\nACCEPTED\nLC\n2023-01-03 18:24:03\n75\nNORTH_AMERICA\nBocas del Toro\n2023\n1\n3\n2023-01-03\n2023-02-02 01:16:10\n2023-08-18 13:41:57\nhttps://www.inaturalist.org/observations/145767431\nhttp://creativecommons.org/licenses/by-nc/4.0/legalcode\nFALSE\niNaturalist research-grade observations\nlucybrown19\nlucybrown19\nWGS84\nMammalia\nPA\nPanama\nlucybrown19\n145767431\nlucybrown19\nTue Jan 03 2023 11:36:24 GMT -0500 (EST)\nObservations\n4028722131\nBocas del Toro, PA-BC, PA\nhttps://www.inaturalist.org/observations/145767431\n47067\n145767431\niNaturalist\n11:36:24-05:00\nwild\n324686542\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nBradypus variegatus Schinz, 1825\n-84.06505\n10.449010\ncdc,cdround\ngbif\n4011771409\nBradypus variegatus Schinz, 1825\n50c9509d-22c7-4a22-a47d-8c48425ef4a7\n28eb1a3f-1c15-4a95-931a-4af90ecb574d\n997448a8-f762-11e1-a439-00145eb45e9a\n28eb1a3f-1c15-4a95-931a-4af90ecb574d\nUS\nDWC_ARCHIVE\n2023-08-17 14:21:14\n2023-08-18 13:41:03\n390\nHUMAN_OBSERVATION\nPRESENT\n2436353\n1\n44\n359\n1494\n9418\n2436350\n2436353\n2436353\nBradypus variegatus Schinz, 1825\nAnimalia\nChordata\nPilosa\nBradypodidae\nBradypus\nBradypus variegatus\nBradypus\nvariegatus\nSPECIES\nACCEPTED\nLC\n2023-01-03 19:44:25\n5\nNORTH_AMERICA\nHeredia\n2023\n1\n3\n2023-01-03\n2023-01-16 15:12:42\n2023-08-18 13:41:03\nhttps://www.inaturalist.org/observations/145772860\nhttp://creativecommons.org/licenses/by/4.0/legalcode\nFALSE\niNaturalist research-grade observations\nChris Harrison\nChris Harrison\nWGS84\nMammalia\nCR\nCosta Rica\nChris Harrison\n145772860\nsandboa\n2023-01-03 11:57:43\nObservations\n4011771409\nSarapiqu√≠, CR-HE, CR\nhttps://www.inaturalist.org/observations/145772860\n47067\n145772860\niNaturalist\n11:57:43-06:00\nwild\n324705982\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nBradypus variegatus Schinz, 1825\n-79.64967\n9.061424\ncdc,cdround\ngbif\n4014910775\nBradypus variegatus Schinz, 1825\n50c9509d-22c7-4a22-a47d-8c48425ef4a7\n28eb1a3f-1c15-4a95-931a-4af90ecb574d\n997448a8-f762-11e1-a439-00145eb45e9a\n28eb1a3f-1c15-4a95-931a-4af90ecb574d\nUS\nDWC_ARCHIVE\n2023-08-17 14:21:14\n2023-08-18 13:21:15\n390\nHUMAN_OBSERVATION\nPRESENT\n2436353\n1\n44\n359\n1494\n9418\n2436350\n2436353\n2436353\nBradypus variegatus Schinz, 1825\nAnimalia\nChordata\nPilosa\nBradypodidae\nBradypus\nBradypus variegatus\nBradypus\nvariegatus\nSPECIES\nACCEPTED\nLC\n2023-01-04 02:04:32\n365\nNORTH_AMERICA\nPanam√°\n2023\n1\n3\n2023-01-03\n2023-03-09 20:50:14\n2023-08-18 13:21:15\nhttps://www.inaturalist.org/observations/145797456\nhttp://creativecommons.org/licenses/by-nc/4.0/legalcode\nFALSE\niNaturalist research-grade observations\nMatt Cohen and Elizabeth Hargrave\nMatt Cohen and Elizabeth Hargrave\nWGS84\nMammalia\nPA\nPanama\nMatt Cohen and Elizabeth Hargrave\n145797456\nmattandeliz\n2023/01/03 9:02 AM\nObservations\n4014910775\nPanama\nhttps://www.inaturalist.org/observations/145797456\n47067\n145797456\niNaturalist\n09:02:00-05:00\nwild\n324778216\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nBradypus variegatus Schinz, 1825\n-80.14370\n8.602112\ncdc,cdround\ngbif\n4015257923\nBradypus variegatus Schinz, 1825\n50c9509d-22c7-4a22-a47d-8c48425ef4a7\n28eb1a3f-1c15-4a95-931a-4af90ecb574d\n997448a8-f762-11e1-a439-00145eb45e9a\n28eb1a3f-1c15-4a95-931a-4af90ecb574d\nUS\nDWC_ARCHIVE\n2023-08-17 14:21:14\n2023-08-18 13:40:25\n390\nHUMAN_OBSERVATION\nPRESENT\n2436353\n1\n44\n359\n1494\n9418\n2436350\n2436353\n2436353\nBradypus variegatus Schinz, 1825\nAnimalia\nChordata\nPilosa\nBradypodidae\nBradypus\nBradypus variegatus\nBradypus\nvariegatus\nSPECIES\nACCEPTED\nLC\n2023-01-05 00:14:33\n171\nNORTH_AMERICA\nCocl√©\n2023\n1\n4\n2023-01-04\n2023-03-09 20:50:13\n2023-08-18 13:40:25\nhttps://www.inaturalist.org/observations/145862390\nhttp://creativecommons.org/licenses/by-nc/4.0/legalcode\nFALSE\niNaturalist research-grade observations\nMatt Cohen and Elizabeth Hargrave\nMatt Cohen and Elizabeth Hargrave\nWGS84\nMammalia\nPA\nPanama\nMatt Cohen and Elizabeth Hargrave\n145862390\nmattandeliz\n2023/01/04 2:49 PM\nObservations\n4015257923\nAnton Valley, Panama\nhttps://www.inaturalist.org/observations/145862390\n47067\n145862390\niNaturalist\n14:49:00-05:00\nwild\n324981107\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\nConvert to points of observation from lon/lat columns in data frame. This part is using the sf R package to make our dataframe into a geospatial object. This way the mapping functions will be able to plot the points easily.\n\nobs &lt;- df %&gt;% \n  sf::st_as_sf(\n    coords = c(\"longitude\", \"latitude\"), # what columns have x and y\n    crs = sf::st_crs(4326)) %&gt;% # what projection to use\n    dplyr::select(prov, key) # save space (optional)\n# save the file\nsf::write_sf(obs, obs_geo, delete_dsn=TRUE)\nnrow(obs) # number of rows\n\n[1] 500\n\n\nThe format of a bounding box is [min-longitude, min-latitude, max-longitude, max-latitude]"
  },
  {
    "objectID": "r-tutorials/SDM-models.html",
    "href": "r-tutorials/SDM-models.html",
    "title": "SDM lab 1 - Downloading species data",
    "section": "",
    "text": "https://biodiversityinformatics.amnh.org/open_source/maxent/\n\nJane Elith, Steven J. Phillips, Trevor Hastie, Miroslav Dud√≠k, Yung En Chee, and Colin J. Yates. 2011. A statistical explanation of MaxEnt for ecologists. Diversity and Distributions, 17:43-57. PDF\nCorey Merow, Matthew J. Smith, and John A. Silander, Jr.¬†2013. A practical guide to MaxEnt for modeling species‚Äô distributions: what it does, and why inputs and settings matter. Ecography, 36: 1058‚Äì1069. PDF\nAleksandar Radosavljevic and Robert P. Anderson. 2014. Making better Maxent models of species distributions: complexity, overfitting, and evaluation. Journal of Biogeography, 41: 629‚Äì643. PDF"
  },
  {
    "objectID": "r-tutorials/newfile.html#hi",
    "href": "r-tutorials/newfile.html#hi",
    "title": "2023 ITCOocean Hack2week",
    "section": "Hi",
    "text": "Hi\nThis is a new file."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "11-22 September 2023"
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Set-up",
    "section": "",
    "text": "Before the course, please do the following set-up and send us the information in the pre-course survey."
  },
  {
    "objectID": "setup.html#user-accounts",
    "href": "setup.html#user-accounts",
    "title": "Set-up",
    "section": "User accounts",
    "text": "User accounts\n\nGitHub username\n\nCreate a GitHub account (if you don‚Äôt already have one) at https://github.com. Advice for choosing a GitHub username: this is a professional username that you will use in work/academic communities. GitHub accounts are not anonymous; this is for sharing work. Using your real name is common.\nRemember your username and password; you will need to be logged in during the course!\nHere is a video showing the whole process\n\nEarthdata Login account\n\nCreate an Earthdata Login account (if you don‚Äôt already have one) at https://urs.earthdata.nasa.gov\nRemember your username and password; you will need to be logged in during the course!\n\nSlack account\n\nWe will use Slack as our messaging platform during the course. Feel free to use to communicate and coordinate with the fellow course participants.\nThe link to join our Slack workspace was sent on Sept 4th in the course info email.\nReach out to the organizers if you can‚Äôt find the email."
  },
  {
    "objectID": "setup.html#install-r-and-rstudio-desktop",
    "href": "setup.html#install-r-and-rstudio-desktop",
    "title": "Set-up",
    "section": "Install R and RStudio Desktop",
    "text": "Install R and RStudio Desktop\nFor the course, we will mainly be using a browser based platform (JupyterHub) for coding and projects. JupyterHub and JupyterLab is a standard platform for ‚Äúbig data geoscience‚Äù and working on this platform will prepare you for using the tools and notebooks developed by this research community. However we also want you to be able to run R and RStudio on your own computer.\n\nR: https://cran.r-project.org/\n\nInstall the latest version of R that you can for your operating system. Preferably you want version 4.3+ but at the least you need version 4.1.\nIf you have R already installed, update to the newest version (4.3)\n\nRStudio Desktop: https://posit.co/download/rstudio-desktop/\n\nInstall the most recent version you can. Update your version if you have an old version installed."
  },
  {
    "objectID": "setup.html#do-an-intro-to-r-course",
    "href": "setup.html#do-an-intro-to-r-course",
    "title": "Set-up",
    "section": "Do an intro to R course",
    "text": "Do an intro to R course\nFor the course and hackweek, we require that you complete a short introduction course to R and RStudio‚Äìunless you already have R experience. This way you will be ready to start working with the course scripts and tutorials on day 1. These will only take you a 2-3 hours and will make your course experience much more worthwhile.\nAny of these is sufficient. Note, if you already program in R, you do not need to do this.\n\nW3Schools Basic R\nBasics of R and installing RStudio\nYouTube: R for Beginners\nFor a longer free course, CodeAcademy has a good Intro to R. Note this is 20 hours. You do NOT need to do this whole course before the hackweek!\nIf you know the basics of R but have never done modeling, then you can start with this 2.5 hour course that covers basic linear regression with R and plotting with ggplot2:\nOne of the best free R courses is Coursera‚Äôs Learn R. You can see the lectures for free. Make sure to click AUDIT when it asks you to sign up for a 7-day trial. You do not need to sign up for anything to go through the material.\n\n\nGitHub Discussion Board\nWe will use GitHub Discussion boards for communication during the course. If you like, you can install the GitHub app on your phone and get updates when there are announcements."
  },
  {
    "objectID": "support.html#thank-you-for-inspiration-and-content",
    "href": "support.html#thank-you-for-inspiration-and-content",
    "title": "Acknowledgements",
    "section": "Thank you for inspiration and content!",
    "text": "Thank you for inspiration and content!\nThank you to the open science community that has created software, teaching resources, and workflows that we have been able to build off of and be inspired by. These include: NASA Openscapes ‚Ä¢ OceanHackWeek ‚Ä¢ SnowEx Hackweek ‚Ä¢ eScience Institute, University of Washington ‚Ä¢ ICESat-2 Hackweek ‚Ä¢ Project Jupyter ‚Ä¢ Pangeo Project ‚Ä¢ CryoCloud   Thanks to OceanHackWeek for content, inspiration, and the opportunity to test out ideas. A big thanks to the SDM team at OceanHackWeek 2023 who helped test and develop SDM tutorials for the ITCOocean Hack2week 2023: Catherine Courtier, Mackenzie Fiss, Denisse Fierro Arcos, Paulo Freire, Jade Hong, Caitlin O‚ÄôBrien, Mary Solokas, Laura Tsang, Tylar Murray, Ben Tupper! OHW23 SDM team project."
  },
  {
    "objectID": "support.html#thank-you-for-support",
    "href": "support.html#thank-you-for-support",
    "title": "Acknowledgements",
    "section": "Thank you for support!",
    "text": "Thank you for support!\n\nThank you! INCOIS and ITCOocean provided all the logistical support and supported Nimit Kumar, TVS Uday Bhaskar and Swarnali Majumder‚Äôs participation. CMLRE supported Smitha BR‚Äôs participation and IITM supported Aditi Modi. A NOAA International Fellowship supported Eli Holmes‚Äô visit. 2i2c provided the JupyterHub infrastructure. ESIPfed provided funding for the JupyterHub via their ESIP Lab fund. Undergraduate summer interns, Minh Phan and Jiarui Yu, were supported by the Usha and S. Rao Varanasi endowment for Math-Fish student support at the University of Washington (Seattle, WA)."
  },
  {
    "objectID": "team.html#organizers-and-instructors",
    "href": "team.html#organizers-and-instructors",
    "title": "Our Team",
    "section": "Organizers and Instructors",
    "text": "Organizers and Instructors\n\n\nDr.¬†Eli Holmes\n\n\nNOAA Fisheries and University of Washington, Seattle, USA.\nwebpage ‚Ä¢ GitHub ‚Ä¢ ORCID\n\n\nDr.¬†Nimit Kumar\n\n\nIndian National Centre for Ocean Information Services (INCOIS), Hyderabad, India\nLinks: webpage ‚Ä¢ ORCID\n\n\nAditi Modi\n\nCentre for Climate Change Research, Indian Institute of Tropical Meteorology, Pune, India\nwebpage ‚Ä¢ GitHub ‚Ä¢ ORCID\n\n\nDr.¬†Smitha BR\n\nCentre for Marine Living Resources & Ecology, Kochi, India\nwebpage ‚Ä¢ ORCID\n\n\n\n\nDr.¬†Swarnali Majumder\n\nIndian National Centre for Ocean Information Services, Hyderabad, India\nwebpage ‚Ä¢ ORCID\n\n\nDr.¬†TVS Uday Bhaskar\n\nIndian National Centre for Ocean Information Services, Hyderabad, India\nResearchGate ‚Ä¢ ORCID\n\n\nDr.¬†Sourav Maity\n\nIndian National Centre for Ocean Information Services, Hyderabad, India\nResearchGate ‚Ä¢ ORCID"
  },
  {
    "objectID": "team.html#speakers",
    "href": "team.html#speakers",
    "title": "Our Team",
    "section": "Speakers",
    "text": "Speakers\n\n\nJohnny Konjarla\n\nCentre for Marine Living Resources & Ecology, Kochi, India\nwebpage ‚Ä¢ Linkedin ‚Ä¢ ORCID\n\n\nMyranda Shirk\n\nSenior Data Scientist, Vanderbilt University Data Science Institute\nLinkedin\n\n\nDr.¬†Ben Best\n\nEcoQuants.com, Santa Barbara, CA\nwebpage ‚Ä¢ GitHub ‚Ä¢ ORCID\n\n\nMinh Phan\n\nApplied & Computational Mathematical Sciences, University of Washington, Seattle WA, USA\nGitHub ‚Ä¢ Linkedin\n\n\n\n\nDr.¬†Sanitha K. Sivadas\n\nNational Centre for Coastal Research, Chennai, India\nOcean Expert ‚Ä¢ ORCID\n\n\nJiarui Yu\n\nApplied & Computational Mathematical Sciences, University of Washington, Seattle WA, USA\nGitHub ‚Ä¢ Linkedin"
  },
  {
    "objectID": "team.html#virtual-helpers",
    "href": "team.html#virtual-helpers",
    "title": "Our Team",
    "section": "Virtual Helpers",
    "text": "Virtual Helpers\n\n\nBen Tupper\n\nTandy Center for Ocean Forecasting, Bigelow Laboratory for Ocean Sciences, East Boothbay, Maine, USA.\nResearchGate\n\n\nDr.¬†Julie Lowndes\n\nOpenscapes, Santa Barbara, California, USA\nWebsite ‚Ä¢ ORCID\n\n\nAdam Kemberling\n\nGulf of Maine Research Institute, Portland, Maine, USA.\nWebsite\n\n\nDr.¬†Rachael Blake\n\nDirector of Data Science, Intertidal Agency, USA.\nWebsite ‚Ä¢ ORCID\n\n\n\n\nSuman Shekhar\n\nRutgers University, New Brunswick, NJ, USA.\nWebsite ‚Ä¢ GitHub\n\n\nSeungyeon Jade Hong\n\nGitHub"
  }
]